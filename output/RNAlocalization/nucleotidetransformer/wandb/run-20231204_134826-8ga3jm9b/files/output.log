
/home/sxr280/BERTLocRNA/saved_model/NT  already exists, loading the model locally
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.63s/it]
Some weights of the model checkpoint at /home/sxr280/BERTLocRNA/saved_model/NT were not used when initializing EsmModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing EsmModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmModel were not initialized from the model checkpoint at /home/sxr280/BERTLocRNA/saved_model/NT and are newly initialized: ['esm.pooler.dense.weight', 'esm.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading the dataset...
Resolving data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 296/296 [00:00<00:00, 497600.80it/s]
Resolving data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [00:00<00:00, 222470.16it/s]
Resolving data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:00<00:00, 340470.27it/s]
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
Total time taken for loading one batch: 0.12793922424316406 seconds
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
LightningModel                           [2, 9]                    --
├─CustomizedModel: 1-1                   [2, 9]                    --
│    └─MaxPool1d: 2-1                    [2, 2560, 167]            --
│    └─Dropout: 2-2                      [2, 2560, 167]            --
│    └─Attention_mask: 2-3               [2, 2560, 3]              --
│    │    └─Linear: 3-1                  [2, 167, 80]              204,800
│    │    └─Tanh: 3-2                    [2, 167, 80]              --
│    │    └─Linear: 3-3                  [2, 167, 3]               240
│    └─Flatten: 2-4                      [2, 7680]                 --
│    └─Embedding: 2-5                    [2, 4]                    76
│    └─Linear: 2-6                       [2, 100]                  768,100
│    └─Actvation: 2-7                    [2, 104]                  --
│    └─Dropout: 2-8                      [2, 104]                  --
│    └─Linear: 2-9                       [2, 9]                    945
│    └─Sigmoid: 2-10                     [2, 9]                    --
==========================================================================================
Total params: 974,161
Trainable params: 974,161
Non-trainable params: 0
Total mult-adds (M): 1.95
==========================================================================================
Input size (MB): 27.36
Forward/backward pass size (MB): 0.22
Params size (MB): 3.90
Estimated Total Size (MB): 31.48
==========================================================================================
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/lightning_fabric/plugins/environments/slurm.py:166: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python train_model.py ...
  rank_zero_warn(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
[2023-12-04 13:50:33,159][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2023-12-04 13:50:33,162][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------
Sanity Checking: 0it [00:00, ?it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
  | Name    | Type            | Params
--------------------------------------------
0 | network | CustomizedModel | 974 K
1 | loss_fn | BCELoss         | 0
--------------------------------------------
974 K     Trainable params
0         Non-trainable params
974 K     Total params
3.897     Total estimated model params size (MB)
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.

Sanity Checking: 0it [00:00, ?it/s]Total time taken for loading one batch: 2.78605580329895 seconds
Sanity Checking DataLoader 0:  50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                           | 1/2 [00:00<00:00, 35.97it/s]Total time taken for loading one batch: 3.133090019226074 seconds
Epoch 0:   0%|                                                                                                                                                                                                                                                  | 0/73 [00:00<?, ?it/s]
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val categorical_accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val categorical_accuracy_strict', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val binary_accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (58) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Epoch 0:   0%|                                                                                                                                                                                                                                                  | 0/73 [00:00<?, ?it/s]Total time taken for loading one batch: 2.7952919006347656 seconds

Epoch 0:   1%|█▎                                                                                          | 1/73 [00:08<10:34,  8.81s/it, loss=2.75, v_num=jm9b, train categorical_accuracy_step=0.000, train categorical_accuracy_strict_step=0.000, train binary_accuracy_step=0.432]Total time taken for loading one batch: 2.395585536956787 seconds
Epoch 0:   3%|██▍                                                                                        | 2/73 [00:17<10:37,  8.97s/it, loss=2.57, v_num=jm9b, train categorical_accuracy_step=0.359, train categorical_accuracy_strict_step=0.0156, train binary_accuracy_step=0.724]Total time taken for loading one batch: 2.657614231109619 seconds
Epoch 0:   4%|███▋                                                                                       | 3/73 [00:26<10:23,  8.91s/it, loss=2.44, v_num=jm9b, train categorical_accuracy_step=0.422, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.771]Total time taken for loading one batch: 2.5235466957092285 seconds
Epoch 0:   5%|████▉                                                                                      | 4/73 [00:35<10:09,  8.83s/it, loss=2.35, v_num=jm9b, train categorical_accuracy_step=0.391, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.774]Total time taken for loading one batch: 2.380603075027466 seconds

Epoch 0:   7%|██████▎                                                                                     | 5/73 [00:44<10:00,  8.82s/it, loss=2.25, v_num=jm9b, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.795]Total time taken for loading one batch: 2.6236612796783447 seconds
Epoch 0:   8%|███████▍                                                                                   | 6/73 [00:53<09:52,  8.84s/it, loss=2.15, v_num=jm9b, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.809]Total time taken for loading one batch: 2.7363715171813965 seconds
Epoch 0:  10%|████████▋                                                                                  | 7/73 [01:02<09:49,  8.93s/it, loss=2.06, v_num=jm9b, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.780]Total time taken for loading one batch: 2.453826665878296 seconds

Epoch 0:  11%|██████████                                                                                  | 8/73 [01:11<09:44,  8.99s/it, loss=1.98, v_num=jm9b, train categorical_accuracy_step=0.422, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.792]Total time taken for loading one batch: 2.928697347640991 seconds
Epoch 0:  12%|███████████▍                                                                                 | 9/73 [01:21<09:37,  9.03s/it, loss=1.9, v_num=jm9b, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.812]Total time taken for loading one batch: 2.787543296813965 seconds
Epoch 0:  14%|████████████▎                                                                             | 10/73 [01:30<09:27,  9.02s/it, loss=1.83, v_num=jm9b, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.786]Total time taken for loading one batch: 2.675307512283325 seconds

Epoch 0:  15%|█████████████▌                                                                            | 11/73 [01:38<09:17,  8.99s/it, loss=1.76, v_num=jm9b, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.783]Total time taken for loading one batch: 2.914050817489624 seconds
[2023-12-04 13:52:35,387][HYDRA] 	#1 : model=base_model task=RNAlocalization embedder=parnet
output dir of this job: /home/sxr280/BERTLocRNA/scripts/../output/RNAlocalization/parnet
embedding will be saved at: /home/sxr280/BERTLocRNA/embeddings/Parnetembedding
loading the dataset...
Resolving data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 243/243 [00:00<00:00, 451780.09it/s]
Resolving data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 268089.53it/s]
Resolving data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:00<00:00, 321354.63it/s]
Resolving data files:   0%|                                                                                                                                                                                                                                     | 0/77 [00:00<?, ?it/s]
Total time taken for loading one batch: 0.07563996315002441 seconds
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
LightningModel                           [2, 9]                    --
├─CustomizedModel: 1-1                   [2, 9]                    --
│    └─MaxPool1d: 2-1                    [2, 256, 1000]            --
│    └─Dropout: 2-2                      [2, 256, 1000]            --
│    └─Attention_mask: 2-3               [2, 256, 3]               --
│    │    └─Linear: 3-1                  [2, 1000, 80]             20,480
│    │    └─Tanh: 3-2                    [2, 1000, 80]             --
│    │    └─Linear: 3-3                  [2, 1000, 3]              240
│    └─Flatten: 2-4                      [2, 768]                  --
│    └─Embedding: 2-5                    [2, 4]                    76
│    └─Linear: 2-6                       [2, 100]                  76,900
│    └─Actvation: 2-7                    [2, 104]                  --
│    └─Dropout: 2-8                      [2, 104]                  --
│    └─Linear: 2-9                       [2, 9]                    945
│    └─Sigmoid: 2-10                     [2, 9]                    --
==========================================================================================
Total params: 98,641
Trainable params: 98,641
Non-trainable params: 0
Total mult-adds (M): 0.20
==========================================================================================
Input size (MB): 16.39
Forward/backward pass size (MB): 1.33
Params size (MB): 0.39
Estimated Total Size (MB): 18.12
GPU available: True (cuda), used: True                                                                                                                                                                                                                          | 0/77 [00:00<?, ?it/s]
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
