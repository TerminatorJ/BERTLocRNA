
/home/sxr280/BERTLocRNA/saved_model/NT  already exists, loading the model locally
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.75s/it]
Some weights of the model checkpoint at /home/sxr280/BERTLocRNA/saved_model/NT were not used when initializing EsmModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']
- This IS expected if you are initializing EsmModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmModel were not initialized from the model checkpoint at /home/sxr280/BERTLocRNA/saved_model/NT and are newly initialized: ['esm.pooler.dense.weight', 'esm.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Resolving data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 296/296 [00:00<00:00, 511121.44it/s]
Resolving data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [00:00<00:00, 259334.54it/s]
Resolving data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:00<00:00, 241287.99it/s]
loading the dataset...
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
Total time taken for loading one batch: 0.1082003116607666 seconds
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/lightning_fabric/plugins/environments/slurm.py:166: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python train_model.py ...
  rank_zero_warn(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
LightningModel                           [2, 9]                    --
├─CustomizedModel: 1-1                   [2, 9]                    --
│    └─MaxPool1d: 2-1                    [2, 2560, 167]            --
│    └─Dropout: 2-2                      [2, 2560, 167]            --
│    └─Attention_mask: 2-3               [2, 2560, 3]              --
│    │    └─Linear: 3-1                  [2, 167, 80]              204,800
│    │    └─Tanh: 3-2                    [2, 167, 80]              --
│    │    └─Linear: 3-3                  [2, 167, 3]               240
│    └─Flatten: 2-4                      [2, 7680]                 --
│    └─Embedding: 2-5                    [2, 4]                    76
│    └─Linear: 2-6                       [2, 100]                  768,100
│    └─Actvation: 2-7                    [2, 104]                  --
│    └─Dropout: 2-8                      [2, 104]                  --
│    └─Linear: 2-9                       [2, 9]                    945
│    └─Sigmoid: 2-10                     [2, 9]                    --
==========================================================================================
Total params: 974,161
Trainable params: 974,161
Non-trainable params: 0
Total mult-adds (M): 1.95
==========================================================================================
Input size (MB): 27.36
Forward/backward pass size (MB): 0.22
Params size (MB): 3.90
Estimated Total Size (MB): 31.48
==========================================================================================
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------
[2023-12-04 14:27:26,405][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2023-12-04 14:27:26,407][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
  | Name    | Type            | Params
--------------------------------------------
0 | network | CustomizedModel | 974 K
1 | loss_fn | BCELoss         | 0
--------------------------------------------
974 K     Trainable params
0         Non-trainable params
974 K     Total params
3.897     Total estimated model params size (MB)
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Sanity Checking: 0it [00:00, ?it/s]Total time taken for loading one batch: 2.6568210124969482 seconds

Sanity Checking DataLoader 0:  50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                           | 1/2 [00:00<00:00, 24.67it/s]Total time taken for loading one batch: 2.971785068511963 seconds
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val categorical_accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val categorical_accuracy_strict', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val binary_accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (58) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Epoch 0:   0%|                                                                                                                                                                                                                                                  | 0/73 [00:00<?, ?it/s]Total time taken for loading one batch: 2.589564085006714 seconds
Epoch 0:   1%|█▎                                                                                          | 1/73 [00:08<10:27,  8.72s/it, loss=2.75, v_num=airh, train categorical_accuracy_step=0.000, train categorical_accuracy_strict_step=0.000, train binary_accuracy_step=0.432]Total time taken for loading one batch: 2.5724918842315674 seconds
Epoch 0:   3%|██▍                                                                                        | 2/73 [00:17<10:27,  8.84s/it, loss=2.57, v_num=airh, train categorical_accuracy_step=0.359, train categorical_accuracy_strict_step=0.0156, train binary_accuracy_step=0.724]Total time taken for loading one batch: 2.7898662090301514 seconds
Epoch 0:   4%|███▋                                                                                       | 3/73 [00:26<10:19,  8.86s/it, loss=2.44, v_num=airh, train categorical_accuracy_step=0.422, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.771]Total time taken for loading one batch: 2.7230472564697266 seconds

Epoch 0:   5%|████▉                                                                                      | 4/73 [00:35<10:07,  8.80s/it, loss=2.35, v_num=airh, train categorical_accuracy_step=0.391, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.774]Total time taken for loading one batch: 2.731194496154785 seconds
Epoch 0:   7%|██████▎                                                                                     | 5/73 [00:44<09:59,  8.82s/it, loss=2.25, v_num=airh, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.795]Total time taken for loading one batch: 2.4029269218444824 seconds

Epoch 0:   8%|███████▍                                                                                   | 6/73 [00:52<09:47,  8.77s/it, loss=2.15, v_num=airh, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.809]Total time taken for loading one batch: 2.7224159240722656 seconds
Epoch 0:  10%|████████▋                                                                                  | 7/73 [01:01<09:40,  8.79s/it, loss=2.06, v_num=airh, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.780]Total time taken for loading one batch: 2.585876941680908 seconds
Epoch 0:  11%|██████████                                                                                  | 8/73 [01:10<09:34,  8.83s/it, loss=1.98, v_num=airh, train categorical_accuracy_step=0.422, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.792]Total time taken for loading one batch: 3.1408705711364746 seconds
Epoch 0:  12%|███████████▍                                                                                 | 9/73 [01:19<09:28,  8.88s/it, loss=1.9, v_num=airh, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.812]Total time taken for loading one batch: 2.8801581859588623 seconds
Epoch 0:  14%|████████████▎                                                                             | 10/73 [01:29<09:24,  8.95s/it, loss=1.83, v_num=airh, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.786]Total time taken for loading one batch: 2.5122952461242676 seconds

Epoch 0:  15%|█████████████▌                                                                            | 11/73 [01:38<09:16,  8.97s/it, loss=1.76, v_num=airh, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.783]Total time taken for loading one batch: 2.8803176879882812 seconds
[2023-12-04 14:29:28,888][HYDRA] 	#1 : model=base_model task=RNAlocalization embedder=parnet
output dir of this job: /home/sxr280/BERTLocRNA/scripts/../output/RNAlocalization/parnet
Resolving data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 243/243 [00:00<00:00, 395198.09it/s]
Resolving data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 297877.26it/s]
Resolving data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:00<00:00, 184443.98it/s]
Resolving data files:   0%|                                                                                                                                                                                                                                     | 0/77 [00:00<?, ?it/s]
embedding will be saved at: /home/sxr280/BERTLocRNA/embeddings/Parnetembedding
GPU available: True (cuda), used: True                                                                                                                                                                                                                          | 0/77 [00:00<?, ?it/s]
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Total time taken for loading one batch: 0.07238078117370605 seconds
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
LightningModel                           [2, 9]                    --
├─CustomizedModel: 1-1                   [2, 9]                    --
│    └─MaxPool1d: 2-1                    [2, 256, 1000]            --
│    └─Dropout: 2-2                      [2, 256, 1000]            --
│    └─Attention_mask: 2-3               [2, 256, 3]               --
│    │    └─Linear: 3-1                  [2, 1000, 80]             20,480
│    │    └─Tanh: 3-2                    [2, 1000, 80]             --
│    │    └─Linear: 3-3                  [2, 1000, 3]              240
│    └─Flatten: 2-4                      [2, 768]                  --
│    └─Embedding: 2-5                    [2, 4]                    76
│    └─Linear: 2-6                       [2, 100]                  76,900
│    └─Actvation: 2-7                    [2, 104]                  --
│    └─Dropout: 2-8                      [2, 104]                  --
│    └─Linear: 2-9                       [2, 9]                    945
│    └─Sigmoid: 2-10                     [2, 9]                    --
==========================================================================================
Total params: 98,641
Trainable params: 98,641
Non-trainable params: 0
Total mult-adds (M): 0.20
==========================================================================================
Input size (MB): 16.39
Forward/backward pass size (MB): 1.33
Params size (MB): 0.39
Estimated Total Size (MB): 18.12
