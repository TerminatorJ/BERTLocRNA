
/home/sxr280/BERTLocRNA/saved_model/NT  already exists, loading the model locally
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:06<00:00,  3.30s/it]
Some weights of the model checkpoint at /home/sxr280/BERTLocRNA/saved_model/NT were not used when initializing EsmModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']
- This IS expected if you are initializing EsmModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmModel were not initialized from the model checkpoint at /home/sxr280/BERTLocRNA/saved_model/NT and are newly initialized: ['esm.pooler.dense.weight', 'esm.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Old caching folder /home/sxr280/.cache/huggingface/datasets/TerminatorJ___localization_multi_rna/default/0.0.0/831181ef80e0f737 for dataset localization_multi_rna exists but no data were found. Removing it.
[2023-12-04 15:24:20,298][datasets.builder][WARNING] - Old caching folder /home/sxr280/.cache/huggingface/datasets/TerminatorJ___localization_multi_rna/default/0.0.0/831181ef80e0f737 for dataset localization_multi_rna exists but no data were found. Removing it.
loading the dataset...
Downloading data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1636.27it/s]
Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 212.49it/s]
Generating train split: 14650 examples [00:00, 21243.58 examples/s]
Generating test split: 4587 examples [00:00, 23717.46 examples/s]
Generating validation split: 3670 examples [00:00, 18163.52 examples/s]
Resolving data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 296/296 [00:00<00:00, 464638.47it/s]
Resolving data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [00:00<00:00, 198845.01it/s]
Resolving data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:00<00:00, 72917.44it/s]
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
Total time taken for loading one batch: 0.08647894859313965 seconds
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
LightningModel                           [2, 9]                    --
├─CustomizedModel: 1-1                   [2, 9]                    --
│    └─MaxPool1d: 2-1                    [2, 2560, 167]            --
│    └─Dropout: 2-2                      [2, 2560, 167]            --
│    └─Attention_mask: 2-3               [2, 2560, 3]              --
│    │    └─Linear: 3-1                  [2, 167, 80]              204,800
│    │    └─Tanh: 3-2                    [2, 167, 80]              --
│    │    └─Linear: 3-3                  [2, 167, 3]               240
│    └─Flatten: 2-4                      [2, 7680]                 --
│    └─Embedding: 2-5                    [2, 4]                    76
│    └─Linear: 2-6                       [2, 100]                  768,100
│    └─Actvation: 2-7                    [2, 104]                  --
│    └─Dropout: 2-8                      [2, 104]                  --
│    └─Linear: 2-9                       [2, 9]                    945
│    └─Sigmoid: 2-10                     [2, 9]                    --
==========================================================================================
Total params: 974,161
Trainable params: 974,161
Non-trainable params: 0
Total mult-adds (M): 1.95
==========================================================================================
Input size (MB): 27.36
Forward/backward pass size (MB): 0.22
Params size (MB): 3.90
Estimated Total Size (MB): 31.48
==========================================================================================
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/lightning_fabric/plugins/environments/slurm.py:166: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python train_model.py ...
  rank_zero_warn(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
[2023-12-04 15:25:15,866][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2023-12-04 15:25:15,867][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------
Sanity Checking: 0it [00:00, ?it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
  | Name    | Type            | Params
--------------------------------------------
0 | network | CustomizedModel | 974 K
1 | loss_fn | BCELoss         | 0
--------------------------------------------
974 K     Trainable params
0         Non-trainable params
974 K     Total params
3.897     Total estimated model params size (MB)
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
Sanity Checking: 0it [00:00, ?it/s]Total time taken for loading one batch: 0.4409060478210449 seconds
Sanity Checking DataLoader 0:  50%|████████████████████████████████████████████████████████████████████████████████████████████                                                                                            | 1/2 [00:00<00:00, 37.77it/s]Total time taken for loading one batch: 0.7973222732543945 seconds
Epoch 0:   0%|                                                                                                                                                                                                                   | 0/144 [00:00<?, ?it/s]
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val categorical_accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val categorical_accuracy_strict', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val binary_accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (115) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Epoch 0:   0%|                                                                                                                                                                                                                   | 0/144 [00:00<?, ?it/s]Total time taken for loading one batch: 0.745171308517456 seconds
Epoch 0:   1%|▍                                                            | 1/144 [00:03<07:54,  3.32s/it, loss=2.75, v_num=70pv, train categorical_accuracy_step=0.000, train categorical_accuracy_strict_step=0.000, train binary_accuracy_step=0.427]Total time taken for loading one batch: 0.7875049114227295 seconds
Epoch 0:   1%|▊                                                           | 2/144 [00:06<07:41,  3.25s/it, loss=2.64, v_num=70pv, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0156, train binary_accuracy_step=0.693]Total time taken for loading one batch: 0.5719103813171387 seconds
Epoch 0:   2%|█▎                                                          | 3/144 [00:09<07:31,  3.20s/it, loss=2.47, v_num=70pv, train categorical_accuracy_step=0.547, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.774]Total time taken for loading one batch: 0.6132428646087646 seconds
Epoch 0:   3%|█▋                                                           | 4/144 [00:12<07:15,  3.11s/it, loss=2.36, v_num=70pv, train categorical_accuracy_step=0.297, train categorical_accuracy_strict_step=0.109, train binary_accuracy_step=0.769]Total time taken for loading one batch: 0.9461474418640137 seconds
Epoch 0:   3%|██                                                          | 5/144 [00:15<07:21,  3.18s/it, loss=2.25, v_num=70pv, train categorical_accuracy_step=0.453, train categorical_accuracy_strict_step=0.0781, train binary_accuracy_step=0.793]Total time taken for loading one batch: 1.0286948680877686 seconds
Epoch 0:   4%|██▌                                                         | 6/144 [00:19<07:26,  3.24s/it, loss=2.15, v_num=70pv, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0781, train binary_accuracy_step=0.806]Total time taken for loading one batch: 0.9911336898803711 seconds

Epoch 0:   5%|██▉                                                          | 7/144 [00:23<07:30,  3.29s/it, loss=2.07, v_num=70pv, train categorical_accuracy_step=0.422, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.785]Total time taken for loading one batch: 1.1583936214447021 seconds
Epoch 0:   6%|███▌                                                            | 8/144 [00:26<07:33,  3.34s/it, loss=2, v_num=70pv, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.172, train binary_accuracy_step=0.818]Total time taken for loading one batch: 0.8550171852111816 seconds
Epoch 0:   6%|███▊                                                        | 9/144 [00:30<07:35,  3.37s/it, loss=1.92, v_num=70pv, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.804]Total time taken for loading one batch: 0.8444972038269043 seconds
Epoch 0:   7%|████▏                                                       | 10/144 [00:33<07:33,  3.38s/it, loss=1.86, v_num=70pv, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.788]Total time taken for loading one batch: 0.828204870223999 seconds
Epoch 0:   8%|████▌                                                       | 11/144 [00:37<07:32,  3.41s/it, loss=1.79, v_num=70pv, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.141, train binary_accuracy_step=0.793]Total time taken for loading one batch: 1.080397605895996 seconds
Epoch 0:   8%|████▉                                                      | 12/144 [00:40<07:29,  3.41s/it, loss=1.72, v_num=70pv, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.769]Total time taken for loading one batch: 0.8846914768218994 seconds
Epoch 0:   9%|█████▎                                                     | 13/144 [00:44<07:27,  3.41s/it, loss=1.66, v_num=70pv, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.800]Total time taken for loading one batch: 0.2972722053527832 seconds
Epoch 0:  10%|█████▉                                                       | 14/144 [00:47<07:24,  3.42s/it, loss=1.6, v_num=70pv, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.109, train binary_accuracy_step=0.818]Total time taken for loading one batch: 0.9285552501678467 seconds
Epoch 0:  10%|██████▏                                                    | 15/144 [00:51<07:23,  3.44s/it, loss=1.54, v_num=70pv, train categorical_accuracy_step=0.359, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.795]Total time taken for loading one batch: 0.8109581470489502 seconds
Epoch 0:  11%|██████▋                                                     | 16/144 [00:55<07:22,  3.45s/it, loss=1.49, v_num=70pv, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.172, train binary_accuracy_step=0.828]Total time taken for loading one batch: 1.0454564094543457 seconds
Epoch 0:  12%|███████                                                     | 17/144 [00:59<07:21,  3.48s/it, loss=1.45, v_num=70pv, train categorical_accuracy_step=0.422, train categorical_accuracy_strict_step=0.109, train binary_accuracy_step=0.818]Total time taken for loading one batch: 0.7982363700866699 seconds
Epoch 0:  12%|███████▋                                                     | 18/144 [01:02<07:18,  3.48s/it, loss=1.4, v_num=70pv, train categorical_accuracy_step=0.578, train categorical_accuracy_strict_step=0.141, train binary_accuracy_step=0.847]Total time taken for loading one batch: 1.1968848705291748 seconds
Epoch 0:  13%|███████▊                                                   | 19/144 [01:06<07:16,  3.49s/it, loss=1.36, v_num=70pv, train categorical_accuracy_step=0.453, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.818]Total time taken for loading one batch: 0.5349256992340088 seconds
Epoch 0:  14%|████████▎                                                   | 20/144 [01:10<07:14,  3.50s/it, loss=1.33, v_num=70pv, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.234, train binary_accuracy_step=0.844]Total time taken for loading one batch: 0.5015990734100342 seconds
Epoch 0:  15%|████████▊                                                   | 21/144 [01:13<07:10,  3.50s/it, loss=1.22, v_num=70pv, train categorical_accuracy_step=0.516, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.835]Total time taken for loading one batch: 0.6865639686584473 seconds


Epoch 0:  15%|█████████▏                                                  | 22/144 [01:16<07:05,  3.49s/it, loss=1.13, v_num=70pv, train categorical_accuracy_step=0.297, train categorical_accuracy_strict_step=0.141, train binary_accuracy_step=0.819]Total time taken for loading one batch: 1.0267610549926758 seconds
[2023-12-04 15:26:44,951][HYDRA] 	#1 : model=base_model task=RNAlocalization embedder=parnet
output dir of this job: /home/sxr280/BERTLocRNA/scripts/../output/RNAlocalization/parnet
embedding will be saved at: /home/sxr280/BERTLocRNA/embeddings/Parnetembedding
loading the dataset...
[2023-12-04 15:26:47,191][datasets.builder][WARNING] - Old caching folder /home/sxr280/.cache/huggingface/datasets/parnetembedding/default-291d860c6e5bd1a7/0.0.0/74f69db2c14c2860059d39860b1f400a03d11bf7fb5a8258ca38c501c878c137 for dataset parnetembedding exists but no data were found. Removing it.
Resolving data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 243/243 [00:00<00:00, 450382.62it/s]
Resolving data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 282659.62it/s]
Resolving data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:00<00:00, 280348.44it/s]
Old caching folder /home/sxr280/.cache/huggingface/datasets/parnetembedding/default-291d860c6e5bd1a7/0.0.0/74f69db2c14c2860059d39860b1f400a03d11bf7fb5a8258ca38c501c878c137 for dataset parnetembedding exists but no data were found. Removing it. it/s]
Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 642.12it/s]
Downloading data files:   0%|                                                                                                                                                                                                      | 0/3 [00:00<?, ?it/s]
Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]













































































































































































































Generating train split: 14650 examples [08:05, 39.96 examples/s]














































Generating validation split: 3670 examples [01:53, 33.18 examples/s]





























































Generating test split: 4587 examples [02:28, 30.87 examples/s]
Total time taken for loading one batch: 0.059900760650634766 seconds
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
LightningModel                           [2, 9]                    --
├─CustomizedModel: 1-1                   [2, 9]                    --
│    └─MaxPool1d: 2-1                    [2, 256, 1000]            --
│    └─Dropout: 2-2                      [2, 256, 1000]            --
│    └─Attention_mask: 2-3               [2, 256, 3]               --
│    │    └─Linear: 3-1                  [2, 1000, 80]             20,480
│    │    └─Tanh: 3-2                    [2, 1000, 80]             --
│    │    └─Linear: 3-3                  [2, 1000, 3]              240
│    └─Flatten: 2-4                      [2, 768]                  --
│    └─Embedding: 2-5                    [2, 4]                    76
│    └─Linear: 2-6                       [2, 100]                  76,900
│    └─Actvation: 2-7                    [2, 104]                  --
│    └─Dropout: 2-8                      [2, 104]                  --
│    └─Linear: 2-9                       [2, 9]                    945
│    └─Sigmoid: 2-10                     [2, 9]                    --
==========================================================================================
Total params: 98,641
Trainable params: 98,641
Non-trainable params: 0
Total mult-adds (M): 0.20
==========================================================================================
Input size (MB): 16.39
Forward/backward pass size (MB): 1.33
Params size (MB): 0.39
Estimated Total Size (MB): 18.12
GPU available: True (cuda), used: True02:28, 30.87 examples/s]
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
