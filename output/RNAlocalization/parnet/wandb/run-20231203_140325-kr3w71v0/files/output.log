ok
/home/sxr280/BERTLocRNA/scripts
embedding will be saved at: /home/sxr280/BERTLocRNA/embeddings/Parnetembedding
loading the dataset...
self.config {'accelerator': 'gpu', 'strategy': 'ddp', 'max_epochs': 500, 'precison': 32, 'num_nodes': 1, 'log_every_n_steps': 1000, 'patience': 20, 'optimizer_cls': 'torch.optim.Adam', 'lr': 0.001, 'weight_decay': 1e-05, 'nb_classes': '${nb_classes}', 'output_dir': '${output_dir}', 'RNA_order': '${base_model.config.RNA_order}'}
class weights: None
['>110100000,Gene_ID:NCBI:64240,Refseq_ID:NM_022436,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>110000100,Gene_ID:NCBI:2055,Refseq_ID:NM_001034061,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>110000000,Gene_ID:NCBI:91683,Refseq_ID:NM_001177880,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010000000,Gene_ID:NCBI:728409,Refseq_ID:NR_024102.1,Species:Homo sapiens,RNA_category:lncRNA,Source:AllSeq', '>010110000,Gene_ID:NCBI:85014,Refseq_ID:NM_032928,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>111100100,Gene_ID:NCBI:9249,Refseq_ID:NM_001319225,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010000100,Gene_ID:NCBI:54463,Refseq_ID:NM_001034850,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>110000000,Gene_ID:NCBI:646600,Refseq_ID:NR_126326.1,Species:Homo sapiens,RNA_category:lncRNA,Source:AllSeq'] ['110100000', '110000100', '110000000', '010000000', '010110000', '111100100', '010000100', '110000000']
embeddings after padding: torch.Size([8, 256, 8000])
embeddings after transport: torch.Size([8, 256, 8000]) tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [3.4964, 3.6949, 3.8419,  ..., 3.3819, 3.4362, 3.6356]],
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [2.5011, 2.2057, 2.0561,  ..., 3.3819, 3.4362, 3.6356]],
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [4.0020, 3.4293, 3.9709,  ..., 3.3819, 3.4362, 3.6356]],
        ...,
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [2.6550, 3.4590, 3.3410,  ..., 3.3819, 3.4362, 3.6356]],
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [2.2513, 3.2927, 3.2541,  ..., 3.3819, 3.4362, 3.6356]],
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [3.0500, 2.8008, 4.2499,  ..., 3.3819, 3.4362, 3.6356]]])
self.tokenizer.pad_token_id 0
masks [tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1])]
masks after padding tensor([[[1, 1, 1,  ..., 1, 1, 1]],
        [[1, 1, 1,  ..., 1, 1, 1]],
        [[1, 1, 1,  ..., 1, 1, 1]],
        ...,
        [[1, 1, 1,  ..., 1, 1, 1]],
        [[1, 1, 1,  ..., 1, 1, 1]],
        [[1, 1, 1,  ..., 1, 1, 1]]])
labels: ['110100000', '110000100', '110000000', '010000000', '010110000', '111100100', '010000100', '110000000']
int labels [tensor([1, 1, 0, 1, 0, 0, 0, 0, 0]), tensor([1, 1, 0, 0, 0, 0, 1, 0, 0]), tensor([1, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 1, 1, 0, 0, 0, 0]), tensor([1, 1, 1, 1, 0, 0, 1, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 1, 0, 0]), tensor([1, 1, 0, 0, 0, 0, 0, 0, 0])]
int labels tensor([[1, 1, 0, 1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0]])
embed: torch.Size([2, 256, 8000])
mask: torch.Size([2, 1, 8000])
input embedding shape torch.Size([2, 256, 8000])
embed_output shape: torch.Size([2, 256, 8000])
mask shape torch.Size([2, 1, 8000])
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
LightningModel                           [2, 9]                    --
├─CustomizedModel: 1-1                   [2, 9]                    --
│    └─Attention_mask: 2-1               [2, 256, 3]               20,720
│    │    └─Tanh: 3-1                    [2, 80, 8000]             --
│    └─Flatten: 2-2                      [2, 768]                  --
│    └─Embedding: 2-3                    [2, 4]                    76
│    └─Linear: 2-4                       [2, 100]                  76,900
│    └─Actvation: 2-5                    [2, 104]                  --
│    └─Dropout: 2-6                      [2, 104]                  --
│    └─Linear: 2-7                       [2, 9]                    945
│    └─Sigmoid: 2-8                      [2, 9]                    --
==========================================================================================
Total params: 98,641
Trainable params: 98,641
Non-trainable params: 0
Total mult-adds (M): 0.16
==========================================================================================
Input size (MB): 16.45
Forward/backward pass size (MB): 0.00
Params size (MB): 0.31
Estimated Total Size (MB): 16.76
==========================================================================================
[2023-12-03 14:03:39,416][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2023-12-03 14:03:39,427][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
Resolving data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 243/243 [00:00<00:00, 476046.65it/s]
Resolving data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 232392.18it/s]
Resolving data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:00<00:00, 304106.79it/s]
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
['>010000000,Gene_ID:NCBI:150084,Refseq_ID:NM_001080444,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010000000,Gene_ID:NCBI:4599,Refseq_ID:NM_001144925,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010000000,Gene_ID:NCBI:1415,Refseq_ID:NM_000496,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010000000,Gene_ID:NCBI:23209,Refseq_ID:NM_015166,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010000000,Gene_ID:NCBI:3976,Refseq_ID:NM_001257135,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010000000,Gene_ID:NCBI:64098,Refseq_ID:NM_001137605,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010000000,Gene_ID:NCBI:83733,Refseq_ID:NM_001303484,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010000000,Gene_ID:NCBI:1232,Refseq_ID:NM_001164680,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc'] ['010000000', '010000000', '010000000', '010000000', '010000000', '010000000', '010000000', '010000000']
embeddings after padding: torch.Size([8, 256, 8000])
embeddings after transport: torch.Size([8, 256, 8000]) tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [2.3009, 2.5190, 2.5758,  ..., 3.3819, 3.4362, 3.6356]],
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [2.9562, 3.4302, 3.0793,  ..., 3.3819, 3.4362, 3.6356]],
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [2.3645, 2.4410, 2.8442,  ..., 3.3819, 3.4362, 3.6356]],
        ...,
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [3.4340, 4.1001, 3.3522,  ..., 3.3819, 3.4362, 3.6356]],
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [2.2701, 2.5931, 2.6464,  ..., 3.3819, 3.4362, 3.6356]],
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [2.7968, 2.3128, 3.3220,  ..., 3.3819, 3.4362, 3.6356]]])
self.tokenizer.pad_token_id 0
masks [tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1])]
masks after padding tensor([[[1, 1, 1,  ..., 1, 1, 1]],
        [[1, 1, 1,  ..., 1, 1, 1]],
        [[1, 1, 1,  ..., 1, 1, 1]],
        ...,
        [[1, 1, 1,  ..., 1, 1, 1]],
        [[1, 1, 1,  ..., 1, 1, 1]],
        [[1, 1, 1,  ..., 1, 1, 1]]])
labels: ['010000000', '010000000', '010000000', '010000000', '010000000', '010000000', '010000000', '010000000']
int labels [tensor([0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0])]
int labels tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0]])
input embedding shape torch.Size([8, 256, 8000])
embed_output shape: torch.Size([8, 256, 8000])
mask shape torch.Size([8, 1, 8000])
y_pred: tensor([[0.4419, 0.8438, 0.4473, 0.4465, 0.2915, 0.5588, 0.4627, 0.4930, 0.5920],
        [0.4856, 0.8180, 0.5065, 0.4283, 0.2993, 0.5103, 0.4639, 0.4511, 0.5992],
        [0.4134, 0.8699, 0.3874, 0.4442, 0.2949, 0.5876, 0.4909, 0.5478, 0.5938],
        [0.4818, 0.8204, 0.5117, 0.4412, 0.2857, 0.5217, 0.4438, 0.4476, 0.6036],
        [0.4714, 0.8163, 0.5548, 0.4521, 0.2817, 0.5220, 0.4289, 0.4374, 0.6122],
        [0.4759, 0.8200, 0.5244, 0.4406, 0.2974, 0.5249, 0.4430, 0.4587, 0.5978],
        [0.4461, 0.8455, 0.4468, 0.4449, 0.2880, 0.5539, 0.4634, 0.4944, 0.5938],
        [0.4285, 0.8523, 0.4167, 0.4446, 0.2934, 0.5650, 0.4719, 0.5079, 0.5878]],
       device='cuda:1')
y_true: tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:1')
['>010000000,Gene_ID:NCBI:151827,Refseq_ID:NM_001172779,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010000000,Gene_ID:NCBI:165721,Refseq_ID:NM_153330,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010000000,Gene_ID:NCBI:23150,Refseq_ID:NM_015123,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010000000,Gene_ID:NCBI:27074,Refseq_ID:NM_014398,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010000000,Gene_ID:NCBI:339855,Refseq_ID:NM_001350859,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010000000,Gene_ID:NCBI:3568,Refseq_ID:NM_000564,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010000000,Gene_ID:NCBI:53829,Refseq_ID:NM_023914,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010000000,Gene_ID:NCBI:5745,Refseq_ID:NM_000316,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc'] ['010000000', '010000000', '010000000', '010000000', '010000000', '010000000', '010000000', '010000000']
embeddings after padding: torch.Size([8, 256, 8000])
embeddings after transport: torch.Size([8, 256, 8000]) tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [2.8335, 3.3781, 3.4546,  ..., 3.3819, 3.4362, 3.6356]],
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [3.1877, 3.3483, 3.8125,  ..., 3.3819, 3.4362, 3.6356]],
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [2.7312, 2.6831, 2.7440,  ..., 3.3819, 3.4362, 3.6356]],
        ...,
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [2.7497, 3.1192, 2.7920,  ..., 3.3819, 3.4362, 3.6356]],
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [2.8443, 2.6917, 2.6066,  ..., 3.3819, 3.4362, 3.6356]],
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [3.4630, 3.2035, 3.6621,  ..., 3.3819, 3.4362, 3.6356]]])
self.tokenizer.pad_token_id 0
masks [tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1])]
masks after padding tensor([[[1, 1, 1,  ..., 1, 1, 1]],
        [[1, 1, 1,  ..., 1, 1, 1]],
        [[1, 1, 1,  ..., 1, 1, 1]],
        ...,
        [[1, 1, 1,  ..., 1, 1, 1]],
        [[1, 1, 1,  ..., 1, 1, 1]],
        [[1, 1, 1,  ..., 1, 1, 1]]])
labels: ['010000000', '010000000', '010000000', '010000000', '010000000', '010000000', '010000000', '010000000']
int labels [tensor([0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0])]
int labels tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0]])
input embedding shape torch.Size([8, 256, 8000])
embed_output shape: torch.Size([8, 256, 8000])
mask shape torch.Size([8, 1, 8000])
y_pred: tensor([[0.4363, 0.8400, 0.4817, 0.4636, 0.2931, 0.5603, 0.4403, 0.4773, 0.5938],
        [0.4584, 0.8443, 0.4590, 0.4415, 0.2918, 0.5396, 0.4691, 0.4800, 0.5838],
        [0.4917, 0.7546, 0.6690, 0.4341, 0.3318, 0.4458, 0.4458, 0.3791, 0.6367],
        [0.4395, 0.8294, 0.5161, 0.4600, 0.2974, 0.5296, 0.4384, 0.4500, 0.5928],
        [0.5020, 0.7778, 0.6125, 0.4240, 0.3133, 0.4661, 0.4561, 0.3939, 0.6143],
        [0.4641, 0.7667, 0.6601, 0.4524, 0.3119, 0.4812, 0.4282, 0.3875, 0.6329],
        [0.4334, 0.8347, 0.5093, 0.4722, 0.2924, 0.5477, 0.4360, 0.4566, 0.5949],
        [0.4701, 0.8461, 0.4368, 0.4254, 0.2944, 0.5436, 0.4807, 0.4977, 0.5872]],
       device='cuda:1')
y_true: tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:1')
['>110000000,Gene_ID:NCBI:147807,Refseq_ID:NM_153219,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010000100,Gene_ID:NCBI:1960,Refseq_ID:NM_001199880,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010000100,Gene_ID:NCBI:643834,Refseq_ID:NM_001079807,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>110000000,Gene_ID:NCBI:79831,Refseq_ID:NM_001145348,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>110110000,Gene_ID:NCBI:6619,Refseq_ID:NM_001039697,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>110000000,Gene_ID:NCBI:115557,Refseq_ID:NM_001111270,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010100000,Gene_ID:NCBI:196446,Refseq_ID:NM_182530,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc', '>010000100,Gene_ID:NCBI:81557,Refseq_ID:NM_001242362,Species:Homo sapiens,RNA_category:mRNA,Source:DM3Loc'] ['110000000', '010000100', '010000100', '110000000', '110110000', '110000000', '010100000', '010000100']
embeddings after padding: torch.Size([8, 256, 8000])
embeddings after transport: torch.Size([8, 256, 8000]) tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [3.5927, 3.8422, 3.4720,  ..., 3.3819, 3.4362, 3.6356]],
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [2.3715, 1.5872, 1.4446,  ..., 3.3819, 3.4362, 3.6356]],
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [2.3699, 2.3962, 3.1421,  ..., 3.3819, 3.4362, 3.6356]],
        ...,
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [3.6042, 4.1785, 4.2627,  ..., 3.3819, 3.4362, 3.6356]],
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [2.8507, 3.7454, 4.0536,  ..., 3.3819, 3.4362, 3.6356]],
        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         ...,
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
         [2.8725, 2.9572, 2.7988,  ..., 3.3819, 3.4362, 3.6356]]])
self.tokenizer.pad_token_id 0
masks [tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor([1, 1, 1,  ..., 1, 1, 1])]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
masks after padding tensor([[[1, 1, 1,  ..., 1, 1, 1]],
        [[1, 1, 1,  ..., 1, 1, 1]],
        [[1, 1, 1,  ..., 1, 1, 1]],
        ...,
        [[1, 1, 1,  ..., 1, 1, 1]],
        [[1, 1, 1,  ..., 1, 1, 1]],
        [[1, 1, 1,  ..., 1, 1, 1]]])
labels: ['110000000', '010000100', '010000100', '110000000', '110110000', '110000000', '010100000', '010000100']
int labels [tensor([1, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 1, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 1, 0, 0]), tensor([1, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([1, 1, 0, 1, 1, 0, 0, 0, 0]), tensor([1, 1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 1, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 1, 0, 0])]
int labels tensor([[1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 1, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 1, 0, 0]])
input embedding shape torch.Size([8, 256, 8000])
embed_output shape: torch.Size([8, 256, 8000])
mask shape torch.Size([8, 1, 8000])
[2023-12-03 14:03:44,980][HYDRA] 	#1 : model=base_model task=RNAlocalization embedder=nucleotidetransformer
output dir of this job: /home/sxr280/BERTLocRNA/scripts/../output/RNAlocalization/nucleotidetransformer
/home/sxr280/BERTLocRNA/saved_model/NT  already exists, loading the model locally
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.63s/it]
Some weights of the model checkpoint at /home/sxr280/BERTLocRNA/saved_model/NT were not used when initializing EsmModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']
- This IS expected if you are initializing EsmModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmModel were not initialized from the model checkpoint at /home/sxr280/BERTLocRNA/saved_model/NT and are newly initialized: ['esm.pooler.dense.weight', 'esm.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Resolving data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 296/296 [00:00<00:00, 183873.52it/s]
Resolving data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [00:00<00:00, 269326.03it/s]
Resolving data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:00<00:00, 215209.92it/s]
Error executing job with overrides: ['model=base_model', 'task=RNAlocalization', 'embedder=parnet']
Traceback (most recent call last):
  File "/home/sxr280/BERTLocRNA/scripts/train_model.py", line 55, in <module>
    train()
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/hydra/main.py", line 90, in decorated_main
    _run_hydra(
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/hydra/_internal/utils.py", line 465, in _run_app
    run_and_report(
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/hydra/_internal/utils.py", line 222, in run_and_report
    raise ex
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/hydra/_internal/utils.py", line 219, in run_and_report
    return func()
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/hydra/_internal/utils.py", line 466, in <lambda>
    lambda: hydra.multirun(
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 162, in multirun
    ret = sweeper.sweep(arguments=task_overrides)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/hydra/_internal/core_plugins/basic_sweeper.py", line 182, in sweep
    _ = r.return_value
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/sxr280/BERTLocRNA/scripts/train_model.py", line 49, in train
    Trainer.train(train_dataloader, eval_dataloader)
  File "/home/sxr280/BERTLocRNA/scripts/../utils/trainer.py", line 247, in train
    trainer.fit(self.plmodel, train_loader, val_loader)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 608, in fit
    call._call_and_handle_interrupt(
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py", line 38, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1112, in _run
    results = self._run_stage()
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1191, in _run_stage
    self._run_train()
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1214, in _run_train
    self.fit_loop.run()
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py", line 199, in run
    self.advance(*args, **kwargs)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 267, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py", line 199, in run
    self.advance(*args, **kwargs)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 213, in advance
    batch_output = self.batch_loop.run(kwargs)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py", line 199, in run
    self.advance(*args, **kwargs)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(optimizers, kwargs)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py", line 199, in run
    self.advance(*args, **kwargs)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 202, in advance
    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 249, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, kwargs.get("batch_idx", 0), closure)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 370, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1356, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/core/module.py", line 1742, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 169, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 280, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, opt_idx, closure, model, **kwargs)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 234, in optimizer_step
    return self.precision_plugin.optimizer_step(
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 119, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/torch/optim/adam.py", line 183, in step
    loss = closure()
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 105, in _wrap_closure
    closure_result = closure()
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 149, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 135, in closure
    step_output = self._step_fn()
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 419, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *kwargs.values())
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1494, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 351, in training_step
    return self.model(*args, **kwargs)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py", line 98, in forward
    output = self._forward_module.training_step(*inputs, **kwargs)
  File "/home/sxr280/BERTLocRNA/scripts/../utils/trainer.py", line 149, in training_step
    if self.config["gradient_clip"]:
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/omegaconf/dictconfig.py", line 371, in __getitem__
    self._format_and_raise(
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/omegaconf/base.py", line 231, in _format_and_raise
    format_and_raise(
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/omegaconf/_utils.py", line 819, in format_and_raise
    _raise(ex, cause)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/omegaconf/_utils.py", line 797, in _raise
    raise ex.with_traceback(sys.exc_info()[2])  # set env var OC_CAUSE=1 for full trace
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/omegaconf/dictconfig.py", line 369, in __getitem__
    return self._get_impl(key=key, default_value=_DEFAULT_MARKER_)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/omegaconf/dictconfig.py", line 442, in _get_impl
    node = self._get_child(
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/omegaconf/basecontainer.py", line 73, in _get_child
    child = self._get_node(
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/omegaconf/dictconfig.py", line 475, in _get_node
    self._validate_get(key)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/omegaconf/dictconfig.py", line 164, in _validate_get
    self._format_and_raise(
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/omegaconf/base.py", line 231, in _format_and_raise
    format_and_raise(
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/omegaconf/_utils.py", line 899, in format_and_raise
    _raise(ex, cause)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/omegaconf/_utils.py", line 797, in _raise
    raise ex.with_traceback(sys.exc_info()[2])  # set env var OC_CAUSE=1 for full trace
omegaconf.errors.ConfigKeyError: Key 'gradient_clip' is not in struct
    full_key: Trainer.config.gradient_clip
    object_type=dict
loading the dataset...