ok
/home/sxr280/BERTLocRNA/scripts
main function initialization:
name of the model parameters: fc1.weight
name of the model parameters: fc1.bias
name of the model parameters: fc2.weight
name of the model parameters: fc2.bias
name of the model parameters: embedding_layer.weight
name of the model parameters: Attention_layer.W1.weight
name of the model parameters: Attention_layer.W2.weight
embedding will be saved at: /home/sxr280/BERTLocRNA/embeddings/Parnetembedding
loading the dataset...
Resolving data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 243/243 [00:00<00:00, 297060.88it/s]
Resolving data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 204439.35it/s]
Resolving data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:00<00:00, 230193.45it/s]
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
Total time taken for loading one batch: 1.7236394882202148 seconds
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
LightningModel                           [2, 9]                    --
├─CustomizedModel: 1-1                   [2, 9]                    --
│    └─MaxPool1d: 2-1                    [2, 256, 1000]            --
│    └─Dropout: 2-2                      [2, 256, 1000]            --
│    └─Attention_mask: 2-3               [2, 256, 3]               --
│    │    └─Linear: 3-1                  [2, 1000, 80]             20,480
│    │    └─Tanh: 3-2                    [2, 1000, 80]             --
│    │    └─Linear: 3-3                  [2, 1000, 3]              240
│    └─Flatten: 2-4                      [2, 768]                  --
│    └─Embedding: 2-5                    [2, 4]                    76
│    └─Linear: 2-6                       [2, 100]                  76,900
│    └─Actvation: 2-7                    [2, 104]                  --
│    └─Dropout: 2-8                      [2, 104]                  --
│    └─Linear: 2-9                       [2, 9]                    945
│    └─Sigmoid: 2-10                     [2, 9]                    --
==========================================================================================
Total params: 98,641
Trainable params: 98,641
Non-trainable params: 0
Total mult-adds (M): 0.20
==========================================================================================
Input size (MB): 16.39
Forward/backward pass size (MB): 1.33
Params size (MB): 0.39
Estimated Total Size (MB): 18.12
==========================================================================================
[2023-12-04 13:39:46,740][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2023-12-04 13:39:46,771][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
Total time taken for loading one batch: 2.4138741493225098 seconds
Total time taken for loading one batch: 4.1815948486328125 seconds
Total time taken for loading one batch: 2.2354884147644043 seconds
Total time taken for loading one batch: 2.417861223220825 seconds
Total time taken for loading one batch: 3.1456427574157715 seconds
Total time taken for loading one batch: 2.1890687942504883 seconds
Total time taken for loading one batch: 2.2880139350891113 seconds
Total time taken for loading one batch: 2.1593995094299316 seconds
Total time taken for loading one batch: 2.525233745574951 seconds
Total time taken for loading one batch: 2.770965814590454 seconds
Total time taken for loading one batch: 2.646528959274292 seconds
Total time taken for loading one batch: 2.337127447128296 seconds
Total time taken for loading one batch: 2.384598731994629 seconds
Total time taken for loading one batch: 2.432954788208008 seconds
Total time taken for loading one batch: 2.780569076538086 seconds
Total time taken for loading one batch: 2.3327035903930664 seconds
Total time taken for loading one batch: 3.2592618465423584 seconds
Total time taken for loading one batch: 2.6815407276153564 seconds
Total time taken for loading one batch: 2.8937888145446777 seconds
Total time taken for loading one batch: 3.326112985610962 seconds
Total time taken for loading one batch: 2.897554874420166 seconds
Total time taken for loading one batch: 2.4169623851776123 seconds
Total time taken for loading one batch: 2.966583490371704 seconds
Total time taken for loading one batch: 2.4104034900665283 seconds
Total time taken for loading one batch: 3.00765061378479 seconds
Traceback (most recent call last):
  File "/home/sxr280/BERTLocRNA/scripts/train_model.py", line 57, in <module>
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/hydra/main.py", line 90, in decorated_main
    _run_hydra(
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/hydra/_internal/utils.py", line 465, in _run_app
    run_and_report(
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/hydra/_internal/utils.py", line 219, in run_and_report
    return func()
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/hydra/_internal/utils.py", line 466, in <lambda>
    lambda: hydra.multirun(
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 162, in multirun
    ret = sweeper.sweep(arguments=task_overrides)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/hydra/_internal/core_plugins/basic_sweeper.py", line 178, in sweep
    results = self.launcher.launch(batch, initial_job_idx=initial_job_idx)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/hydra/_internal/core_plugins/basic_launcher.py", line 74, in launch
    ret = run_job(
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/sxr280/BERTLocRNA/scripts/train_model.py", line 54, in train
    train()
  File "/home/sxr280/BERTLocRNA/scripts/../utils/trainer.py", line 267, in test
    self.get_metrics_all(test_loader)
  File "/home/sxr280/BERTLocRNA/scripts/../utils/trainer.py", line 274, in get_metrics_all
    for batch in dataloader_test:
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 628, in __next__
    data = self._next_data()
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 671, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 56, in fetch
    data = self.dataset.__getitems__(possibly_batched_index)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 2799, in __getitems__
    batch = self.__getitem__(keys)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 2795, in __getitem__
    return self._getitem(key)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 2780, in _getitem
    formatted_output = format_table(
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/formatting/formatting.py", line 629, in format_table
    return formatter(pa_table, query_type=query_type)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/formatting/formatting.py", line 400, in __call__
    return self.format_batch(pa_table)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py", line 102, in format_batch
    batch = self.recursive_tensorize(batch)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py", line 85, in recursive_tensorize
    return map_nested(self._recursive_tensorize, data_struct, map_list=False)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/utils/py_utils.py", line 464, in map_nested
    mapped = [
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/utils/py_utils.py", line 465, in <listcomp>
    _single_map_nested((function, obj, types, None, True, None))
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/utils/py_utils.py", line 367, in _single_map_nested
    return function(data_struct)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py", line 79, in _recursive_tensorize
    return self._consolidate([self.recursive_tensorize(substruct) for substruct in data_struct])
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py", line 79, in <listcomp>
    return self._consolidate([self.recursive_tensorize(substruct) for substruct in data_struct])
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py", line 85, in recursive_tensorize
    return map_nested(self._recursive_tensorize, data_struct, map_list=False)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/utils/py_utils.py", line 456, in map_nested
    return function(data_struct)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py", line 79, in _recursive_tensorize
    return self._consolidate([self.recursive_tensorize(substruct) for substruct in data_struct])
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py", line 79, in <listcomp>
    return self._consolidate([self.recursive_tensorize(substruct) for substruct in data_struct])
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py", line 85, in recursive_tensorize
    return map_nested(self._recursive_tensorize, data_struct, map_list=False)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/utils/py_utils.py", line 456, in map_nested
    return function(data_struct)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py", line 82, in _recursive_tensorize
    return self._tensorize(data_struct)
  File "/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py", line 68, in _tensorize
    return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})
KeyboardInterrupt