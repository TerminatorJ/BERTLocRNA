embedding will be saved at: /home/sxr280/BERTLocRNA/embeddings/Parnetembedding
loading the dataset...
Resolving data files: 100%|███████████████████████████████████████████████████████| 243/243 [00:00<00:00, 393064.35it/s]
Resolving data files: 100%|█████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 170634.41it/s]
Resolving data files: 100%|█████████████████████████████████████████████████████████| 77/77 [00:00<00:00, 293068.43it/s]
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/lightning_fabric/plugins/environments/slurm.py:166: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python train_model.py ...
  rank_zero_warn(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
LightningModel                           [2, 9]                    --
├─CustomizedModel: 1-1                   [2, 9]                    --
│    └─MaxPool1d: 2-1                    [2, 256, 1000]            --
│    └─Dropout: 2-2                      [2, 256, 1000]            --
│    └─Attention_mask: 2-3               [2, 256, 3]               --
│    │    └─Linear: 3-1                  [2, 1000, 80]             20,480
│    │    └─Tanh: 3-2                    [2, 1000, 80]             --
│    │    └─Linear: 3-3                  [2, 1000, 3]              240
│    └─Flatten: 2-4                      [2, 768]                  --
│    └─Embedding: 2-5                    [2, 4]                    76
│    └─Linear: 2-6                       [2, 100]                  76,900
│    └─Actvation: 2-7                    [2, 104]                  --
│    └─Dropout: 2-8                      [2, 104]                  --
│    └─Linear: 2-9                       [2, 9]                    945
│    └─Sigmoid: 2-10                     [2, 9]                    --
==========================================================================================
Total params: 98,641
Trainable params: 98,641
Non-trainable params: 0
Total mult-adds (M): 0.20
==========================================================================================
Input size (MB): 16.39
Forward/backward pass size (MB): 1.33
Params size (MB): 0.39
Estimated Total Size (MB): 18.12
==========================================================================================
[2023-12-05 16:04:15,832][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2023-12-05 16:04:15,832][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
Sanity Checking: 0it [00:00, ?it/s]
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/sxr280/BERTLocRNA/output/RNAlocalization/parnet/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
  | Name    | Type            | Params
--------------------------------------------
0 | network | CustomizedModel | 98.6 K
1 | loss_fn | BCELoss         | 0
--------------------------------------------
98.6 K    Trainable params
0         Non-trainable params
98.6 K    Total params
0.395     Total estimated model params size (MB)
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.


Epoch 0:   0%|                                                                                  | 0/144 [00:00<?, ?it/s]
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val categorical_accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val categorical_accuracy_strict', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val binary_accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (115) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.




















































































































Epoch 0:  80%|▊| 115/144 [07:21<01:51,  3.84s/it, loss=0.349, v_num=msub, train categorical_accuracy_step=0.414, train c




























Validation DataLoader 0:  93%|██████████████████████████████████████████████████████    | 27/29 [01:04<00:04,  2.41s/it]
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('train categorical_accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('train categorical_accuracy_strict', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('train binary_accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
[rank: 0] Metric val_loss improved. New best score: 0.342
Epoch 0: 100%|█| 144/144 [08:33<00:00,  3.57s/it, loss=0.349, v_num=msub, train categorical_accuracy_step=0.414, train cEpoch duration: 513.84 seconds




















































































































Epoch 1:  80%|▊| 115/144 [06:49<01:43,  3.56s/it, loss=0.336, v_num=msub, train categorical_accuracy_step=0.483, train c




























Validation DataLoader 0:  97%|████████████████████████████████████████████████████████  | 28/29 [01:06<00:02,  2.39s/it]
[rank: 0] Metric val_loss improved by 0.011 >= min_delta = 0.0. New best score: 0.331

Epoch 1: 100%|█| 144/144 [08:00<00:00,  3.33s/it, loss=0.336, v_num=msub, train categorical_accuracy_step=0.483, train cEpoch duration: 480.60 seconds






















































































































Epoch 2:  80%|▊| 115/144 [06:43<01:41,  3.51s/it, loss=0.334, v_num=msub, train categorical_accuracy_step=0.414, train c



























Validation DataLoader 0:  97%|████████████████████████████████████████████████████████  | 28/29 [01:04<00:02,  2.30s/it]
[rank: 0] Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.330

Epoch 2: 100%|█| 144/144 [07:53<00:00,  3.29s/it, loss=0.334, v_num=msub, train categorical_accuracy_step=0.414, train cEpoch duration: 474.68 seconds





















































































































Epoch 3:  80%|▊| 115/144 [06:40<01:40,  3.48s/it, loss=0.326, v_num=msub, train categorical_accuracy_step=0.310, train c





























Validation DataLoader 0:  97%|████████████████████████████████████████████████████████  | 28/29 [01:06<00:02,  2.37s/it]
[rank: 0] Metric val_loss improved by 0.011 >= min_delta = 0.0. New best score: 0.319
Epoch 3: 100%|█| 144/144 [07:50<00:00,  3.27s/it, loss=0.326, v_num=msub, train categorical_accuracy_step=0.310, train cEpoch duration: 471.12 seconds



















































































































Epoch 4:  80%|▊| 115/144 [06:47<01:42,  3.54s/it, loss=0.316, v_num=msub, train categorical_accuracy_step=0.345, train c




























Epoch 5:   0%| | 0/144 [00:00<?, ?it/s, loss=0.316, v_num=msub, train categorical_accuracy_step=0.345, train categorical




















































































































Epoch 5:  80%|▊| 115/144 [06:47<01:42,  3.54s/it, loss=0.323, v_num=msub, train categorical_accuracy_step=0.310, train c




























Validation DataLoader 0:  97%|████████████████████████████████████████████████████████  | 28/29 [01:07<00:02,  2.40s/it]
[rank: 0] Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.313

Epoch 5: 100%|█| 144/144 [07:59<00:00,  3.33s/it, loss=0.323, v_num=msub, train categorical_accuracy_step=0.310, train cEpoch duration: 479.54 seconds























































































































Epoch 6:  80%|▊| 115/144 [06:51<01:43,  3.58s/it, loss=0.316, v_num=msub, train categorical_accuracy_step=0.310, train c




























Validation DataLoader 0:  97%|████████████████████████████████████████████████████████  | 28/29 [01:09<00:02,  2.48s/it]




















































































































Epoch 7:  80%|▊| 115/144 [06:50<01:43,  3.57s/it, loss=0.322, v_num=msub, train categorical_accuracy_step=0.448, train c




























Validation DataLoader 0:  97%|████████████████████████████████████████████████████████  | 28/29 [01:04<00:02,  2.31s/it]























































































































Epoch 8:  80%|▊| 115/144 [06:43<01:41,  3.51s/it, loss=0.307, v_num=msub, train categorical_accuracy_step=0.483, train c




























Validation DataLoader 0:  97%|████████████████████████████████████████████████████████  | 28/29 [01:06<00:02,  2.37s/it]




















































































































Epoch 9:  80%|▊| 115/144 [06:30<01:38,  3.39s/it, loss=0.305, v_num=msub, train categorical_accuracy_step=0.448, train c




























Validation DataLoader 0:  97%|████████████████████████████████████████████████████████  | 28/29 [01:04<00:02,  2.30s/it]
[rank: 0] Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.310

Epoch 9: 100%|█| 144/144 [07:39<00:00,  3.19s/it, loss=0.305, v_num=msub, train categorical_accuracy_step=0.448, train cEpoch duration: 459.63 seconds



















































































































Epoch 10:  80%|▊| 115/144 [06:32<01:39,  3.42s/it, loss=0.314, v_num=msub, train categorical_accuracy_step=0.517, train





























Epoch 11:   0%| | 0/144 [00:00<?, ?it/s, loss=0.314, v_num=msub, train categorical_accuracy_step=0.517, train categorica
[rank: 0] Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.309





















































































































Epoch 11:  80%|▊| 115/144 [06:34<01:39,  3.43s/it, loss=0.308, v_num=msub, train categorical_accuracy_step=0.379, train





























Epoch 12:   0%| | 0/144 [00:00<?, ?it/s, loss=0.308, v_num=msub, train categorical_accuracy_step=0.379, train categorica


















































































































Epoch 12:  80%|▊| 115/144 [06:31<01:38,  3.40s/it, loss=0.306, v_num=msub, train categorical_accuracy_step=0.379, train




























Validation DataLoader 0:  97%|████████████████████████████████████████████████████████  | 28/29 [01:06<00:02,  2.37s/it]
[rank: 0] Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.309

Epoch 12: 100%|█| 144/144 [07:42<00:00,  3.21s/it, loss=0.306, v_num=msub, train categorical_accuracy_step=0.379, train Epoch duration: 462.59 seconds


















































































































Epoch 13:  80%|▊| 115/144 [06:38<01:40,  3.47s/it, loss=0.323, v_num=msub, train categorical_accuracy_step=0.310, train




























Validation DataLoader 0:  97%|████████████████████████████████████████████████████████  | 28/29 [01:05<00:02,  2.33s/it]
[rank: 0] Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.308

Epoch 13: 100%|█| 144/144 [07:48<00:00,  3.25s/it, loss=0.323, v_num=msub, train categorical_accuracy_step=0.310, train Epoch duration: 468.79 seconds



















































































































Epoch 14:  80%|▊| 115/144 [06:36<01:40,  3.45s/it, loss=0.309, v_num=msub, train categorical_accuracy_step=0.414, train




























Validation DataLoader 0:  97%|████████████████████████████████████████████████████████  | 28/29 [01:05<00:02,  2.35s/it]




















































































































Epoch 15:  80%|▊| 115/144 [06:34<01:39,  3.43s/it, loss=0.3, v_num=msub, train categorical_accuracy_step=0.310, train ca




























Validation DataLoader 0:  93%|██████████████████████████████████████████████████████    | 27/29 [01:03<00:04,  2.36s/it]
[rank: 0] Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.307
Epoch 15: 100%|█| 144/144 [07:44<00:00,  3.22s/it, loss=0.3, v_num=msub, train categorical_accuracy_step=0.310, train caEpoch duration: 464.87 seconds



















































































































Epoch 16:  80%|▊| 115/144 [06:34<01:39,  3.43s/it, loss=0.316, v_num=msub, train categorical_accuracy_step=0.379, train




























Validation DataLoader 0:  97%|████████████████████████████████████████████████████████  | 28/29 [01:05<00:02,  2.35s/it]
[rank: 0] Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.305

Epoch 16: 100%|█| 144/144 [07:45<00:00,  3.23s/it, loss=0.316, v_num=msub, train categorical_accuracy_step=0.379, train Epoch duration: 465.46 seconds

















































































































Epoch 17:  80%|▊| 115/144 [06:35<01:39,  3.44s/it, loss=0.313, v_num=msub, train categorical_accuracy_step=0.414, train





























Epoch 18:   0%| | 0/144 [00:00<?, ?it/s, loss=0.313, v_num=msub, train categorical_accuracy_step=0.414, train categorica



















































































































Epoch 18:  80%|▊| 115/144 [06:31<01:38,  3.40s/it, loss=0.309, v_num=msub, train categorical_accuracy_step=0.276, train




























Validation DataLoader 0:  97%|████████████████████████████████████████████████████████  | 28/29 [01:05<00:02,  2.35s/it]




















































































































Epoch 19:  80%|▊| 115/144 [06:38<01:40,  3.46s/it, loss=0.314, v_num=msub, train categorical_accuracy_step=0.517, train





























Epoch 20:   0%| | 0/144 [00:00<?, ?it/s, loss=0.314, v_num=msub, train categorical_accuracy_step=0.517, train categorica
































































































































Epoch 20:  80%|▊| 115/144 [06:31<01:38,  3.40s/it, loss=0.306, v_num=msub, train categorical_accuracy_step=0.483, train



























Validation DataLoader 0:  97%|████████████████████████████████████████████████████████  | 28/29 [01:06<00:02,  2.37s/it]
[rank: 0] Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.305

Epoch 20: 100%|█| 144/144 [07:41<00:00,  3.21s/it, loss=0.306, v_num=msub, train categorical_accuracy_step=0.483, train Epoch duration: 462.63 seconds





















































































































Epoch 21:  80%|▊| 115/144 [06:41<01:41,  3.49s/it, loss=0.301, v_num=msub, train categorical_accuracy_step=0.276, train




























Validation DataLoader 0:  97%|████████████████████████████████████████████████████████  | 28/29 [01:06<00:02,  2.36s/it]
[rank: 0] Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.304

Epoch 21: 100%|█| 144/144 [07:51<00:00,  3.28s/it, loss=0.301, v_num=msub, train categorical_accuracy_step=0.276, train Epoch duration: 472.44 seconds



















































































































Epoch 22:  80%|▊| 115/144 [06:40<01:40,  3.48s/it, loss=0.299, v_num=msub, train categorical_accuracy_step=0.483, train




























Validation DataLoader 0:  97%|████████████████████████████████████████████████████████  | 28/29 [01:08<00:02,  2.44s/it]
[rank: 0] Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.302

Epoch 22: 100%|█| 144/144 [07:52<00:00,  3.28s/it, loss=0.299, v_num=msub, train categorical_accuracy_step=0.483, train Epoch duration: 472.92 seconds


















































































































Epoch 23:  80%|▊| 115/144 [06:45<01:42,  3.53s/it, loss=0.307, v_num=msub, train categorical_accuracy_step=0.345, train




























Epoch 24:   0%| | 0/144 [00:00<?, ?it/s, loss=0.307, v_num=msub, train categorical_accuracy_step=0.345, train categorica




















































































































Epoch 24:  80%|▊| 115/144 [06:43<01:41,  3.51s/it, loss=0.308, v_num=msub, train categorical_accuracy_step=0.379, train




























Validation DataLoader 0:  93%|██████████████████████████████████████████████████████    | 27/29 [01:05<00:04,  2.44s/it]
[rank: 0] Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.298
Epoch 24: 100%|█| 144/144 [07:56<00:00,  3.31s/it, loss=0.308, v_num=msub, train categorical_accuracy_step=0.379, train Epoch duration: 476.81 seconds

















































































































Epoch 25:  80%|▊| 115/144 [06:43<01:41,  3.51s/it, loss=0.305, v_num=msub, train categorical_accuracy_step=0.345, train




























Validation DataLoader 0:  97%|████████████████████████████████████████████████████████  | 28/29 [01:07<00:02,  2.41s/it]


















































































































Epoch 26:  80%|▊| 115/144 [06:36<01:39,  3.45s/it, loss=0.299, v_num=msub, train categorical_accuracy_step=0.276, train





























Epoch 27:   0%| | 0/144 [00:00<?, ?it/s, loss=0.299, v_num=msub, train categorical_accuracy_step=0.276, train categorica



















































































































Epoch 27:  80%|▊| 115/144 [06:37<01:40,  3.45s/it, loss=0.308, v_num=msub, train categorical_accuracy_step=0.241, train





























Validation DataLoader 0:  97%|████████████████████████████████████████████████████████  | 28/29 [01:07<00:02,  2.40s/it]
[rank: 0] Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.298
Epoch 27: 100%|█| 144/144 [07:48<00:00,  3.25s/it, loss=0.308, v_num=msub, train categorical_accuracy_step=0.241, train Epoch duration: 469.02 seconds


















































































































Epoch 28:  80%|▊| 115/144 [06:33<01:39,  3.42s/it, loss=0.304, v_num=msub, train categorical_accuracy_step=0.448, train





























Validation DataLoader 0:  97%|████████████████████████████████████████████████████████  | 28/29 [01:05<00:02,  2.34s/it]
[rank: 0] Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.298
Epoch 28: 100%|█| 144/144 [07:42<00:00,  3.21s/it, loss=0.304, v_num=msub, train categorical_accuracy_step=0.448, train Epoch duration: 463.16 seconds

















































































































Epoch 29:  80%|▊| 115/144 [06:31<01:38,  3.41s/it, loss=0.296, v_num=msub, train categorical_accuracy_step=0.345, train





























Epoch 30:   0%| | 0/144 [00:00<?, ?it/s, loss=0.296, v_num=msub, train categorical_accuracy_step=0.345, train categorica


















































































































Epoch 30:  80%|▊| 115/144 [06:32<01:38,  3.41s/it, loss=0.311, v_num=msub, train categorical_accuracy_step=0.207, train





























Epoch 31:   0%| | 0/144 [00:00<?, ?it/s, loss=0.311, v_num=msub, train categorical_accuracy_step=0.207, train categorica

























































































