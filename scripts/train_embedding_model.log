[2024-02-20 09:06:53,027][HYDRA] Joblib.Parallel(n_jobs=-1,backend=loky,prefer=processes,require=None,verbose=0,timeout=None,pre_dispatch=2*n_jobs,batch_size=auto,temp_folder=None,max_nbytes=None,mmap_mode=r) is launching 4 jobs
[2024-02-20 09:06:53,027][HYDRA] Launching jobs, sweep output dir : multirun/2024-02-20/09-06-52
[2024-02-20 09:06:53,027][HYDRA] 	#0 : model=base_model task=RNAlocalization embedder=RNAFM
[2024-02-20 09:06:53,027][HYDRA] 	#1 : model=base_model task=RNAlocalization embedder=parnet
[2024-02-20 09:06:53,027][HYDRA] 	#2 : model=base_model task=RNAlocalization embedder=nucleotidetransformer
[2024-02-20 09:06:53,027][HYDRA] 	#3 : model=base_model task=RNAlocalization embedder=DNABERT2
wandb: WARNING Path /home/sxr280/wandb_logs/wandb/ wasn't writable, using system temp directory
wandb: WARNING Path /home/sxr280/wandb_logs/wandb/ wasn't writable, using system temp directory
wandb: WARNING Path /home/sxr280/wandb_logs/wandb/ wasn't writable, using system temp directory
wandb: WARNING Path /home/sxr280/wandb_logs/wandb/ wasn't writable, using system temp directory
wandb: WARNING Path /home/sxr280/wandb_logs/wandb/ wasn't writable, using system temp directory
wandb: WARNING Path /home/sxr280/wandb_logs/wandb/ wasn't writable, using system temp directory
wandb: WARNING Path /home/sxr280/wandb_logs/wandb/ wasn't writable, using system temp directory
wandb: WARNING Path /home/sxr280/wandb_logs/wandb/ wasn't writable, using system temp directory
wandb: Currently logged in as: junwang666 (junwanggroup). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: junwang666 (junwanggroup). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: junwang666 (junwanggroup). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: junwang666 (junwanggroup). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /home/sxr280/BERTLocRNA/scripts/../output/RNAlocalization/parnet/wandb/run-20240220_090701-51rl8wsw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run parnet
wandb: ‚≠êÔ∏è View project at https://wandb.ai/junwanggroup/BERTLocRNA
wandb: üöÄ View run at https://wandb.ai/junwanggroup/BERTLocRNA/runs/51rl8wsw
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /home/sxr280/BERTLocRNA/scripts/../output/RNAlocalization/DNABERT2/wandb/run-20240220_090701-82j5hitj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DNABERT2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/junwanggroup/BERTLocRNA
wandb: üöÄ View run at https://wandb.ai/junwanggroup/BERTLocRNA/runs/82j5hitj
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /home/sxr280/BERTLocRNA/scripts/../output/RNAlocalization/nucleotidetransformer/wandb/run-20240220_090701-j0wi8g0t
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /home/sxr280/BERTLocRNA/scripts/../output/RNAlocalization/RNAFM/wandb/run-20240220_090701-vsr6ahxt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run nucleotidetransformer
wandb: ‚≠êÔ∏è View project at https://wandb.ai/junwanggroup/BERTLocRNA
wandb: üöÄ View run at https://wandb.ai/junwanggroup/BERTLocRNA/runs/j0wi8g0t
wandb: Syncing run RNAFM
wandb: ‚≠êÔ∏è View project at https://wandb.ai/junwanggroup/BERTLocRNA
wandb: üöÄ View run at https://wandb.ai/junwanggroup/BERTLocRNA/runs/vsr6ahxt
2024-02-20 09:07:36.438620: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2024-02-20 09:07:36.438620: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2024-02-20 09:07:36.438650: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2024-02-20 09:07:36.438649: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2024-02-20 09:07:43.083037: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2024-02-20 09:07:43.083166: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2024-02-20 09:07:43.083805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:14:00.0 name: Quadro RTX 6000 computeCapability: 7.5
coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.64GiB deviceMemoryBandwidth: 625.94GiB/s
2024-02-20 09:07:43.084303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:39:00.0 name: Quadro RTX 6000 computeCapability: 7.5
coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.64GiB deviceMemoryBandwidth: 625.94GiB/s
2024-02-20 09:07:43.084348: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2024-02-20 09:07:43.084396: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2024-02-20 09:07:43.084416: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2024-02-20 09:07:43.110438: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2024-02-20 09:07:43.169748: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2024-02-20 09:07:43.210891: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2024-02-20 09:07:43.239379: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2024-02-20 09:07:43.239481: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2024-02-20 09:07:43.241381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1
initializing
output dir of this job: /home/sxr280/BERTLocRNA/scripts/../output/RNAlocalization/RNAFM
loading task: {'path': 'TerminatorJ/localization_multiRNA'}
pos weight: [ 1.71565757  1.0257667   8.8253012   2.28477854  4.38229136  6.82983683
 11.62698413 11.72       23.44      ]
weight_dict {'lncRNA': [0.8969458861870055, 0.24384255318528325, 3.053073549251424, 0.0, 0.0, 5.806138011376286, 0.0, 0.0, 0.0], 'mRNA': [0.5106930304115337, 0.36113995907232, 2.5254766994106728, 0.6264803401068899, 1.145488696375963, 1.834995696302938, 2.9957255783196834, 0.0, 0.0], 'miRNA': [2.751022931164908, 0.8092170313344499, 0.0, 5.553215100891041, 0.0, 0.0, 0.0, 0.8865449366096001, 0.0], 'snRNA': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'snoRNA': [0.0, 4.934740455322644, 0.0, 0.0, 0.0, 0.0, 0.0, 5.065259544677355, 0.0]}
flt_dict {'lncRNA': [0, 1, 2, 5], 'mRNA': [0, 1, 2, 3, 4, 5, 6], 'miRNA': [0, 1, 3, 7], 'snRNA': [], 'snoRNA': [1, 7]}
embedding will be saved at: /tmp/erda/BERTLocRNA/embeddings/RNAlocalization_RNAFMembedding
loading the dataset...
Resolving data files:   0%|          | 0/221 [00:00<?, ?it/s]Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 221/221 [00:00<00:00, 401273.24it/s]initializing
output dir of this job: /home/sxr280/BERTLocRNA/scripts/../output/RNAlocalization/parnet
loading task: {'path': 'TerminatorJ/localization_multiRNA'}
pos weight: [ 1.71565757  1.0257667   8.8253012   2.28477854  4.38229136  6.82983683
 11.62698413 11.72       23.44      ]
weight_dict {'lncRNA': [0.8969458861870055, 0.24384255318528325, 3.053073549251424, 0.0, 0.0, 5.806138011376286, 0.0, 0.0, 0.0], 'mRNA': [0.5106930304115337, 0.36113995907232, 2.5254766994106728, 0.6264803401068899, 1.145488696375963, 1.834995696302938, 2.9957255783196834, 0.0, 0.0], 'miRNA': [2.751022931164908, 0.8092170313344499, 0.0, 5.553215100891041, 0.0, 0.0, 0.0, 0.8865449366096001, 0.0], 'snRNA': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'snoRNA': [0.0, 4.934740455322644, 0.0, 0.0, 0.0, 0.0, 0.0, 5.065259544677355, 0.0]}
flt_dict {'lncRNA': [0, 1, 2, 5], 'mRNA': [0, 1, 2, 3, 4, 5, 6], 'miRNA': [0, 1, 3, 7], 'snRNA': [], 'snoRNA': [1, 7]}
embedding will be saved at: /tmp/erda/BERTLocRNA/embeddings/RNAlocalization_Parnetembedding
loading the dataset...
Resolving data files:   0%|          | 0/243 [00:00<?, ?it/s]Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 243/243 [00:00<00:00, 273248.22it/s]
Resolving data files:   0%|          | 0/56 [00:00<?, ?it/s]Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:00<00:00, 246983.20it/s]
Resolving data files:   0%|          | 0/62 [00:00<?, ?it/s]Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62/62 [00:00<00:00, 268366.20it/s]
Resolving data files:   0%|          | 0/77 [00:00<?, ?it/s]Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:00<00:00, 262144.00it/s]
Resolving data files:   0%|          | 0/70 [00:00<?, ?it/s]Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:00<00:00, 260284.82it/s]
Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]Downloading data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 332.82it/s]
Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]
Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]Downloading data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 379.78it/s]
Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]initializing
output dir of this job: /home/sxr280/BERTLocRNA/scripts/../output/RNAlocalization/nucleotidetransformer
/home/sxr280/BERTLocRNA/saved_model/NT  already exists, loading the model locally
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:08<00:08,  8.84s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  3.84s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.59s/it]
Some weights of the model checkpoint at /home/sxr280/BERTLocRNA/saved_model/NT were not used when initializing EsmModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing EsmModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing EsmModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of EsmModel were not initialized from the model checkpoint at /home/sxr280/BERTLocRNA/saved_model/NT and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading task: {'path': 'TerminatorJ/localization_multiRNA'}
pos weight: [ 1.71565757  1.0257667   8.8253012   2.28477854  4.38229136  6.82983683
 11.62698413 11.72       23.44      ]
weight_dict {'lncRNA': [0.8969458861870055, 0.24384255318528325, 3.053073549251424, 0.0, 0.0, 5.806138011376286, 0.0, 0.0, 0.0], 'mRNA': [0.5106930304115337, 0.36113995907232, 2.5254766994106728, 0.6264803401068899, 1.145488696375963, 1.834995696302938, 2.9957255783196834, 0.0, 0.0], 'miRNA': [2.751022931164908, 0.8092170313344499, 0.0, 5.553215100891041, 0.0, 0.0, 0.0, 0.8865449366096001, 0.0], 'snRNA': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'snoRNA': [0.0, 4.934740455322644, 0.0, 0.0, 0.0, 0.0, 0.0, 5.065259544677355, 0.0]}
flt_dict {'lncRNA': [0, 1, 2, 5], 'mRNA': [0, 1, 2, 3, 4, 5, 6], 'miRNA': [0, 1, 3, 7], 'snRNA': [], 'snoRNA': [1, 7]}
processing################################
loading the dataset...
Resolving data files:   0%|          | 0/296 [00:00<?, ?it/s]Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 296/296 [00:00<00:00, 513234.39it/s]
Resolving data files:   0%|          | 0/75 [00:00<?, ?it/s]Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:00<00:00, 259120.92it/s]
Resolving data files:   0%|          | 0/94 [00:00<?, ?it/s]Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 94/94 [00:00<00:00, 339298.26it/s]
Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]Downloading data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 271.18it/s]
Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]/tmp/erda/BERTLocRNA/cache/modules/transformers_modules/zhihan1996/DNABERT-2-117M/25abaf0bd247444fcfa837109f12088114898d98/bert_layers.py:125: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Some weights of the model checkpoint at zhihan1996/DNABERT-2-117M were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
initializing
output dir of this job: /home/sxr280/BERTLocRNA/scripts/../output/RNAlocalization/DNABERT2
loading task: {'path': 'TerminatorJ/localization_multiRNA'}
pos weight: [ 1.71565757  1.0257667   8.8253012   2.28477854  4.38229136  6.82983683
 11.62698413 11.72       23.44      ]
weight_dict {'lncRNA': [0.8969458861870055, 0.24384255318528325, 3.053073549251424, 0.0, 0.0, 5.806138011376286, 0.0, 0.0, 0.0], 'mRNA': [0.5106930304115337, 0.36113995907232, 2.5254766994106728, 0.6264803401068899, 1.145488696375963, 1.834995696302938, 2.9957255783196834, 0.0, 0.0], 'miRNA': [2.751022931164908, 0.8092170313344499, 0.0, 5.553215100891041, 0.0, 0.0, 0.0, 0.8865449366096001, 0.0], 'snRNA': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'snoRNA': [0.0, 4.934740455322644, 0.0, 0.0, 0.0, 0.0, 0.0, 5.065259544677355, 0.0]}
flt_dict {'lncRNA': [0, 1, 2, 5], 'mRNA': [0, 1, 2, 3, 4, 5, 6], 'miRNA': [0, 1, 3, 7], 'snRNA': [], 'snoRNA': [1, 7]}
embedding will be saved at: /tmp/erda/BERTLocRNA/embeddings/RNAlocalization_DNABERT2embedding
loading the dataset...
Resolving data files:   0%|          | 0/54 [00:00<?, ?it/s]Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 54/54 [00:00<00:00, 265524.52it/s]
Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:00<00:00, 133152.51it/s]Extracting data files:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [01:18<02:36, 78.24s/it]Extracting data files:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [01:27<02:54, 87.48s/it]
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/lightning_fabric/plugins/environments/slurm.py:166: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python train_embedding_model.py ...
  rank_zero_warn(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
tokenized_datasets DatasetDict({
    train: Dataset({
        features: ['label', 'idx', 'Xall', 'Xtag', 'ids', 'input_ids', 'embedding', 'attention_mask'],
        num_rows: 14650
    })
    validation: Dataset({
        features: ['label', 'idx', 'Xall', 'Xtag', 'ids', 'input_ids', 'embedding', 'attention_mask'],
        num_rows: 3670
    })
    test: Dataset({
        features: ['label', 'idx', 'Xall', 'Xtag', 'ids', 'input_ids', 'embedding', 'attention_mask'],
        num_rows: 4587
    })
})
embeddings shape: torch.Size([2, 768, 1551])
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
LightningModel                           [2, 9]                    9
‚îú‚îÄCustomizedModel: 1-1                   [2, 9]                    --
‚îÇ    ‚îî‚îÄMaxPool1d: 2-1                    [2, 768, 258]             --
‚îÇ    ‚îî‚îÄDropout: 2-2                      [2, 768, 258]             --
‚îÇ    ‚îî‚îÄAttention_mask: 2-3               [2, 768, 3]               --
‚îÇ    ‚îÇ    ‚îî‚îÄLinear: 3-1                  [2, 258, 80]              61,440
‚îÇ    ‚îÇ    ‚îî‚îÄTanh: 3-2                    [2, 258, 80]              --
‚îÇ    ‚îÇ    ‚îî‚îÄLinear: 3-3                  [2, 258, 3]               240
‚îÇ    ‚îî‚îÄFlatten: 2-4                      [2, 2304]                 --
‚îÇ    ‚îî‚îÄEmbedding: 2-5                    [2, 4]                    76
‚îÇ    ‚îî‚îÄLinear: 2-6                       [2, 100]                  230,500
‚îÇ    ‚îî‚îÄActvation: 2-7                    [2, 104]                  --
‚îÇ    ‚îî‚îÄDropout: 2-8                      [2, 104]                  --
‚îÇ    ‚îî‚îÄLinear: 2-9                       [2, 9]                    945
‚îÇ    ‚îî‚îÄSigmoid: 2-10                     [2, 9]                    --
==========================================================================================
Total params: 293,210
Trainable params: 293,210
Non-trainable params: 0
Total mult-adds (M): 0.59
==========================================================================================
Input size (MB): 9.53
Forward/backward pass size (MB): 0.34
Params size (MB): 1.17
Estimated Total Size (MB): 11.05
==========================================================================================
[2024-02-20 09:09:34,815][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2024-02-20 09:09:34,816][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/sxr280/BERTLocRNA/output/RNAlocalization/DNABERT2/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name           | Type                 | Params
--------------------------------------------------------
0 | network        | CustomizedModel      | 293 K 
1 | learnable_loss | MultiTaskLossWrapper | 9     
2 | loss_fn        | BCELoss              | 0     
--------------------------------------------------------
293 K     Trainable params
0         Non-trainable params
293 K     Total params
1.173     Total estimated model params size (MB)
Sanity Checking: 0it [00:00, ?it/s]Extracting data files:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [01:40<00:45, 45.42s/it]Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]embeddings shape: torch.Size([32, 768, 1569])
Sanity Checking DataLoader 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 70.61it/s]Extracting data files:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [01:49<00:49, 49.07s/it]embeddings shape: torch.Size([32, 768, 1635])
Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:12<00:00,  6.23s/it]                                                                           /home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val categorical_accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val categorical_accuracy_strict', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val binary_accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (458) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/573 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/573 [00:00<?, ?it/s] Extracting data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [02:05<00:00, 36.08s/it]Extracting data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [02:05<00:00, 41.88s/it]
Generating train split: 0 examples [00:00, ? examples/s]Extracting data files:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [01:51<03:43, 111.74s/it]embeddings shape: torch.Size([32, 768, 1569])
Epoch 0:   0%|          | 1/573 [00:14<2:22:33, 14.95s/it]Epoch 0:   0%|          | 1/573 [00:14<2:22:34, 14.95s/it, loss=1.82, v_num=hitj, train categorical_accuracy_step=0.000, train categorical_accuracy_strict_step=0.000, train binary_accuracy_step=0.483]Extracting data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [02:20<00:00, 40.59s/it]Extracting data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [02:20<00:00, 46.72s/it]
Generating train split: 0 examples [00:00, ? examples/s]embeddings shape: torch.Size([32, 768, 1595])
Epoch 0:   0%|          | 2/573 [00:28<2:17:13, 14.42s/it, loss=1.82, v_num=hitj, train categorical_accuracy_step=0.000, train categorical_accuracy_strict_step=0.000, train binary_accuracy_step=0.483]Epoch 0:   0%|          | 2/573 [00:28<2:17:13, 14.42s/it, loss=1.77, v_num=hitj, train categorical_accuracy_step=0.000, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.719]embeddings shape: torch.Size([32, 768, 1494])
Epoch 0:   1%|          | 3/573 [00:41<2:12:10, 13.91s/it, loss=1.77, v_num=hitj, train categorical_accuracy_step=0.000, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.719]Epoch 0:   1%|          | 3/573 [00:41<2:12:11, 13.91s/it, loss=1.72, v_num=hitj, train categorical_accuracy_step=0.000, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.753]Extracting data files:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [02:22<01:04, 64.08s/it] embeddings shape: torch.Size([32, 768, 1566])
Epoch 0:   1%|          | 4/573 [00:55<2:10:34, 13.77s/it, loss=1.72, v_num=hitj, train categorical_accuracy_step=0.000, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.753]Epoch 0:   1%|          | 4/573 [00:55<2:10:34, 13.77s/it, loss=1.67, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.767]embeddings shape: torch.Size([32, 768, 1582])
Epoch 0:   1%|          | 5/573 [01:07<2:07:07, 13.43s/it, loss=1.67, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.767]Epoch 0:   1%|          | 5/573 [01:07<2:07:07, 13.43s/it, loss=1.63, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.750]embeddings shape: torch.Size([32, 768, 1536])
Epoch 0:   1%|          | 6/573 [01:18<2:03:04, 13.02s/it, loss=1.63, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.750]Epoch 0:   1%|          | 6/573 [01:18<2:03:04, 13.02s/it, loss=1.59, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.795]Extracting data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [02:59<00:00, 51.55s/it]Extracting data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [02:59<00:00, 59.70s/it]
Generating train split: 0 examples [00:00, ? examples/s]embeddings shape: torch.Size([32, 768, 1567])
Epoch 0:   1%|          | 7/573 [01:42<2:18:42, 14.70s/it, loss=1.59, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.795]Epoch 0:   1%|          | 7/573 [01:42<2:18:42, 14.70s/it, loss=1.55, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.781]Generating train split: 67 examples [01:52,  1.68s/ examples]embeddings shape: torch.Size([32, 768, 1592])
Epoch 0:   1%|‚ñè         | 8/573 [02:16<2:40:13, 17.02s/it, loss=1.55, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.781]Epoch 0:   1%|‚ñè         | 8/573 [02:16<2:40:13, 17.02s/it, loss=1.51, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.778]Generating train split: 61 examples [02:09,  2.12s/ examples]embeddings shape: torch.Size([32, 768, 1240])
Epoch 0:   2%|‚ñè         | 9/573 [02:35<2:41:57, 17.23s/it, loss=1.51, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.778]Epoch 0:   2%|‚ñè         | 9/573 [02:35<2:41:57, 17.23s/it, loss=1.47, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.819]embeddings shape: torch.Size([32, 768, 1671])
Epoch 0:   2%|‚ñè         | 10/573 [02:45<2:35:24, 16.56s/it, loss=1.47, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.819]Epoch 0:   2%|‚ñè         | 10/573 [02:45<2:35:24, 16.56s/it, loss=1.43, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.806]embeddings shape: torch.Size([32, 768, 1555])
Epoch 0:   2%|‚ñè         | 11/573 [02:54<2:28:38, 15.87s/it, loss=1.43, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.806]Epoch 0:   2%|‚ñè         | 11/573 [02:54<2:28:38, 15.87s/it, loss=1.4, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.788]  embeddings shape: torch.Size([32, 768, 1519])
Epoch 0:   2%|‚ñè         | 12/573 [03:05<2:24:41, 15.47s/it, loss=1.4, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.788]Epoch 0:   2%|‚ñè         | 12/573 [03:05<2:24:41, 15.47s/it, loss=1.36, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.837]embeddings shape: torch.Size([32, 768, 1609])
Epoch 0:   2%|‚ñè         | 13/573 [03:25<2:27:25, 15.80s/it, loss=1.36, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.837]Epoch 0:   2%|‚ñè         | 13/573 [03:25<2:27:26, 15.80s/it, loss=1.33, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.830]Generating train split: 50 examples [02:20,  2.80s/ examples]embeddings shape: torch.Size([32, 768, 1590])
Epoch 0:   2%|‚ñè         | 14/573 [03:47<2:31:38, 16.28s/it, loss=1.33, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.830]Epoch 0:   2%|‚ñè         | 14/573 [03:47<2:31:38, 16.28s/it, loss=1.31, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.771]Generating train split: 134 examples [04:11,  1.91s/ examples]embeddings shape: torch.Size([32, 768, 1566])
Epoch 0:   3%|‚ñé         | 15/573 [04:20<2:41:41, 17.39s/it, loss=1.31, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.771]Epoch 0:   3%|‚ñé         | 15/573 [04:20<2:41:41, 17.39s/it, loss=1.28, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.795]Generating train split: 122 examples [04:27,  2.21s/ examples]embeddings shape: torch.Size([32, 768, 1587])
Epoch 0:   3%|‚ñé         | 16/573 [04:48<2:47:40, 18.06s/it, loss=1.28, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.795]Epoch 0:   3%|‚ñé         | 16/573 [04:48<2:47:40, 18.06s/it, loss=1.25, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.809] embeddings shape: torch.Size([32, 768, 1592])
Epoch 0:   3%|‚ñé         | 17/573 [04:59<2:43:12, 17.61s/it, loss=1.25, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.809]Epoch 0:   3%|‚ñé         | 17/573 [04:59<2:43:12, 17.61s/it, loss=1.23, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.792]embeddings shape: torch.Size([32, 768, 1605])
Epoch 0:   3%|‚ñé         | 18/573 [05:07<2:38:00, 17.08s/it, loss=1.23, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.792]Epoch 0:   3%|‚ñé         | 18/573 [05:07<2:38:00, 17.08s/it, loss=1.21, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.764]embeddings shape: torch.Size([32, 768, 1360])
Epoch 0:   3%|‚ñé         | 19/573 [05:16<2:33:35, 16.63s/it, loss=1.21, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.764]Epoch 0:   3%|‚ñé         | 19/573 [05:16<2:33:35, 16.63s/it, loss=1.18, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.812]embeddings shape: torch.Size([32, 768, 1570])
Epoch 0:   3%|‚ñé         | 20/573 [05:25<2:30:01, 16.28s/it, loss=1.18, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.812]Epoch 0:   3%|‚ñé         | 20/573 [05:25<2:30:01, 16.28s/it, loss=1.16, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.812]embeddings shape: torch.Size([32, 768, 1608])
Epoch 0:   4%|‚ñé         | 21/573 [05:57<2:36:25, 17.00s/it, loss=1.16, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.812]Epoch 0:   4%|‚ñé         | 21/573 [05:57<2:36:25, 17.00s/it, loss=1.11, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.840]Generating train split: 100 examples [04:37,  2.77s/ examples]embeddings shape: torch.Size([32, 768, 1620])
Epoch 0:   4%|‚ñç         | 22/573 [06:07<2:33:19, 16.70s/it, loss=1.11, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.840]Epoch 0:   4%|‚ñç         | 22/573 [06:07<2:33:19, 16.70s/it, loss=1.05, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.802]embeddings shape: torch.Size([32, 768, 1210])
Epoch 0:   4%|‚ñç         | 23/573 [06:28<2:34:49, 16.89s/it, loss=1.05, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.802]Epoch 0:   4%|‚ñç         | 23/573 [06:28<2:34:49, 16.89s/it, loss=1.01, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.781] Generating train split: 201 examples [06:46,  2.09s/ examples]embeddings shape: torch.Size([32, 768, 1617])
Epoch 0:   4%|‚ñç         | 24/573 [07:03<2:41:29, 17.65s/it, loss=1.01, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.781]Epoch 0:   4%|‚ñç         | 24/573 [07:03<2:41:29, 17.65s/it, loss=0.963, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.799]Generating train split: 183 examples [06:50,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1607])
Epoch 0:   4%|‚ñç         | 25/573 [07:20<2:40:51, 17.61s/it, loss=0.963, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.799]Epoch 0:   4%|‚ñç         | 25/573 [07:20<2:40:51, 17.61s/it, loss=0.919, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.816] embeddings shape: torch.Size([32, 768, 1570])
Epoch 0:   5%|‚ñç         | 26/573 [07:30<2:37:53, 17.32s/it, loss=0.919, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.816]Epoch 0:   5%|‚ñç         | 26/573 [07:30<2:37:53, 17.32s/it, loss=0.879, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833]embeddings shape: torch.Size([32, 768, 1592])
Epoch 0:   5%|‚ñç         | 27/573 [07:39<2:34:49, 17.01s/it, loss=0.879, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833]Epoch 0:   5%|‚ñç         | 27/573 [07:39<2:34:49, 17.01s/it, loss=0.844, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.806]embeddings shape: torch.Size([32, 768, 1566])
Epoch 0:   5%|‚ñç         | 28/573 [07:47<2:31:45, 16.71s/it, loss=0.844, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.806]Epoch 0:   5%|‚ñç         | 28/573 [07:47<2:31:45, 16.71s/it, loss=0.81, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.833]embeddings shape: torch.Size([32, 768, 1592])
Epoch 0:   5%|‚ñå         | 29/573 [08:15<2:34:50, 17.08s/it, loss=0.81, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.833]Epoch 0:   5%|‚ñå         | 29/573 [08:15<2:34:50, 17.08s/it, loss=0.776, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861]Generating train split: 150 examples [07:00,  2.81s/ examples]embeddings shape: torch.Size([32, 768, 1567])
Epoch 0:   5%|‚ñå         | 30/573 [08:30<2:33:52, 17.00s/it, loss=0.776, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861]Epoch 0:   5%|‚ñå         | 30/573 [08:30<2:33:52, 17.00s/it, loss=0.749, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.799]embeddings shape: torch.Size([32, 768, 1598])
Epoch 0:   5%|‚ñå         | 31/573 [08:38<2:31:01, 16.72s/it, loss=0.749, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.799]Epoch 0:   5%|‚ñå         | 31/573 [08:38<2:31:01, 16.72s/it, loss=0.722, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.802] embeddings shape: torch.Size([32, 768, 1556])
Epoch 0:   6%|‚ñå         | 32/573 [09:03<2:33:10, 16.99s/it, loss=0.722, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.802]Epoch 0:   6%|‚ñå         | 32/573 [09:03<2:33:10, 16.99s/it, loss=0.702, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.788]Generating train split: 244 examples [09:11,  2.29s/ examples]embeddings shape: torch.Size([32, 768, 1417])
Epoch 0:   6%|‚ñå         | 33/573 [09:35<2:36:51, 17.43s/it, loss=0.702, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.788]Epoch 0:   6%|‚ñå         | 33/573 [09:35<2:36:51, 17.43s/it, loss=0.679, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.809]Generating train split: 268 examples [09:34,  2.26s/ examples]embeddings shape: torch.Size([32, 768, 1618])
Epoch 0:   6%|‚ñå         | 34/573 [09:48<2:35:36, 17.32s/it, loss=0.679, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.809]Epoch 0:   6%|‚ñå         | 34/573 [09:48<2:35:36, 17.32s/it, loss=0.653, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.806]embeddings shape: torch.Size([32, 768, 1574])
Epoch 0:   6%|‚ñå         | 35/573 [09:57<2:32:59, 17.06s/it, loss=0.653, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.806]Epoch 0:   6%|‚ñå         | 35/573 [09:57<2:32:59, 17.06s/it, loss=0.633, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.840] embeddings shape: torch.Size([32, 768, 1540])
Epoch 0:   6%|‚ñã         | 36/573 [10:05<2:30:33, 16.82s/it, loss=0.633, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.840]Epoch 0:   6%|‚ñã         | 36/573 [10:05<2:30:33, 16.82s/it, loss=0.619, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.809]embeddings shape: torch.Size([32, 768, 1632])
Epoch 0:   6%|‚ñã         | 37/573 [10:16<2:28:45, 16.65s/it, loss=0.619, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.809]Epoch 0:   6%|‚ñã         | 37/573 [10:16<2:28:45, 16.65s/it, loss=0.6, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.826]   Generating train split: 200 examples [09:20,  2.81s/ examples]embeddings shape: torch.Size([32, 768, 1561])
Epoch 0:   7%|‚ñã         | 38/573 [10:42<2:30:49, 16.91s/it, loss=0.6, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.826]Epoch 0:   7%|‚ñã         | 38/573 [10:42<2:30:49, 16.91s/it, loss=0.583, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.795]embeddings shape: torch.Size([32, 768, 1552])
Epoch 0:   7%|‚ñã         | 39/573 [10:50<2:28:30, 16.69s/it, loss=0.583, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.795]Epoch 0:   7%|‚ñã         | 39/573 [10:50<2:28:30, 16.69s/it, loss=0.569, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.833] embeddings shape: torch.Size([32, 768, 1584])
Epoch 0:   7%|‚ñã         | 40/573 [11:02<2:27:09, 16.57s/it, loss=0.569, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.833]Epoch 0:   7%|‚ñã         | 40/573 [11:02<2:27:09, 16.57s/it, loss=0.555, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.844]Generating train split: 335 examples [11:22,  2.03s/ examples]embeddings shape: torch.Size([32, 768, 1644])
Epoch 0:   7%|‚ñã         | 41/573 [11:40<2:31:28, 17.08s/it, loss=0.555, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.844]Epoch 0:   7%|‚ñã         | 41/573 [11:40<2:31:28, 17.08s/it, loss=0.544, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.851]Generating train split: 305 examples [11:35,  2.31s/ examples]embeddings shape: torch.Size([32, 768, 1565])
Epoch 0:   7%|‚ñã         | 42/573 [12:02<2:32:15, 17.20s/it, loss=0.544, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.851]Epoch 0:   7%|‚ñã         | 42/573 [12:02<2:32:15, 17.20s/it, loss=0.535, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.799]embeddings shape: torch.Size([32, 768, 1595])
Epoch 0:   8%|‚ñä         | 43/573 [12:11<2:30:13, 17.01s/it, loss=0.535, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.799]Epoch 0:   8%|‚ñä         | 43/573 [12:11<2:30:13, 17.01s/it, loss=0.525, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851] embeddings shape: torch.Size([32, 768, 1597])
Epoch 0:   8%|‚ñä         | 44/573 [12:20<2:28:21, 16.83s/it, loss=0.525, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851]Epoch 0:   8%|‚ñä         | 44/573 [12:20<2:28:21, 16.83s/it, loss=0.515, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861]embeddings shape: torch.Size([32, 768, 1531])
Epoch 0:   8%|‚ñä         | 45/573 [12:31<2:27:02, 16.71s/it, loss=0.515, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861]Epoch 0:   8%|‚ñä         | 45/573 [12:31<2:27:02, 16.71s/it, loss=0.508, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.826]Generating train split: 250 examples [11:35,  2.77s/ examples]embeddings shape: torch.Size([32, 768, 1572])
Epoch 0:   8%|‚ñä         | 46/573 [12:58<2:28:41, 16.93s/it, loss=0.508, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.826]Epoch 0:   8%|‚ñä         | 46/573 [12:58<2:28:41, 16.93s/it, loss=0.505, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.837]embeddings shape: torch.Size([32, 768, 1537])
Epoch 0:   8%|‚ñä         | 47/573 [13:07<2:26:55, 16.76s/it, loss=0.505, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.837]Epoch 0:   8%|‚ñä         | 47/573 [13:07<2:26:55, 16.76s/it, loss=0.5, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.816]  embeddings shape: torch.Size([32, 768, 1609])
Epoch 0:   8%|‚ñä         | 48/573 [13:28<2:27:27, 16.85s/it, loss=0.5, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.816]Epoch 0:   8%|‚ñä         | 48/573 [13:28<2:27:27, 16.85s/it, loss=0.497, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.816]Generating train split: 402 examples [13:31,  1.99s/ examples]embeddings shape: torch.Size([32, 768, 1560])
Epoch 0:   9%|‚ñä         | 49/573 [13:51<2:28:07, 16.96s/it, loss=0.497, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.816]Epoch 0:   9%|‚ñä         | 49/573 [13:51<2:28:07, 16.96s/it, loss=0.496, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837] Generating train split: 366 examples [13:52,  2.29s/ examples]embeddings shape: torch.Size([32, 768, 1555])
Epoch 0:   9%|‚ñä         | 50/573 [14:16<2:29:14, 17.12s/it, loss=0.496, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837]Epoch 0:   9%|‚ñä         | 50/573 [14:16<2:29:14, 17.12s/it, loss=0.492, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.816]embeddings shape: torch.Size([32, 768, 1588])
Epoch 0:   9%|‚ñâ         | 51/573 [14:24<2:27:27, 16.95s/it, loss=0.492, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.816]Epoch 0:   9%|‚ñâ         | 51/573 [14:24<2:27:27, 16.95s/it, loss=0.492, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833]embeddings shape: torch.Size([32, 768, 1610])
Epoch 0:   9%|‚ñâ         | 52/573 [14:34<2:26:00, 16.81s/it, loss=0.492, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833]Epoch 0:   9%|‚ñâ         | 52/573 [14:34<2:26:00, 16.81s/it, loss=0.488, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.806]embeddings shape: torch.Size([32, 768, 1609])
Epoch 0:   9%|‚ñâ         | 53/573 [14:43<2:24:31, 16.68s/it, loss=0.488, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.806]Epoch 0:   9%|‚ñâ         | 53/573 [14:43<2:24:31, 16.68s/it, loss=0.487, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.806]Generating train split: 300 examples [13:53,  2.77s/ examples]embeddings shape: torch.Size([32, 768, 1568])
Epoch 0:   9%|‚ñâ         | 54/573 [15:14<2:26:31, 16.94s/it, loss=0.487, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.806]Epoch 0:   9%|‚ñâ         | 54/573 [15:14<2:26:31, 16.94s/it, loss=0.486, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.788]embeddings shape: torch.Size([32, 768, 1407])
Epoch 0:  10%|‚ñâ         | 55/573 [15:23<2:24:56, 16.79s/it, loss=0.486, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.788]Epoch 0:  10%|‚ñâ         | 55/573 [15:23<2:24:56, 16.79s/it, loss=0.486, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.000, train binary_accuracy_step=0.802] embeddings shape: torch.Size([32, 768, 1577])
Epoch 0:  10%|‚ñâ         | 56/573 [15:51<2:26:26, 17.00s/it, loss=0.486, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.000, train binary_accuracy_step=0.802]Epoch 0:  10%|‚ñâ         | 56/573 [15:51<2:26:26, 17.00s/it, loss=0.48, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.816] Generating train split: 469 examples [15:50,  2.02s/ examples]embeddings shape: torch.Size([32, 768, 1630])
Epoch 0:  10%|‚ñâ         | 57/573 [16:08<2:26:11, 17.00s/it, loss=0.48, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.816]Epoch 0:  10%|‚ñâ         | 57/573 [16:08<2:26:11, 17.00s/it, loss=0.479, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.830]Generating train split: 427 examples [16:11,  2.29s/ examples]embeddings shape: torch.Size([32, 768, 1456])
Epoch 0:  10%|‚ñà         | 58/573 [16:33<2:27:00, 17.13s/it, loss=0.479, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.830]Epoch 0:  10%|‚ñà         | 58/573 [16:33<2:27:00, 17.13s/it, loss=0.476, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.865] embeddings shape: torch.Size([32, 768, 1545])
Epoch 0:  10%|‚ñà         | 59/573 [16:41<2:25:28, 16.98s/it, loss=0.476, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.865]Epoch 0:  10%|‚ñà         | 59/573 [16:41<2:25:28, 16.98s/it, loss=0.475, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.823]embeddings shape: torch.Size([32, 768, 1649])
Epoch 0:  10%|‚ñà         | 60/573 [16:52<2:24:15, 16.87s/it, loss=0.475, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.823]Epoch 0:  10%|‚ñà         | 60/573 [16:52<2:24:15, 16.87s/it, loss=0.472, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861]embeddings shape: torch.Size([32, 768, 1585])
Epoch 0:  11%|‚ñà         | 61/573 [17:00<2:22:46, 16.73s/it, loss=0.472, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861]Epoch 0:  11%|‚ñà         | 61/573 [17:00<2:22:46, 16.73s/it, loss=0.471, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837]embeddings shape: torch.Size([32, 768, 1589])
Epoch 0:  11%|‚ñà         | 62/573 [17:23<2:23:23, 16.84s/it, loss=0.471, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837]Epoch 0:  11%|‚ñà         | 62/573 [17:23<2:23:23, 16.84s/it, loss=0.467, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851]Generating train split: 350 examples [16:13,  2.78s/ examples]embeddings shape: torch.Size([32, 768, 1563])
Epoch 0:  11%|‚ñà         | 63/573 [17:49<2:24:18, 16.98s/it, loss=0.467, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851]Epoch 0:  11%|‚ñà         | 63/573 [17:49<2:24:18, 16.98s/it, loss=0.466, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.837]Generating train split: 536 examples [17:55,  1.97s/ examples]embeddings shape: torch.Size([32, 768, 1595])
Epoch 0:  11%|‚ñà         | 64/573 [18:06<2:24:03, 16.98s/it, loss=0.466, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.837]Epoch 0:  11%|‚ñà         | 64/573 [18:06<2:24:03, 16.98s/it, loss=0.467, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861] embeddings shape: torch.Size([32, 768, 1584])
Epoch 0:  11%|‚ñà‚ñè        | 65/573 [18:16<2:22:46, 16.86s/it, loss=0.467, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861]Epoch 0:  11%|‚ñà‚ñè        | 65/573 [18:16<2:22:46, 16.86s/it, loss=0.464, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.899]embeddings shape: torch.Size([32, 768, 1608])
Epoch 0:  12%|‚ñà‚ñè        | 66/573 [18:40<2:23:31, 16.98s/it, loss=0.464, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.899]Epoch 0:  12%|‚ñà‚ñè        | 66/573 [18:40<2:23:31, 16.98s/it, loss=0.463, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.826]Generating train split: 488 examples [18:28,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1351])
Epoch 0:  12%|‚ñà‚ñè        | 67/573 [18:54<2:22:49, 16.94s/it, loss=0.463, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.826]Epoch 0:  12%|‚ñà‚ñè        | 67/573 [18:54<2:22:49, 16.94s/it, loss=0.46, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.816]embeddings shape: torch.Size([32, 768, 1376])
Epoch 0:  12%|‚ñà‚ñè        | 68/573 [19:02<2:21:26, 16.80s/it, loss=0.46, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.816]Epoch 0:  12%|‚ñà‚ñè        | 68/573 [19:02<2:21:26, 16.80s/it, loss=0.456, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840]embeddings shape: torch.Size([32, 768, 1448])
Epoch 0:  12%|‚ñà‚ñè        | 69/573 [19:10<2:20:07, 16.68s/it, loss=0.456, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840]Epoch 0:  12%|‚ñà‚ñè        | 69/573 [19:10<2:20:07, 16.68s/it, loss=0.455, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.830]embeddings shape: torch.Size([32, 768, 1574])
Epoch 0:  12%|‚ñà‚ñè        | 70/573 [19:19<2:18:53, 16.57s/it, loss=0.455, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.830]Epoch 0:  12%|‚ñà‚ñè        | 70/573 [19:19<2:18:53, 16.57s/it, loss=0.453, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.826]embeddings shape: torch.Size([32, 768, 1621])
Epoch 0:  12%|‚ñà‚ñè        | 71/573 [19:46<2:19:51, 16.72s/it, loss=0.453, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.826]Epoch 0:  12%|‚ñà‚ñè        | 71/573 [19:46<2:19:51, 16.72s/it, loss=0.448, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858]Generating train split: 400 examples [18:31,  2.77s/ examples]Generating train split: 603 examples [20:11,  1.99s/ examples]embeddings shape: torch.Size([32, 768, 1659])
Epoch 0:  13%|‚ñà‚ñé        | 72/573 [20:17<2:21:14, 16.92s/it, loss=0.448, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858]Epoch 0:  13%|‚ñà‚ñé        | 72/573 [20:17<2:21:14, 16.92s/it, loss=0.445, v_num=hitj, train categorical_accuracy_step=0.656, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.833]embeddings shape: torch.Size([32, 768, 1571])
Epoch 0:  13%|‚ñà‚ñé        | 73/573 [20:27<2:20:07, 16.82s/it, loss=0.445, v_num=hitj, train categorical_accuracy_step=0.656, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.833]Epoch 0:  13%|‚ñà‚ñé        | 73/573 [20:27<2:20:07, 16.82s/it, loss=0.441, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858] embeddings shape: torch.Size([32, 768, 1625])
Epoch 0:  13%|‚ñà‚ñé        | 74/573 [20:39<2:19:18, 16.75s/it, loss=0.441, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858]Epoch 0:  13%|‚ñà‚ñé        | 74/573 [20:39<2:19:18, 16.75s/it, loss=0.438, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865]Generating train split: 549 examples [20:45,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1599])
Epoch 0:  13%|‚ñà‚ñé        | 75/573 [21:07<2:20:16, 16.90s/it, loss=0.438, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865]Epoch 0:  13%|‚ñà‚ñé        | 75/573 [21:07<2:20:16, 16.90s/it, loss=0.435, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.847]embeddings shape: torch.Size([32, 768, 1698])
Epoch 0:  13%|‚ñà‚ñé        | 76/573 [21:14<2:18:53, 16.77s/it, loss=0.435, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.847]Epoch 0:  13%|‚ñà‚ñé        | 76/573 [21:14<2:18:53, 16.77s/it, loss=0.435, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847] embeddings shape: torch.Size([32, 768, 1613])
Epoch 0:  13%|‚ñà‚ñé        | 77/573 [21:21<2:17:36, 16.65s/it, loss=0.435, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847]Epoch 0:  13%|‚ñà‚ñé        | 77/573 [21:21<2:17:36, 16.65s/it, loss=0.433, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865]embeddings shape: torch.Size([32, 768, 1591])
Epoch 0:  14%|‚ñà‚ñé        | 78/573 [21:29<2:16:21, 16.53s/it, loss=0.433, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865]Epoch 0:  14%|‚ñà‚ñé        | 78/573 [21:29<2:16:21, 16.53s/it, loss=0.432, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844]embeddings shape: torch.Size([32, 768, 1616])
Epoch 0:  14%|‚ñà‚ñç        | 79/573 [21:37<2:15:12, 16.42s/it, loss=0.432, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844]Epoch 0:  14%|‚ñà‚ñç        | 79/573 [21:37<2:15:12, 16.42s/it, loss=0.431, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.830]embeddings shape: torch.Size([32, 768, 1674])
Epoch 0:  14%|‚ñà‚ñç        | 80/573 [21:59<2:15:28, 16.49s/it, loss=0.431, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.830]Epoch 0:  14%|‚ñà‚ñç        | 80/573 [21:59<2:15:28, 16.49s/it, loss=0.434, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.802]Generating train split: 450 examples [20:46,  2.75s/ examples]embeddings shape: torch.Size([32, 768, 1640])
Epoch 0:  14%|‚ñà‚ñç        | 81/573 [22:17<2:15:22, 16.51s/it, loss=0.434, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.802]Epoch 0:  14%|‚ñà‚ñç        | 81/573 [22:17<2:15:22, 16.51s/it, loss=0.434, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.830] Generating train split: 670 examples [22:37,  2.05s/ examples]embeddings shape: torch.Size([32, 768, 1591])
Epoch 0:  14%|‚ñà‚ñç        | 82/573 [22:45<2:16:15, 16.65s/it, loss=0.434, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.830]Epoch 0:  14%|‚ñà‚ñç        | 82/573 [22:45<2:16:15, 16.65s/it, loss=0.432, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.851]embeddings shape: torch.Size([32, 768, 1566])
Epoch 0:  14%|‚ñà‚ñç        | 83/573 [22:53<2:15:08, 16.55s/it, loss=0.432, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.851]Epoch 0:  14%|‚ñà‚ñç        | 83/573 [22:53<2:15:08, 16.55s/it, loss=0.432, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.795]embeddings shape: torch.Size([32, 768, 1595])
Epoch 0:  15%|‚ñà‚ñç        | 84/573 [23:20<2:15:55, 16.68s/it, loss=0.432, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.795]Epoch 0:  15%|‚ñà‚ñç        | 84/573 [23:20<2:15:55, 16.68s/it, loss=0.43, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.844]  Generating train split: 610 examples [23:02,  2.26s/ examples]embeddings shape: torch.Size([32, 768, 1601])
Epoch 0:  15%|‚ñà‚ñç        | 85/573 [23:31<2:15:02, 16.60s/it, loss=0.43, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.844]Epoch 0:  15%|‚ñà‚ñç        | 85/573 [23:31<2:15:02, 16.60s/it, loss=0.433, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.806]embeddings shape: torch.Size([32, 768, 1612])
Epoch 0:  15%|‚ñà‚ñå        | 86/573 [23:38<2:13:54, 16.50s/it, loss=0.433, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.806]Epoch 0:  15%|‚ñà‚ñå        | 86/573 [23:38<2:13:54, 16.50s/it, loss=0.43, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.844]embeddings shape: torch.Size([32, 768, 1553])
Epoch 0:  15%|‚ñà‚ñå        | 87/573 [23:44<2:12:39, 16.38s/it, loss=0.43, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.844]Epoch 0:  15%|‚ñà‚ñå        | 87/573 [23:44<2:12:39, 16.38s/it, loss=0.431, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.799]embeddings shape: torch.Size([32, 768, 1618])
Epoch 0:  15%|‚ñà‚ñå        | 88/573 [23:54<2:11:45, 16.30s/it, loss=0.431, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.799]Epoch 0:  15%|‚ñà‚ñå        | 88/573 [23:54<2:11:45, 16.30s/it, loss=0.431, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.830]Generating train split: 500 examples [23:01,  2.73s/ examples]embeddings shape: torch.Size([32, 768, 1630])
Epoch 0:  16%|‚ñà‚ñå        | 89/573 [24:23<2:12:39, 16.44s/it, loss=0.431, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.830]Epoch 0:  16%|‚ñà‚ñå        | 89/573 [24:23<2:12:39, 16.44s/it, loss=0.429, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861] embeddings shape: torch.Size([32, 768, 1613])
Epoch 0:  16%|‚ñà‚ñå        | 90/573 [24:33<2:11:47, 16.37s/it, loss=0.429, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861]Epoch 0:  16%|‚ñà‚ñå        | 90/573 [24:33<2:11:47, 16.37s/it, loss=0.429, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.819]embeddings shape: torch.Size([32, 768, 1606])
Epoch 0:  16%|‚ñà‚ñå        | 91/573 [24:44<2:11:04, 16.32s/it, loss=0.429, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.819]Epoch 0:  16%|‚ñà‚ñå        | 91/573 [24:44<2:11:04, 16.32s/it, loss=0.428, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.837]embeddings shape: torch.Size([32, 768, 1585])
Epoch 0:  16%|‚ñà‚ñå        | 92/573 [25:12<2:11:46, 16.44s/it, loss=0.428, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.837]Epoch 0:  16%|‚ñà‚ñå        | 92/573 [25:12<2:11:46, 16.44s/it, loss=0.429, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865] Generating train split: 737 examples [25:10,  2.12s/ examples]Generating train split: 671 examples [25:23,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1454])
Epoch 0:  16%|‚ñà‚ñå        | 93/573 [25:43<2:12:47, 16.60s/it, loss=0.429, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865]Epoch 0:  16%|‚ñà‚ñå        | 93/573 [25:43<2:12:47, 16.60s/it, loss=0.429, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.830]embeddings shape: torch.Size([32, 768, 1413])
Epoch 0:  16%|‚ñà‚ñã        | 94/573 [25:52<2:11:51, 16.52s/it, loss=0.429, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.830]Epoch 0:  16%|‚ñà‚ñã        | 94/573 [25:52<2:11:51, 16.52s/it, loss=0.43, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.826] embeddings shape: torch.Size([32, 768, 1598])
Epoch 0:  17%|‚ñà‚ñã        | 95/573 [26:01<2:10:57, 16.44s/it, loss=0.43, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.826]Epoch 0:  17%|‚ñà‚ñã        | 95/573 [26:01<2:10:57, 16.44s/it, loss=0.428, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861]embeddings shape: torch.Size([32, 768, 1593])
Epoch 0:  17%|‚ñà‚ñã        | 96/573 [26:10<2:10:01, 16.36s/it, loss=0.428, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861]Epoch 0:  17%|‚ñà‚ñã        | 96/573 [26:10<2:10:01, 16.36s/it, loss=0.426, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.830]embeddings shape: torch.Size([32, 768, 1619])
Epoch 0:  17%|‚ñà‚ñã        | 97/573 [26:35<2:10:28, 16.45s/it, loss=0.426, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.830]Epoch 0:  17%|‚ñà‚ñã        | 97/573 [26:35<2:10:28, 16.45s/it, loss=0.428, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.819]Generating train split: 550 examples [25:19,  2.74s/ examples]embeddings shape: torch.Size([32, 768, 1623])
Epoch 0:  17%|‚ñà‚ñã        | 98/573 [26:49<2:09:59, 16.42s/it, loss=0.428, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.819]Epoch 0:  17%|‚ñà‚ñã        | 98/573 [26:49<2:09:59, 16.42s/it, loss=0.423, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865] embeddings shape: torch.Size([32, 768, 1694])
Epoch 0:  17%|‚ñà‚ñã        | 99/573 [27:11<2:10:09, 16.48s/it, loss=0.423, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865]Epoch 0:  17%|‚ñà‚ñã        | 99/573 [27:11<2:10:09, 16.48s/it, loss=0.421, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851]Generating train split: 804 examples [27:10,  2.02s/ examples]embeddings shape: torch.Size([32, 768, 1564])
Epoch 0:  17%|‚ñà‚ñã        | 100/573 [27:23<2:09:33, 16.43s/it, loss=0.421, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851]Epoch 0:  17%|‚ñà‚ñã        | 100/573 [27:23<2:09:33, 16.43s/it, loss=0.416, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837]embeddings shape: torch.Size([32, 768, 1575])
Epoch 0:  18%|‚ñà‚ñä        | 101/573 [27:46<2:09:49, 16.50s/it, loss=0.416, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837]Epoch 0:  18%|‚ñà‚ñä        | 101/573 [27:46<2:09:49, 16.50s/it, loss=0.416, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840]Generating train split: 732 examples [27:32,  2.23s/ examples]embeddings shape: torch.Size([32, 768, 1502])
Epoch 0:  18%|‚ñà‚ñä        | 102/573 [27:58<2:09:09, 16.45s/it, loss=0.416, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840]Epoch 0:  18%|‚ñà‚ñä        | 102/573 [27:58<2:09:09, 16.45s/it, loss=0.418, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]embeddings shape: torch.Size([32, 768, 1637])
Epoch 0:  18%|‚ñà‚ñä        | 103/573 [28:07<2:08:19, 16.38s/it, loss=0.418, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]Epoch 0:  18%|‚ñà‚ñä        | 103/573 [28:07<2:08:19, 16.38s/it, loss=0.412, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.875]embeddings shape: torch.Size([32, 768, 1594])
Epoch 0:  18%|‚ñà‚ñä        | 104/573 [28:17<2:07:34, 16.32s/it, loss=0.412, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.875]Epoch 0:  18%|‚ñà‚ñä        | 104/573 [28:17<2:07:34, 16.32s/it, loss=0.41, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.840]embeddings shape: torch.Size([32, 768, 1575])
Epoch 0:  18%|‚ñà‚ñä        | 105/573 [28:28<2:06:52, 16.27s/it, loss=0.41, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.840]Epoch 0:  18%|‚ñà‚ñä        | 105/573 [28:28<2:06:52, 16.27s/it, loss=0.408, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.833]Generating train split: 600 examples [27:30,  2.71s/ examples]embeddings shape: torch.Size([32, 768, 1582])
Epoch 0:  18%|‚ñà‚ñä        | 106/573 [28:53<2:07:16, 16.35s/it, loss=0.408, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.833]Epoch 0:  18%|‚ñà‚ñä        | 106/573 [28:53<2:07:16, 16.35s/it, loss=0.409, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.844]embeddings shape: torch.Size([32, 768, 1600])
Epoch 0:  19%|‚ñà‚ñä        | 107/573 [29:16<2:07:31, 16.42s/it, loss=0.409, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.844]Epoch 0:  19%|‚ñà‚ñä        | 107/573 [29:16<2:07:31, 16.42s/it, loss=0.406, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.840]Generating train split: 871 examples [29:15,  1.98s/ examples]embeddings shape: torch.Size([32, 768, 1561])
Epoch 0:  19%|‚ñà‚ñâ        | 108/573 [29:30<2:07:04, 16.40s/it, loss=0.406, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.840]Epoch 0:  19%|‚ñà‚ñâ        | 108/573 [29:30<2:07:04, 16.40s/it, loss=0.406, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.816]embeddings shape: torch.Size([32, 768, 1061])
Epoch 0:  19%|‚ñà‚ñâ        | 109/573 [29:38<2:06:11, 16.32s/it, loss=0.406, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.816]Epoch 0:  19%|‚ñà‚ñâ        | 109/573 [29:38<2:06:11, 16.32s/it, loss=0.405, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847] Generating train split: 793 examples [29:46,  2.22s/ examples]embeddings shape: torch.Size([32, 768, 1583])
Epoch 0:  19%|‚ñà‚ñâ        | 110/573 [30:08<2:06:53, 16.44s/it, loss=0.405, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847]Epoch 0:  19%|‚ñà‚ñâ        | 110/573 [30:08<2:06:53, 16.44s/it, loss=0.403, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854]embeddings shape: torch.Size([32, 768, 1602])
Epoch 0:  19%|‚ñà‚ñâ        | 111/573 [30:19<2:06:12, 16.39s/it, loss=0.403, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854]Epoch 0:  19%|‚ñà‚ñâ        | 111/573 [30:19<2:06:12, 16.39s/it, loss=0.402, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844]embeddings shape: torch.Size([32, 768, 1624])
Epoch 0:  20%|‚ñà‚ñâ        | 112/573 [30:28<2:05:26, 16.33s/it, loss=0.402, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844]Epoch 0:  20%|‚ñà‚ñâ        | 112/573 [30:28<2:05:26, 16.33s/it, loss=0.402, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851]embeddings shape: torch.Size([32, 768, 1600])
Epoch 0:  20%|‚ñà‚ñâ        | 113/573 [30:43<2:05:06, 16.32s/it, loss=0.402, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851]Epoch 0:  20%|‚ñà‚ñâ        | 113/573 [30:43<2:05:06, 16.32s/it, loss=0.4, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875]  Generating train split: 650 examples [29:45,  2.70s/ examples]embeddings shape: torch.Size([32, 768, 1453])
Epoch 0:  20%|‚ñà‚ñâ        | 114/573 [31:14<2:05:46, 16.44s/it, loss=0.4, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875]Epoch 0:  20%|‚ñà‚ñâ        | 114/573 [31:14<2:05:46, 16.44s/it, loss=0.399, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844]Generating train split: 938 examples [31:23,  1.96s/ examples]embeddings shape: torch.Size([32, 768, 1559])
Epoch 0:  20%|‚ñà‚ñà        | 115/573 [31:34<2:05:47, 16.48s/it, loss=0.399, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844]Epoch 0:  20%|‚ñà‚ñà        | 115/573 [31:34<2:05:47, 16.48s/it, loss=0.403, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.799]embeddings shape: torch.Size([32, 768, 1615])
Epoch 0:  20%|‚ñà‚ñà        | 116/573 [31:44<2:05:02, 16.42s/it, loss=0.403, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.799]Epoch 0:  20%|‚ñà‚ñà        | 116/573 [31:44<2:05:02, 16.42s/it, loss=0.404, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851] embeddings shape: torch.Size([32, 768, 1603])
Epoch 0:  20%|‚ñà‚ñà        | 117/573 [31:56<2:04:31, 16.38s/it, loss=0.404, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851]Epoch 0:  20%|‚ñà‚ñà        | 117/573 [31:56<2:04:31, 16.38s/it, loss=0.403, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837]Generating train split: 854 examples [32:03,  2.23s/ examples]embeddings shape: torch.Size([32, 768, 1663])
Epoch 0:  21%|‚ñà‚ñà        | 118/573 [32:26<2:05:04, 16.49s/it, loss=0.403, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837]Epoch 0:  21%|‚ñà‚ñà        | 118/573 [32:26<2:05:04, 16.49s/it, loss=0.408, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.812]embeddings shape: torch.Size([32, 768, 1560])
Epoch 0:  21%|‚ñà‚ñà        | 119/573 [32:36<2:04:24, 16.44s/it, loss=0.408, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.812]Epoch 0:  21%|‚ñà‚ñà        | 119/573 [32:36<2:04:24, 16.44s/it, loss=0.408, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.816]embeddings shape: torch.Size([32, 768, 1591])
Epoch 0:  21%|‚ñà‚ñà        | 120/573 [32:43<2:03:31, 16.36s/it, loss=0.408, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.816]Epoch 0:  21%|‚ñà‚ñà        | 120/573 [32:43<2:03:31, 16.36s/it, loss=0.408, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872] embeddings shape: torch.Size([32, 768, 1613])
Epoch 0:  21%|‚ñà‚ñà        | 121/573 [32:54<2:02:54, 16.31s/it, loss=0.408, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872]Epoch 0:  21%|‚ñà‚ñà        | 121/573 [32:54<2:02:54, 16.31s/it, loss=0.409, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.816]Generating train split: 700 examples [32:02,  2.72s/ examples]embeddings shape: torch.Size([32, 768, 1560])
Epoch 0:  21%|‚ñà‚ñà‚ñè       | 122/573 [33:26<2:03:37, 16.45s/it, loss=0.409, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.816]Epoch 0:  21%|‚ñà‚ñà‚ñè       | 122/573 [33:26<2:03:37, 16.45s/it, loss=0.408, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847] embeddings shape: torch.Size([32, 768, 1613])
Epoch 0:  21%|‚ñà‚ñà‚ñè       | 123/573 [33:56<2:04:11, 16.56s/it, loss=0.408, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847]Epoch 0:  21%|‚ñà‚ñà‚ñè       | 123/573 [33:56<2:04:11, 16.56s/it, loss=0.41, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.837]Generating train split: 1005 examples [33:54,  2.04s/ examples]embeddings shape: torch.Size([32, 768, 1658])
Epoch 0:  22%|‚ñà‚ñà‚ñè       | 124/573 [34:08<2:03:38, 16.52s/it, loss=0.41, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.837]Epoch 0:  22%|‚ñà‚ñà‚ñè       | 124/573 [34:08<2:03:38, 16.52s/it, loss=0.413, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837]embeddings shape: torch.Size([32, 768, 1358])
Epoch 0:  22%|‚ñà‚ñà‚ñè       | 125/573 [34:19<2:03:02, 16.48s/it, loss=0.413, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837]Epoch 0:  22%|‚ñà‚ñà‚ñè       | 125/573 [34:19<2:03:02, 16.48s/it, loss=0.41, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.833]Generating train split: 915 examples [34:25,  2.26s/ examples]embeddings shape: torch.Size([32, 768, 1341])
Epoch 0:  22%|‚ñà‚ñà‚ñè       | 126/573 [34:46<2:03:21, 16.56s/it, loss=0.41, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.833]Epoch 0:  22%|‚ñà‚ñà‚ñè       | 126/573 [34:46<2:03:21, 16.56s/it, loss=0.409, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851]embeddings shape: torch.Size([32, 768, 1611])
Epoch 0:  22%|‚ñà‚ñà‚ñè       | 127/573 [34:55<2:02:38, 16.50s/it, loss=0.409, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851]Epoch 0:  22%|‚ñà‚ñà‚ñè       | 127/573 [34:55<2:02:38, 16.50s/it, loss=0.407, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844]embeddings shape: torch.Size([32, 768, 1599])
Epoch 0:  22%|‚ñà‚ñà‚ñè       | 128/573 [35:04<2:01:57, 16.44s/it, loss=0.407, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844]Epoch 0:  22%|‚ñà‚ñà‚ñè       | 128/573 [35:04<2:01:57, 16.44s/it, loss=0.407, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.851]embeddings shape: torch.Size([32, 768, 1615])
Epoch 0:  23%|‚ñà‚ñà‚ñé       | 129/573 [35:14<2:01:16, 16.39s/it, loss=0.407, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.851]Epoch 0:  23%|‚ñà‚ñà‚ñé       | 129/573 [35:14<2:01:16, 16.39s/it, loss=0.406, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.847]embeddings shape: torch.Size([32, 768, 1569])
Epoch 0:  23%|‚ñà‚ñà‚ñé       | 130/573 [35:33<2:01:11, 16.41s/it, loss=0.406, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.847]Epoch 0:  23%|‚ñà‚ñà‚ñé       | 130/573 [35:33<2:01:11, 16.41s/it, loss=0.407, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.816]Generating train split: 750 examples [34:41,  2.85s/ examples]embeddings shape: torch.Size([32, 768, 1586])
Epoch 0:  23%|‚ñà‚ñà‚ñé       | 131/573 [36:03<2:01:39, 16.51s/it, loss=0.407, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.816]Epoch 0:  23%|‚ñà‚ñà‚ñé       | 131/573 [36:03<2:01:39, 16.51s/it, loss=0.409, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.833]Generating train split: 1072 examples [36:00,  1.99s/ examples]embeddings shape: torch.Size([32, 768, 1337])
Epoch 0:  23%|‚ñà‚ñà‚ñé       | 132/573 [36:13<2:01:01, 16.47s/it, loss=0.409, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.833]Epoch 0:  23%|‚ñà‚ñà‚ñé       | 132/573 [36:13<2:01:01, 16.47s/it, loss=0.407, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837] embeddings shape: torch.Size([32, 768, 1589])
Epoch 0:  23%|‚ñà‚ñà‚ñé       | 133/573 [36:20<2:00:14, 16.40s/it, loss=0.407, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837]Epoch 0:  23%|‚ñà‚ñà‚ñé       | 133/573 [36:20<2:00:14, 16.40s/it, loss=0.409, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854]embeddings shape: torch.Size([32, 768, 1609])
Epoch 0:  23%|‚ñà‚ñà‚ñé       | 134/573 [36:37<1:59:59, 16.40s/it, loss=0.409, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854]Epoch 0:  23%|‚ñà‚ñà‚ñé       | 134/573 [36:37<1:59:59, 16.40s/it, loss=0.406, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868]Generating train split: 976 examples [36:33,  2.21s/ examples]embeddings shape: torch.Size([32, 768, 1306])
Epoch 0:  24%|‚ñà‚ñà‚ñé       | 135/573 [36:57<1:59:54, 16.43s/it, loss=0.406, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868]Epoch 0:  24%|‚ñà‚ñà‚ñé       | 135/573 [36:57<1:59:54, 16.43s/it, loss=0.403, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854]embeddings shape: torch.Size([32, 768, 1600])
Epoch 0:  24%|‚ñà‚ñà‚ñé       | 136/573 [37:07<1:59:16, 16.38s/it, loss=0.403, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854]Epoch 0:  24%|‚ñà‚ñà‚ñé       | 136/573 [37:07<1:59:16, 16.38s/it, loss=0.401, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.844]embeddings shape: torch.Size([32, 768, 1575])
Epoch 0:  24%|‚ñà‚ñà‚ñç       | 137/573 [37:16<1:58:37, 16.33s/it, loss=0.401, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.844]Epoch 0:  24%|‚ñà‚ñà‚ñç       | 137/573 [37:16<1:58:37, 16.33s/it, loss=0.398, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.826]embeddings shape: torch.Size([32, 768, 1554])
Epoch 0:  24%|‚ñà‚ñà‚ñç       | 138/573 [37:33<1:58:22, 16.33s/it, loss=0.398, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.826]Epoch 0:  24%|‚ñà‚ñà‚ñç       | 138/573 [37:33<1:58:22, 16.33s/it, loss=0.397, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840] Generating train split: 1139 examples [37:42,  1.85s/ examples]embeddings shape: torch.Size([32, 768, 1599])
Epoch 0:  24%|‚ñà‚ñà‚ñç       | 139/573 [38:07<1:59:01, 16.45s/it, loss=0.397, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840]Epoch 0:  24%|‚ñà‚ñà‚ñç       | 139/573 [38:07<1:59:01, 16.45s/it, loss=0.398, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847]Generating train split: 800 examples [36:57,  2.81s/ examples]embeddings shape: torch.Size([32, 768, 1352])
Epoch 0:  24%|‚ñà‚ñà‚ñç       | 140/573 [38:21<1:58:39, 16.44s/it, loss=0.398, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847]Epoch 0:  24%|‚ñà‚ñà‚ñç       | 140/573 [38:21<1:58:39, 16.44s/it, loss=0.395, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.882]embeddings shape: torch.Size([32, 768, 1581])
Epoch 0:  25%|‚ñà‚ñà‚ñç       | 141/573 [38:29<1:57:55, 16.38s/it, loss=0.395, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.882]Epoch 0:  25%|‚ñà‚ñà‚ñç       | 141/573 [38:29<1:57:55, 16.38s/it, loss=0.394, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]embeddings shape: torch.Size([32, 768, 1605])
Epoch 0:  25%|‚ñà‚ñà‚ñç       | 142/573 [38:47<1:57:45, 16.39s/it, loss=0.394, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]Epoch 0:  25%|‚ñà‚ñà‚ñç       | 142/573 [38:47<1:57:45, 16.39s/it, loss=0.393, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833]Generating train split: 1037 examples [38:46,  2.20s/ examples]embeddings shape: torch.Size([32, 768, 1595])
Epoch 0:  25%|‚ñà‚ñà‚ñç       | 143/573 [39:09<1:57:44, 16.43s/it, loss=0.393, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833]Epoch 0:  25%|‚ñà‚ñà‚ñç       | 143/573 [39:09<1:57:44, 16.43s/it, loss=0.392, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837]embeddings shape: torch.Size([32, 768, 1565])
Epoch 0:  25%|‚ñà‚ñà‚ñå       | 144/573 [39:17<1:57:02, 16.37s/it, loss=0.392, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837]Epoch 0:  25%|‚ñà‚ñà‚ñå       | 144/573 [39:17<1:57:02, 16.37s/it, loss=0.392, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.792]embeddings shape: torch.Size([32, 768, 1530])
Epoch 0:  25%|‚ñà‚ñà‚ñå       | 145/573 [39:25<1:56:23, 16.32s/it, loss=0.392, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.792]Epoch 0:  25%|‚ñà‚ñà‚ñå       | 145/573 [39:25<1:56:23, 16.32s/it, loss=0.393, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.826]embeddings shape: torch.Size([32, 768, 1606])
Epoch 0:  25%|‚ñà‚ñà‚ñå       | 146/573 [39:35<1:55:47, 16.27s/it, loss=0.393, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.826]Epoch 0:  25%|‚ñà‚ñà‚ñå       | 146/573 [39:35<1:55:47, 16.27s/it, loss=0.393, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844]Generating train split: 1206 examples [39:53,  1.88s/ examples]embeddings shape: torch.Size([32, 768, 1563])
Epoch 0:  26%|‚ñà‚ñà‚ñå       | 147/573 [40:00<1:55:57, 16.33s/it, loss=0.393, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844]Epoch 0:  26%|‚ñà‚ñà‚ñå       | 147/573 [40:00<1:55:57, 16.33s/it, loss=0.393, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840]embeddings shape: torch.Size([32, 768, 1313])
Epoch 0:  26%|‚ñà‚ñà‚ñå       | 148/573 [40:25<1:56:05, 16.39s/it, loss=0.393, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840]Epoch 0:  26%|‚ñà‚ñà‚ñå       | 148/573 [40:25<1:56:05, 16.39s/it, loss=0.395, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.795]Generating train split: 850 examples [39:15,  2.80s/ examples]embeddings shape: torch.Size([32, 768, 1571])
Epoch 0:  26%|‚ñà‚ñà‚ñå       | 149/573 [40:40<1:55:44, 16.38s/it, loss=0.395, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.795]Epoch 0:  26%|‚ñà‚ñà‚ñå       | 149/573 [40:40<1:55:44, 16.38s/it, loss=0.398, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.816] embeddings shape: torch.Size([32, 768, 1582])
Epoch 0:  26%|‚ñà‚ñà‚ñå       | 150/573 [40:51<1:55:11, 16.34s/it, loss=0.398, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.816]Epoch 0:  26%|‚ñà‚ñà‚ñå       | 150/573 [40:51<1:55:11, 16.34s/it, loss=0.399, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.799]embeddings shape: torch.Size([32, 768, 1340])
Epoch 0:  26%|‚ñà‚ñà‚ñã       | 151/573 [41:14<1:55:16, 16.39s/it, loss=0.399, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.799]Epoch 0:  26%|‚ñà‚ñà‚ñã       | 151/573 [41:14<1:55:16, 16.39s/it, loss=0.398, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823] Generating train split: 1098 examples [41:06,  2.23s/ examples]embeddings shape: torch.Size([32, 768, 1671])
Epoch 0:  27%|‚ñà‚ñà‚ñã       | 152/573 [41:31<1:55:01, 16.39s/it, loss=0.398, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823]Epoch 0:  27%|‚ñà‚ñà‚ñã       | 152/573 [41:31<1:55:01, 16.39s/it, loss=0.399, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851]embeddings shape: torch.Size([32, 768, 1631])
Epoch 0:  27%|‚ñà‚ñà‚ñã       | 153/573 [41:39<1:54:20, 16.33s/it, loss=0.399, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851]Epoch 0:  27%|‚ñà‚ñà‚ñã       | 153/573 [41:39<1:54:20, 16.33s/it, loss=0.397, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861]embeddings shape: torch.Size([32, 768, 1557])
Epoch 0:  27%|‚ñà‚ñà‚ñã       | 154/573 [41:48<1:53:45, 16.29s/it, loss=0.397, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861]Epoch 0:  27%|‚ñà‚ñà‚ñã       | 154/573 [41:48<1:53:45, 16.29s/it, loss=0.398, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]embeddings shape: torch.Size([32, 768, 1573])
Epoch 0:  27%|‚ñà‚ñà‚ñã       | 155/573 [41:57<1:53:09, 16.24s/it, loss=0.398, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]Epoch 0:  27%|‚ñà‚ñà‚ñã       | 155/573 [41:57<1:53:09, 16.24s/it, loss=0.4, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.816]  embeddings shape: torch.Size([32, 768, 1588])
Epoch 0:  27%|‚ñà‚ñà‚ñã       | 156/573 [42:21<1:53:13, 16.29s/it, loss=0.4, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.816]Epoch 0:  27%|‚ñà‚ñà‚ñã       | 156/573 [42:21<1:53:13, 16.29s/it, loss=0.4, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840]Generating train split: 1273 examples [42:29,  2.02s/ examples]embeddings shape: torch.Size([32, 768, 1456])
Epoch 0:  27%|‚ñà‚ñà‚ñã       | 157/573 [42:52<1:53:37, 16.39s/it, loss=0.4, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840]Epoch 0:  27%|‚ñà‚ñà‚ñã       | 157/573 [42:52<1:53:37, 16.39s/it, loss=0.4, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.847]Generating train split: 900 examples [41:39,  2.82s/ examples]embeddings shape: torch.Size([32, 768, 1599])
Epoch 0:  28%|‚ñà‚ñà‚ñä       | 158/573 [43:06<1:53:13, 16.37s/it, loss=0.4, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.847]Epoch 0:  28%|‚ñà‚ñà‚ñä       | 158/573 [43:06<1:53:13, 16.37s/it, loss=0.399, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833]embeddings shape: torch.Size([32, 768, 1628])
Epoch 0:  28%|‚ñà‚ñà‚ñä       | 159/573 [43:15<1:52:39, 16.33s/it, loss=0.399, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833]Epoch 0:  28%|‚ñà‚ñà‚ñä       | 159/573 [43:15<1:52:39, 16.33s/it, loss=0.396, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.851]embeddings shape: torch.Size([32, 768, 1475])
Epoch 0:  28%|‚ñà‚ñà‚ñä       | 160/573 [43:37<1:52:35, 16.36s/it, loss=0.396, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.851]Epoch 0:  28%|‚ñà‚ñà‚ñä       | 160/573 [43:37<1:52:35, 16.36s/it, loss=0.399, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847]Generating train split: 1159 examples [43:28,  2.26s/ examples]embeddings shape: torch.Size([32, 768, 1590])
Epoch 0:  28%|‚ñà‚ñà‚ñä       | 161/573 [43:56<1:52:26, 16.38s/it, loss=0.399, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847]Epoch 0:  28%|‚ñà‚ñà‚ñä       | 161/573 [43:56<1:52:26, 16.38s/it, loss=0.399, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.816]embeddings shape: torch.Size([32, 768, 1536])
Epoch 0:  28%|‚ñà‚ñà‚ñä       | 162/573 [44:04<1:51:47, 16.32s/it, loss=0.399, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.816]Epoch 0:  28%|‚ñà‚ñà‚ñä       | 162/573 [44:04<1:51:47, 16.32s/it, loss=0.397, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854] embeddings shape: torch.Size([32, 768, 1659])
Epoch 0:  28%|‚ñà‚ñà‚ñä       | 163/573 [44:13<1:51:15, 16.28s/it, loss=0.397, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854]Epoch 0:  28%|‚ñà‚ñà‚ñä       | 163/573 [44:13<1:51:15, 16.28s/it, loss=0.397, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837]embeddings shape: torch.Size([32, 768, 1582])
Epoch 0:  29%|‚ñà‚ñà‚ñä       | 164/573 [44:22<1:50:40, 16.24s/it, loss=0.397, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837]Epoch 0:  29%|‚ñà‚ñà‚ñä       | 164/573 [44:22<1:50:40, 16.24s/it, loss=0.393, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875]embeddings shape: torch.Size([32, 768, 1404])
Epoch 0:  29%|‚ñà‚ñà‚ñâ       | 165/573 [44:30<1:50:03, 16.18s/it, loss=0.393, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875]Epoch 0:  29%|‚ñà‚ñà‚ñâ       | 165/573 [44:30<1:50:03, 16.18s/it, loss=0.395, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.830]embeddings shape: torch.Size([32, 768, 1638])
Epoch 0:  29%|‚ñà‚ñà‚ñâ       | 166/573 [45:03<1:50:27, 16.28s/it, loss=0.395, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.830]Epoch 0:  29%|‚ñà‚ñà‚ñâ       | 166/573 [45:03<1:50:27, 16.28s/it, loss=0.395, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865]Generating train split: 950 examples [44:01,  2.83s/ examples]Generating train split: 1340 examples [45:21,  2.18s/ examples]embeddings shape: torch.Size([32, 768, 1556])
Epoch 0:  29%|‚ñà‚ñà‚ñâ       | 167/573 [45:28<1:50:32, 16.34s/it, loss=0.395, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865]Epoch 0:  29%|‚ñà‚ñà‚ñâ       | 167/573 [45:28<1:50:32, 16.34s/it, loss=0.395, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]embeddings shape: torch.Size([32, 768, 1597])
Epoch 0:  29%|‚ñà‚ñà‚ñâ       | 168/573 [45:47<1:50:23, 16.35s/it, loss=0.395, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]Epoch 0:  29%|‚ñà‚ñà‚ñâ       | 168/573 [45:47<1:50:23, 16.35s/it, loss=0.393, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.858]Generating train split: 1220 examples [45:45,  2.25s/ examples]embeddings shape: torch.Size([32, 768, 1337])
Epoch 0:  29%|‚ñà‚ñà‚ñâ       | 169/573 [46:09<1:50:20, 16.39s/it, loss=0.393, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.858]Epoch 0:  29%|‚ñà‚ñà‚ñâ       | 169/573 [46:09<1:50:20, 16.39s/it, loss=0.39, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885] embeddings shape: torch.Size([32, 768, 1560])
Epoch 0:  30%|‚ñà‚ñà‚ñâ       | 170/573 [46:17<1:49:45, 16.34s/it, loss=0.39, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885]Epoch 0:  30%|‚ñà‚ñà‚ñâ       | 170/573 [46:17<1:49:45, 16.34s/it, loss=0.386, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.854]embeddings shape: torch.Size([32, 768, 1644])
Epoch 0:  30%|‚ñà‚ñà‚ñâ       | 171/573 [46:26<1:49:10, 16.29s/it, loss=0.386, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.854]Epoch 0:  30%|‚ñà‚ñà‚ñâ       | 171/573 [46:26<1:49:10, 16.29s/it, loss=0.388, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858]embeddings shape: torch.Size([32, 768, 1551])
Epoch 0:  30%|‚ñà‚ñà‚ñà       | 172/573 [46:34<1:48:35, 16.25s/it, loss=0.388, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858]Epoch 0:  30%|‚ñà‚ñà‚ñà       | 172/573 [46:34<1:48:35, 16.25s/it, loss=0.388, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.823]embeddings shape: torch.Size([32, 768, 1628])
Epoch 0:  30%|‚ñà‚ñà‚ñà       | 173/573 [46:42<1:47:59, 16.20s/it, loss=0.388, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.823]Epoch 0:  30%|‚ñà‚ñà‚ñà       | 173/573 [46:42<1:47:59, 16.20s/it, loss=0.393, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.792] embeddings shape: torch.Size([32, 768, 1236])
Epoch 0:  30%|‚ñà‚ñà‚ñà       | 174/573 [46:50<1:47:24, 16.15s/it, loss=0.393, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.792]Epoch 0:  30%|‚ñà‚ñà‚ñà       | 174/573 [46:50<1:47:24, 16.15s/it, loss=0.391, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858]embeddings shape: torch.Size([32, 768, 1571])
Epoch 0:  31%|‚ñà‚ñà‚ñà       | 175/573 [47:06<1:47:08, 16.15s/it, loss=0.391, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858]Epoch 0:  31%|‚ñà‚ñà‚ñà       | 175/573 [47:06<1:47:08, 16.15s/it, loss=0.389, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851]embeddings shape: torch.Size([32, 768, 1620])
Epoch 0:  31%|‚ñà‚ñà‚ñà       | 176/573 [47:31<1:47:13, 16.20s/it, loss=0.389, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851]Epoch 0:  31%|‚ñà‚ñà‚ñà       | 176/573 [47:31<1:47:13, 16.20s/it, loss=0.387, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.885]Generating train split: 1000 examples [46:22,  2.83s/ examples]Generating train split: 1407 examples [47:48,  2.19s/ examples]embeddings shape: torch.Size([32, 768, 1552])
Epoch 0:  31%|‚ñà‚ñà‚ñà       | 177/573 [47:56<1:47:15, 16.25s/it, loss=0.387, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.885]Epoch 0:  31%|‚ñà‚ñà‚ñà       | 177/573 [47:56<1:47:15, 16.25s/it, loss=0.386, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.896]Generating train split: 1281 examples [48:06,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1687])
Epoch 0:  31%|‚ñà‚ñà‚ñà       | 178/573 [48:28<1:47:34, 16.34s/it, loss=0.386, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.896]Epoch 0:  31%|‚ñà‚ñà‚ñà       | 178/573 [48:28<1:47:34, 16.34s/it, loss=0.386, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854]embeddings shape: torch.Size([32, 768, 1584])
Epoch 0:  31%|‚ñà‚ñà‚ñà       | 179/573 [48:39<1:47:06, 16.31s/it, loss=0.386, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854]Epoch 0:  31%|‚ñà‚ñà‚ñà       | 179/573 [48:39<1:47:06, 16.31s/it, loss=0.385, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.878]embeddings shape: torch.Size([32, 768, 1533])
Epoch 0:  31%|‚ñà‚ñà‚ñà‚ñè      | 180/573 [48:46<1:46:29, 16.26s/it, loss=0.385, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.878]Epoch 0:  31%|‚ñà‚ñà‚ñà‚ñè      | 180/573 [48:46<1:46:29, 16.26s/it, loss=0.384, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837]embeddings shape: torch.Size([32, 768, 1591])
Epoch 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 181/573 [48:55<1:45:57, 16.22s/it, loss=0.384, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837]Epoch 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 181/573 [48:55<1:45:57, 16.22s/it, loss=0.381, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868]embeddings shape: torch.Size([32, 768, 1573])
Epoch 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 182/573 [49:04<1:45:25, 16.18s/it, loss=0.381, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868]Epoch 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 182/573 [49:04<1:45:25, 16.18s/it, loss=0.383, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844]embeddings shape: torch.Size([32, 768, 1548])
Epoch 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 183/573 [49:13<1:44:53, 16.14s/it, loss=0.383, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844]Epoch 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 183/573 [49:13<1:44:53, 16.14s/it, loss=0.38, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.878] embeddings shape: torch.Size([32, 768, 1623])
Epoch 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 184/573 [49:27<1:44:34, 16.13s/it, loss=0.38, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.878]Epoch 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 184/573 [49:27<1:44:34, 16.13s/it, loss=0.38, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.861]embeddings shape: torch.Size([32, 768, 1534])
Epoch 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 185/573 [49:59<1:44:51, 16.22s/it, loss=0.38, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.861]Epoch 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 185/573 [49:59<1:44:51, 16.22s/it, loss=0.377, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.851]Generating train split: 1050 examples [48:53,  2.88s/ examples]Generating train split: 1474 examples [50:18,  2.20s/ examples]embeddings shape: torch.Size([32, 768, 1585])
Epoch 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 186/573 [50:29<1:45:02, 16.29s/it, loss=0.377, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.851]Epoch 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 186/573 [50:29<1:45:02, 16.29s/it, loss=0.377, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840]Generating train split: 1342 examples [50:25,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1588])
Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 187/573 [50:50<1:44:56, 16.31s/it, loss=0.377, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840]Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 187/573 [50:50<1:44:56, 16.31s/it, loss=0.377, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.833]embeddings shape: torch.Size([32, 768, 1578])
Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 188/573 [50:58<1:44:23, 16.27s/it, loss=0.377, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.833]Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 188/573 [50:58<1:44:23, 16.27s/it, loss=0.376, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851] embeddings shape: torch.Size([32, 768, 1605])
Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 189/573 [51:07<1:43:51, 16.23s/it, loss=0.376, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851]Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 189/573 [51:07<1:43:51, 16.23s/it, loss=0.38, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.802] embeddings shape: torch.Size([32, 768, 1645])
Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 190/573 [51:15<1:43:19, 16.19s/it, loss=0.38, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.802]Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 190/573 [51:15<1:43:19, 16.19s/it, loss=0.38, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847]embeddings shape: torch.Size([32, 768, 1618])
Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 191/573 [51:23<1:42:46, 16.14s/it, loss=0.38, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847]Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 191/573 [51:23<1:42:46, 16.14s/it, loss=0.379, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.878]embeddings shape: torch.Size([32, 768, 1552])
Epoch 0:  34%|‚ñà‚ñà‚ñà‚ñé      | 192/573 [51:30<1:42:11, 16.09s/it, loss=0.379, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.878]Epoch 0:  34%|‚ñà‚ñà‚ñà‚ñé      | 192/573 [51:30<1:42:11, 16.09s/it, loss=0.377, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.833]embeddings shape: torch.Size([32, 768, 1614])
Epoch 0:  34%|‚ñà‚ñà‚ñà‚ñé      | 193/573 [51:39<1:41:42, 16.06s/it, loss=0.377, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.833]Epoch 0:  34%|‚ñà‚ñà‚ñà‚ñé      | 193/573 [51:39<1:41:42, 16.06s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.882]  embeddings shape: torch.Size([32, 768, 1616])
Epoch 0:  34%|‚ñà‚ñà‚ñà‚ñç      | 194/573 [52:03<1:41:41, 16.10s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.882]Epoch 0:  34%|‚ñà‚ñà‚ñà‚ñç      | 194/573 [52:03<1:41:41, 16.10s/it, loss=0.371, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833]Generating train split: 1100 examples [50:56,  2.76s/ examples]embeddings shape: torch.Size([32, 768, 1625])
Epoch 0:  34%|‚ñà‚ñà‚ñà‚ñç      | 195/573 [52:35<1:41:57, 16.18s/it, loss=0.371, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833]Epoch 0:  34%|‚ñà‚ñà‚ñà‚ñç      | 195/573 [52:35<1:41:57, 16.18s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.861]Generating train split: 1403 examples [52:40,  2.25s/ examples]embeddings shape: torch.Size([32, 768, 1592])
Epoch 0:  34%|‚ñà‚ñà‚ñà‚ñç      | 196/573 [53:04<1:42:05, 16.25s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.861]Epoch 0:  34%|‚ñà‚ñà‚ñà‚ñç      | 196/573 [53:04<1:42:05, 16.25s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.875]Generating train split: 1541 examples [53:05,  2.29s/ examples]embeddings shape: torch.Size([32, 768, 1592])
Epoch 0:  34%|‚ñà‚ñà‚ñà‚ñç      | 197/573 [53:18<1:41:44, 16.24s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.875]Epoch 0:  34%|‚ñà‚ñà‚ñà‚ñç      | 197/573 [53:18<1:41:44, 16.24s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.812]embeddings shape: torch.Size([32, 768, 1564])
Epoch 0:  35%|‚ñà‚ñà‚ñà‚ñç      | 198/573 [53:26<1:41:11, 16.19s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.812]Epoch 0:  35%|‚ñà‚ñà‚ñà‚ñç      | 198/573 [53:26<1:41:11, 16.19s/it, loss=0.371, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.865]embeddings shape: torch.Size([32, 768, 1543])
Epoch 0:  35%|‚ñà‚ñà‚ñà‚ñç      | 199/573 [53:33<1:40:39, 16.15s/it, loss=0.371, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.865]Epoch 0:  35%|‚ñà‚ñà‚ñà‚ñç      | 199/573 [53:33<1:40:39, 16.15s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868] embeddings shape: torch.Size([32, 768, 1565])
Epoch 0:  35%|‚ñà‚ñà‚ñà‚ñç      | 200/573 [53:40<1:40:07, 16.10s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868]Epoch 0:  35%|‚ñà‚ñà‚ñà‚ñç      | 200/573 [53:40<1:40:07, 16.10s/it, loss=0.371, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.826]embeddings shape: torch.Size([32, 768, 1585])
Epoch 0:  35%|‚ñà‚ñà‚ñà‚ñå      | 201/573 [53:50<1:39:38, 16.07s/it, loss=0.371, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.826]Epoch 0:  35%|‚ñà‚ñà‚ñà‚ñå      | 201/573 [53:50<1:39:38, 16.07s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858]embeddings shape: torch.Size([32, 768, 1648])
Epoch 0:  35%|‚ñà‚ñà‚ñà‚ñå      | 202/573 [53:59<1:39:09, 16.04s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858]Epoch 0:  35%|‚ñà‚ñà‚ñà‚ñå      | 202/573 [53:59<1:39:09, 16.04s/it, loss=0.371, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.830]embeddings shape: torch.Size([32, 768, 1556])
Epoch 0:  35%|‚ñà‚ñà‚ñà‚ñå      | 203/573 [54:28<1:39:17, 16.10s/it, loss=0.371, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.830]Epoch 0:  35%|‚ñà‚ñà‚ñà‚ñå      | 203/573 [54:28<1:39:17, 16.10s/it, loss=0.373, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.826]Generating train split: 1150 examples [53:13,  2.75s/ examples]embeddings shape: torch.Size([32, 768, 1567])
Epoch 0:  36%|‚ñà‚ñà‚ñà‚ñå      | 204/573 [54:55<1:39:20, 16.15s/it, loss=0.373, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.826]Epoch 0:  36%|‚ñà‚ñà‚ñà‚ñå      | 204/573 [54:55<1:39:20, 16.15s/it, loss=0.374, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823]Generating train split: 1608 examples [54:56,  2.10s/ examples]Generating train split: 1464 examples [55:02,  2.28s/ examples]embeddings shape: torch.Size([32, 768, 1651])
Epoch 0:  36%|‚ñà‚ñà‚ñà‚ñå      | 205/573 [55:24<1:39:28, 16.22s/it, loss=0.374, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823]Epoch 0:  36%|‚ñà‚ñà‚ñà‚ñå      | 205/573 [55:24<1:39:28, 16.22s/it, loss=0.373, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854]embeddings shape: torch.Size([32, 768, 1573])
Epoch 0:  36%|‚ñà‚ñà‚ñà‚ñå      | 206/573 [55:33<1:38:58, 16.18s/it, loss=0.373, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854]Epoch 0:  36%|‚ñà‚ñà‚ñà‚ñå      | 206/573 [55:33<1:38:58, 16.18s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875]embeddings shape: torch.Size([32, 768, 1657])
Epoch 0:  36%|‚ñà‚ñà‚ñà‚ñå      | 207/573 [55:42<1:38:29, 16.15s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875]Epoch 0:  36%|‚ñà‚ñà‚ñà‚ñå      | 207/573 [55:42<1:38:29, 16.15s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.819]embeddings shape: torch.Size([32, 768, 1560])
Epoch 0:  36%|‚ñà‚ñà‚ñà‚ñã      | 208/573 [55:51<1:38:00, 16.11s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.819]Epoch 0:  36%|‚ñà‚ñà‚ñà‚ñã      | 208/573 [55:51<1:38:00, 16.11s/it, loss=0.373, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.840]embeddings shape: torch.Size([32, 768, 1555])
Epoch 0:  36%|‚ñà‚ñà‚ñà‚ñã      | 209/573 [55:59<1:37:30, 16.07s/it, loss=0.373, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.840]Epoch 0:  36%|‚ñà‚ñà‚ñà‚ñã      | 209/573 [55:59<1:37:30, 16.07s/it, loss=0.369, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868]embeddings shape: torch.Size([32, 768, 1598])
Epoch 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 210/573 [56:07<1:37:00, 16.04s/it, loss=0.369, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868]Epoch 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 210/573 [56:07<1:37:00, 16.04s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840] embeddings shape: torch.Size([32, 768, 1588])
Epoch 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 211/573 [56:20<1:36:39, 16.02s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840]Epoch 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 211/573 [56:20<1:36:39, 16.02s/it, loss=0.368, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.882]Generating train split: 1200 examples [55:18,  2.67s/ examples]embeddings shape: torch.Size([32, 768, 1545])
Epoch 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 212/573 [56:42<1:36:34, 16.05s/it, loss=0.368, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.882]Epoch 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 212/573 [56:42<1:36:34, 16.05s/it, loss=0.367, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.858]embeddings shape: torch.Size([32, 768, 1634])
Epoch 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 213/573 [57:09<1:36:36, 16.10s/it, loss=0.367, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.858]Epoch 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 213/573 [57:09<1:36:36, 16.10s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.837]Generating train split: 1675 examples [57:28,  2.15s/ examples]embeddings shape: torch.Size([32, 768, 1565])
Epoch 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 214/573 [57:38<1:36:41, 16.16s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.837]Epoch 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 214/573 [57:38<1:36:41, 16.16s/it, loss=0.367, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872]Generating train split: 1525 examples [57:19,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1215])
Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 215/573 [57:46<1:36:11, 16.12s/it, loss=0.367, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872]Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 215/573 [57:46<1:36:11, 16.12s/it, loss=0.367, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.823]embeddings shape: torch.Size([32, 768, 1568])
Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 216/573 [57:55<1:35:43, 16.09s/it, loss=0.367, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.823]Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 216/573 [57:55<1:35:43, 16.09s/it, loss=0.369, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844] embeddings shape: torch.Size([32, 768, 1551])
Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 217/573 [58:02<1:35:13, 16.05s/it, loss=0.369, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844]Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 217/573 [58:02<1:35:13, 16.05s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.823]embeddings shape: torch.Size([32, 768, 1620])
Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 218/573 [58:10<1:34:44, 16.01s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.823]Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 218/573 [58:10<1:34:44, 16.01s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840] embeddings shape: torch.Size([32, 768, 1566])
Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 219/573 [58:19<1:34:16, 15.98s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 219/573 [58:19<1:34:16, 15.98s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]Generating train split: 1250 examples [57:28,  2.65s/ examples]embeddings shape: torch.Size([32, 768, 1647])
Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 220/573 [58:49<1:34:23, 16.05s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 220/573 [58:49<1:34:23, 16.05s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.830]embeddings shape: torch.Size([32, 768, 1525])
Epoch 0:  39%|‚ñà‚ñà‚ñà‚ñä      | 221/573 [58:58<1:33:55, 16.01s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.830]Epoch 0:  39%|‚ñà‚ñà‚ñà‚ñä      | 221/573 [58:58<1:33:55, 16.01s/it, loss=0.371, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]embeddings shape: torch.Size([32, 768, 1560])
Epoch 0:  39%|‚ñà‚ñà‚ñà‚ñä      | 222/573 [59:23<1:33:53, 16.05s/it, loss=0.371, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]Epoch 0:  39%|‚ñà‚ñà‚ñà‚ñä      | 222/573 [59:23<1:33:53, 16.05s/it, loss=0.371, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.816]Generating train split: 1742 examples [59:26,  2.04s/ examples]embeddings shape: torch.Size([32, 768, 1590])
Epoch 0:  39%|‚ñà‚ñà‚ñà‚ñâ      | 223/573 [59:54<1:34:01, 16.12s/it, loss=0.371, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.816]Epoch 0:  39%|‚ñà‚ñà‚ñà‚ñâ      | 223/573 [59:54<1:34:01, 16.12s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847]Generating train split: 1586 examples [59:35,  2.26s/ examples]embeddings shape: torch.Size([32, 768, 1589])
Epoch 0:  39%|‚ñà‚ñà‚ñà‚ñâ      | 224/573 [1:00:03<1:33:34, 16.09s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847]Epoch 0:  39%|‚ñà‚ñà‚ñà‚ñâ      | 224/573 [1:00:03<1:33:34, 16.09s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.847]embeddings shape: torch.Size([32, 768, 1650])
Epoch 0:  39%|‚ñà‚ñà‚ñà‚ñâ      | 225/573 [1:00:13<1:33:08, 16.06s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.847]Epoch 0:  39%|‚ñà‚ñà‚ñà‚ñâ      | 225/573 [1:00:13<1:33:08, 16.06s/it, loss=0.373, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833]embeddings shape: torch.Size([32, 768, 1588])
Epoch 0:  39%|‚ñà‚ñà‚ñà‚ñâ      | 226/573 [1:00:22<1:32:41, 16.03s/it, loss=0.373, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833]Epoch 0:  39%|‚ñà‚ñà‚ñà‚ñâ      | 226/573 [1:00:22<1:32:41, 16.03s/it, loss=0.371, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847]embeddings shape: torch.Size([32, 768, 1590])
Epoch 0:  40%|‚ñà‚ñà‚ñà‚ñâ      | 227/573 [1:00:30<1:32:13, 15.99s/it, loss=0.371, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847]Epoch 0:  40%|‚ñà‚ñà‚ñà‚ñâ      | 227/573 [1:00:30<1:32:13, 15.99s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833]embeddings shape: torch.Size([32, 768, 1614])
Epoch 0:  40%|‚ñà‚ñà‚ñà‚ñâ      | 228/573 [1:00:46<1:31:57, 15.99s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833]Epoch 0:  40%|‚ñà‚ñà‚ñà‚ñâ      | 228/573 [1:00:46<1:31:57, 15.99s/it, loss=0.373, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.830]Generating train split: 1300 examples [59:46,  2.69s/ examples]embeddings shape: torch.Size([32, 768, 1583])
Epoch 0:  40%|‚ñà‚ñà‚ñà‚ñâ      | 229/573 [1:01:10<1:31:53, 16.03s/it, loss=0.373, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.830]Epoch 0:  40%|‚ñà‚ñà‚ñà‚ñâ      | 229/573 [1:01:10<1:31:53, 16.03s/it, loss=0.375, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.802]embeddings shape: torch.Size([32, 768, 1594])
Epoch 0:  40%|‚ñà‚ñà‚ñà‚ñà      | 230/573 [1:01:37<1:31:53, 16.07s/it, loss=0.375, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.802]Epoch 0:  40%|‚ñà‚ñà‚ñà‚ñà      | 230/573 [1:01:37<1:31:53, 16.07s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868]Generating train split: 1809 examples [1:01:38,  2.01s/ examples]embeddings shape: torch.Size([32, 768, 1594])
Epoch 0:  40%|‚ñà‚ñà‚ñà‚ñà      | 231/573 [1:02:09<1:32:01, 16.15s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868]Epoch 0:  40%|‚ñà‚ñà‚ñà‚ñà      | 231/573 [1:02:09<1:32:01, 16.15s/it, loss=0.373, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847]Generating train split: 1647 examples [1:01:56,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1619])
Epoch 0:  40%|‚ñà‚ñà‚ñà‚ñà      | 232/573 [1:02:22<1:31:40, 16.13s/it, loss=0.373, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847]Epoch 0:  40%|‚ñà‚ñà‚ñà‚ñà      | 232/573 [1:02:22<1:31:40, 16.13s/it, loss=0.371, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872]embeddings shape: torch.Size([32, 768, 1596])
Epoch 0:  41%|‚ñà‚ñà‚ñà‚ñà      | 233/573 [1:02:31<1:31:13, 16.10s/it, loss=0.371, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872]Epoch 0:  41%|‚ñà‚ñà‚ñà‚ñà      | 233/573 [1:02:31<1:31:13, 16.10s/it, loss=0.373, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.819]embeddings shape: torch.Size([32, 768, 1635])
Epoch 0:  41%|‚ñà‚ñà‚ñà‚ñà      | 234/573 [1:02:39<1:30:46, 16.07s/it, loss=0.373, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.819]Epoch 0:  41%|‚ñà‚ñà‚ñà‚ñà      | 234/573 [1:02:39<1:30:46, 16.07s/it, loss=0.375, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837]embeddings shape: torch.Size([32, 768, 1616])
Epoch 0:  41%|‚ñà‚ñà‚ñà‚ñà      | 235/573 [1:02:48<1:30:20, 16.04s/it, loss=0.375, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837]Epoch 0:  41%|‚ñà‚ñà‚ñà‚ñà      | 235/573 [1:02:48<1:30:20, 16.04s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.875]embeddings shape: torch.Size([32, 768, 1268])
Epoch 0:  41%|‚ñà‚ñà‚ñà‚ñà      | 236/573 [1:02:55<1:29:51, 16.00s/it, loss=0.372, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.875]Epoch 0:  41%|‚ñà‚ñà‚ñà‚ñà      | 236/573 [1:02:55<1:29:51, 16.00s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865] Generating train split: 1350 examples [1:02:03,  2.70s/ examples]embeddings shape: torch.Size([32, 768, 1619])
Epoch 0:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 237/573 [1:03:23<1:29:53, 16.05s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865]Epoch 0:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 237/573 [1:03:23<1:29:53, 16.05s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.809]embeddings shape: torch.Size([32, 768, 1586])
Epoch 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 238/573 [1:03:48<1:29:48, 16.09s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.809]Epoch 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 238/573 [1:03:48<1:29:48, 16.09s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.851] Generating train split: 1876 examples [1:03:47,  1.99s/ examples]embeddings shape: torch.Size([32, 768, 1604])
Epoch 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 239/573 [1:04:00<1:29:26, 16.07s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.851]Epoch 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 239/573 [1:04:00<1:29:26, 16.07s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865]embeddings shape: torch.Size([32, 768, 1439])
Epoch 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 240/573 [1:04:24<1:29:21, 16.10s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865]Epoch 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 240/573 [1:04:24<1:29:21, 16.10s/it, loss=0.368, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]Generating train split: 1708 examples [1:04:12,  2.26s/ examples]embeddings shape: torch.Size([32, 768, 1694])
Epoch 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 241/573 [1:04:39<1:29:03, 16.10s/it, loss=0.368, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]Epoch 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 241/573 [1:04:39<1:29:03, 16.10s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.861]embeddings shape: torch.Size([32, 768, 1611])
Epoch 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 242/573 [1:04:45<1:28:35, 16.06s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.861]Epoch 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 242/573 [1:04:45<1:28:35, 16.06s/it, loss=0.369, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.826]embeddings shape: torch.Size([32, 768, 1585])
Epoch 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 243/573 [1:04:53<1:28:07, 16.02s/it, loss=0.369, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.826]Epoch 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 243/573 [1:04:53<1:28:07, 16.02s/it, loss=0.368, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858]embeddings shape: torch.Size([32, 768, 1685])
Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 244/573 [1:05:00<1:27:39, 15.99s/it, loss=0.368, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858]Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 244/573 [1:05:00<1:27:39, 15.99s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872]embeddings shape: torch.Size([32, 768, 1340])
Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 245/573 [1:05:06<1:27:09, 15.94s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872]Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 245/573 [1:05:06<1:27:09, 15.94s/it, loss=0.368, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.806]embeddings shape: torch.Size([32, 768, 1553])
Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 246/573 [1:05:23<1:26:55, 15.95s/it, loss=0.368, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.806]Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 246/573 [1:05:23<1:26:55, 15.95s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.823] Generating train split: 1400 examples [1:04:16,  2.69s/ examples]embeddings shape: torch.Size([32, 768, 1507])
Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 247/573 [1:05:41<1:26:42, 15.96s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.823]Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 247/573 [1:05:41<1:26:42, 15.96s/it, loss=0.368, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854]embeddings shape: torch.Size([32, 768, 1566])
Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 248/573 [1:06:01<1:26:31, 15.97s/it, loss=0.368, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854]Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 248/573 [1:06:01<1:26:31, 15.97s/it, loss=0.367, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844]Generating train split: 1943 examples [1:06:12,  2.04s/ examples]embeddings shape: torch.Size([32, 768, 1647])
Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 249/573 [1:06:22<1:26:21, 15.99s/it, loss=0.367, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844]Epoch 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 249/573 [1:06:22<1:26:21, 15.99s/it, loss=0.364, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854]embeddings shape: torch.Size([32, 768, 1591])
Epoch 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 250/573 [1:06:48<1:26:19, 16.03s/it, loss=0.364, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854]Epoch 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 250/573 [1:06:48<1:26:19, 16.03s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854]Generating train split: 1769 examples [1:06:30,  2.26s/ examples]embeddings shape: torch.Size([32, 768, 1590])
Epoch 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 251/573 [1:06:58<1:25:54, 16.01s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854]Epoch 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 251/573 [1:06:58<1:25:54, 16.01s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.847]embeddings shape: torch.Size([32, 768, 1495])
Epoch 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 252/573 [1:07:05<1:25:27, 15.97s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.847]Epoch 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 252/573 [1:07:05<1:25:27, 15.97s/it, loss=0.367, v_num=hitj, train categorical_accuracy_step=0.156, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.844]embeddings shape: torch.Size([32, 768, 1618])
Epoch 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 253/573 [1:07:14<1:25:03, 15.95s/it, loss=0.367, v_num=hitj, train categorical_accuracy_step=0.156, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.844]Epoch 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 253/573 [1:07:14<1:25:03, 15.95s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837] embeddings shape: torch.Size([32, 768, 1557])
Epoch 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 254/573 [1:07:21<1:24:35, 15.91s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837]Epoch 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 254/573 [1:07:21<1:24:35, 15.91s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840]embeddings shape: torch.Size([32, 768, 1558])
Epoch 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 255/573 [1:07:33<1:24:15, 15.90s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840]Epoch 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 255/573 [1:07:33<1:24:15, 15.90s/it, loss=0.367, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.826]Generating train split: 1450 examples [1:06:32,  2.70s/ examples]embeddings shape: torch.Size([32, 768, 1645])
Epoch 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 256/573 [1:07:59<1:24:11, 15.94s/it, loss=0.367, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.826]Epoch 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 256/573 [1:07:59<1:24:11, 15.94s/it, loss=0.369, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840] embeddings shape: torch.Size([32, 768, 1660])
Epoch 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 257/573 [1:08:07<1:23:45, 15.90s/it, loss=0.369, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]Epoch 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 257/573 [1:08:07<1:23:45, 15.90s/it, loss=0.368, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833]embeddings shape: torch.Size([32, 768, 1579])
Epoch 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 258/573 [1:08:16<1:23:21, 15.88s/it, loss=0.368, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833]Epoch 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 258/573 [1:08:16<1:23:21, 15.88s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.851]embeddings shape: torch.Size([32, 768, 1561])
Epoch 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 259/573 [1:08:42<1:23:17, 15.92s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.851]Epoch 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 259/573 [1:08:42<1:23:17, 15.92s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865]Generating train split: 1830 examples [1:08:36,  2.20s/ examples]Generating train split: 2010 examples [1:09:06,  2.21s/ examples]embeddings shape: torch.Size([32, 768, 1588])
Epoch 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 260/573 [1:09:13<1:23:20, 15.98s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865]Epoch 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 260/573 [1:09:13<1:23:20, 15.98s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861]embeddings shape: torch.Size([32, 768, 1570])
Epoch 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 261/573 [1:09:21<1:22:55, 15.95s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861]Epoch 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 261/573 [1:09:21<1:22:55, 15.95s/it, loss=0.369, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.816]embeddings shape: torch.Size([32, 768, 1587])
Epoch 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 262/573 [1:09:31<1:22:31, 15.92s/it, loss=0.369, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.816]Epoch 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 262/573 [1:09:31<1:22:31, 15.92s/it, loss=0.368, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.837]embeddings shape: torch.Size([32, 768, 1573])
Epoch 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 263/573 [1:09:43<1:22:11, 15.91s/it, loss=0.368, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.837]Epoch 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 263/573 [1:09:43<1:22:11, 15.91s/it, loss=0.368, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.823] Generating train split: 1500 examples [1:08:45,  2.69s/ examples]embeddings shape: torch.Size([32, 768, 1672])
Epoch 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 264/573 [1:10:10<1:22:07, 15.95s/it, loss=0.368, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.823]Epoch 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 264/573 [1:10:10<1:22:07, 15.95s/it, loss=0.371, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837]embeddings shape: torch.Size([32, 768, 1581])
Epoch 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 265/573 [1:10:17<1:21:42, 15.92s/it, loss=0.371, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837]Epoch 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 265/573 [1:10:17<1:21:42, 15.92s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.830]embeddings shape: torch.Size([32, 768, 1619])
Epoch 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 266/573 [1:10:24<1:21:15, 15.88s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.830]Epoch 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 266/573 [1:10:24<1:21:15, 15.88s/it, loss=0.367, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865]embeddings shape: torch.Size([32, 768, 1455])
Epoch 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 267/573 [1:10:33<1:20:51, 15.85s/it, loss=0.367, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865]Epoch 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 267/573 [1:10:33<1:20:51, 15.85s/it, loss=0.365, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865]embeddings shape: torch.Size([32, 768, 1572])
Epoch 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 268/573 [1:11:03<1:20:52, 15.91s/it, loss=0.365, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865]Epoch 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 268/573 [1:11:03<1:20:52, 15.91s/it, loss=0.364, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875]Generating train split: 1891 examples [1:10:52,  2.21s/ examples]Generating train split: 2077 examples [1:11:26,  2.17s/ examples]embeddings shape: torch.Size([32, 768, 1582])
Epoch 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 269/573 [1:11:33<1:20:51, 15.96s/it, loss=0.364, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875]Epoch 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 269/573 [1:11:33<1:20:51, 15.96s/it, loss=0.363, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.868]embeddings shape: torch.Size([32, 768, 1561])
Epoch 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 270/573 [1:11:43<1:20:29, 15.94s/it, loss=0.363, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.868]Epoch 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 270/573 [1:11:43<1:20:29, 15.94s/it, loss=0.363, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.819]embeddings shape: torch.Size([32, 768, 1585])
Epoch 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 271/573 [1:11:49<1:20:03, 15.90s/it, loss=0.363, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.819]Epoch 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 271/573 [1:11:49<1:20:03, 15.90s/it, loss=0.362, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865]embeddings shape: torch.Size([32, 768, 1372])
Epoch 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 272/573 [1:12:00<1:19:41, 15.88s/it, loss=0.362, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865]Epoch 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 272/573 [1:12:00<1:19:41, 15.88s/it, loss=0.364, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823]Generating train split: 1550 examples [1:11:02,  2.70s/ examples]embeddings shape: torch.Size([32, 768, 1601])
Epoch 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 273/573 [1:12:26<1:19:36, 15.92s/it, loss=0.364, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823]Epoch 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 273/573 [1:12:26<1:19:36, 15.92s/it, loss=0.361, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868]embeddings shape: torch.Size([32, 768, 1618])
Epoch 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 274/573 [1:12:36<1:19:13, 15.90s/it, loss=0.361, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868]Epoch 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 274/573 [1:12:36<1:19:13, 15.90s/it, loss=0.36, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868] embeddings shape: torch.Size([32, 768, 1556])
Epoch 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 275/573 [1:12:44<1:18:49, 15.87s/it, loss=0.36, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868]Epoch 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 275/573 [1:12:44<1:18:49, 15.87s/it, loss=0.36, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858]embeddings shape: torch.Size([32, 768, 1531])
Epoch 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 276/573 [1:12:58<1:18:31, 15.86s/it, loss=0.36, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858]Epoch 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 276/573 [1:12:58<1:18:31, 15.86s/it, loss=0.358, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844]Generating train split: 1952 examples [1:12:59,  2.17s/ examples]embeddings shape: torch.Size([32, 768, 1614])
Epoch 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 277/573 [1:13:24<1:18:26, 15.90s/it, loss=0.358, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844]Epoch 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 277/573 [1:13:24<1:18:26, 15.90s/it, loss=0.358, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833]embeddings shape: torch.Size([32, 768, 1621])
Epoch 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 278/573 [1:13:47<1:18:17, 15.93s/it, loss=0.358, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833]Epoch 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 278/573 [1:13:47<1:18:17, 15.93s/it, loss=0.362, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.809]Generating train split: 2144 examples [1:13:53,  2.18s/ examples]embeddings shape: torch.Size([32, 768, 1578])
Epoch 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 279/573 [1:14:05<1:18:04, 15.93s/it, loss=0.362, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.809]Epoch 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 279/573 [1:14:05<1:18:04, 15.93s/it, loss=0.363, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865]embeddings shape: torch.Size([32, 768, 1584])
Epoch 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 280/573 [1:14:29<1:17:56, 15.96s/it, loss=0.363, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865]Epoch 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 280/573 [1:14:29<1:17:56, 15.96s/it, loss=0.363, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833]Generating train split: 1600 examples [1:13:15,  2.69s/ examples]embeddings shape: torch.Size([32, 768, 1593])
Epoch 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 281/573 [1:14:43<1:17:38, 15.95s/it, loss=0.363, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833]Epoch 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 281/573 [1:14:43<1:17:38, 15.95s/it, loss=0.362, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.830]embeddings shape: torch.Size([32, 768, 1645])
Epoch 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 282/573 [1:14:53<1:17:16, 15.93s/it, loss=0.362, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.830]Epoch 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 282/573 [1:14:53<1:17:16, 15.93s/it, loss=0.361, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858] embeddings shape: torch.Size([32, 768, 1224])
Epoch 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 283/573 [1:15:01<1:16:52, 15.91s/it, loss=0.361, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858]Epoch 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 283/573 [1:15:01<1:16:52, 15.91s/it, loss=0.361, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.809]embeddings shape: torch.Size([32, 768, 1577])
Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 284/573 [1:15:22<1:16:41, 15.92s/it, loss=0.361, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.809]Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 284/573 [1:15:22<1:16:41, 15.92s/it, loss=0.362, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.823]Generating train split: 2013 examples [1:15:13,  2.18s/ examples]embeddings shape: torch.Size([32, 768, 1600])
Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 285/573 [1:15:38<1:16:26, 15.93s/it, loss=0.362, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.823]Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 285/573 [1:15:38<1:16:26, 15.93s/it, loss=0.363, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.816]embeddings shape: torch.Size([32, 768, 1579])
Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 286/573 [1:15:50<1:16:06, 15.91s/it, loss=0.363, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.816]Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 286/573 [1:15:50<1:16:06, 15.91s/it, loss=0.363, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851]Generating train split: 2211 examples [1:16:16,  2.16s/ examples]embeddings shape: torch.Size([32, 768, 1599])
Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 287/573 [1:16:21<1:16:06, 15.97s/it, loss=0.363, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851]Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 287/573 [1:16:21<1:16:06, 15.97s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844]embeddings shape: torch.Size([32, 768, 1530])
Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 288/573 [1:16:50<1:16:02, 16.01s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844]Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 288/573 [1:16:50<1:16:02, 16.01s/it, loss=0.367, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.826]Generating train split: 1650 examples [1:15:32,  2.71s/ examples]embeddings shape: torch.Size([32, 768, 1687])
Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 289/573 [1:17:01<1:15:41, 15.99s/it, loss=0.367, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.826]Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 289/573 [1:17:01<1:15:41, 15.99s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851]  embeddings shape: torch.Size([32, 768, 1624])
Epoch 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 290/573 [1:17:10<1:15:19, 15.97s/it, loss=0.37, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851]Epoch 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 290/573 [1:17:10<1:15:19, 15.97s/it, loss=0.368, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.858]embeddings shape: torch.Size([32, 768, 1561])
Epoch 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 291/573 [1:17:21<1:14:57, 15.95s/it, loss=0.368, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.858]Epoch 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 291/573 [1:17:21<1:14:57, 15.95s/it, loss=0.369, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.833]Generating train split: 2074 examples [1:17:33,  2.21s/ examples]embeddings shape: torch.Size([32, 768, 1578])
Epoch 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 292/573 [1:17:54<1:14:58, 16.01s/it, loss=0.369, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.833]Epoch 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 292/573 [1:17:54<1:14:58, 16.01s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865]embeddings shape: torch.Size([32, 768, 1432])
Epoch 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 293/573 [1:18:03<1:14:35, 15.98s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865]Epoch 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 293/573 [1:18:03<1:14:35, 15.98s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.885]embeddings shape: torch.Size([32, 768, 1617])
Epoch 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 294/573 [1:18:13<1:14:13, 15.96s/it, loss=0.366, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.885]Epoch 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 294/573 [1:18:13<1:14:13, 15.96s/it, loss=0.367, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.819]embeddings shape: torch.Size([32, 768, 1630])
Epoch 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 295/573 [1:18:38<1:14:06, 15.99s/it, loss=0.367, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.819]Epoch 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 295/573 [1:18:38<1:14:06, 15.99s/it, loss=0.365, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.885]Generating train split: 2278 examples [1:18:51,  2.21s/ examples]embeddings shape: torch.Size([32, 768, 1565])
Epoch 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 296/573 [1:19:11<1:14:06, 16.05s/it, loss=0.365, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.885]Epoch 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 296/573 [1:19:11<1:14:06, 16.05s/it, loss=0.363, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.861]Generating train split: 1700 examples [1:17:57,  2.76s/ examples]embeddings shape: torch.Size([32, 768, 1451])
Epoch 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 297/573 [1:19:24<1:13:47, 16.04s/it, loss=0.363, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.861]Epoch 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 297/573 [1:19:24<1:13:47, 16.04s/it, loss=0.361, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854]embeddings shape: torch.Size([32, 768, 1649])
Epoch 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 298/573 [1:19:34<1:13:25, 16.02s/it, loss=0.361, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854]Epoch 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 298/573 [1:19:34<1:13:25, 16.02s/it, loss=0.358, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.847]embeddings shape: torch.Size([32, 768, 1555])
Epoch 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 299/573 [1:19:44<1:13:04, 16.00s/it, loss=0.358, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.847]Epoch 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 299/573 [1:19:44<1:13:04, 16.00s/it, loss=0.357, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872] Generating train split: 2135 examples [1:19:50,  2.22s/ examples]embeddings shape: torch.Size([32, 768, 1634])
Epoch 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 300/573 [1:20:12<1:12:59, 16.04s/it, loss=0.357, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872]Epoch 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 300/573 [1:20:12<1:12:59, 16.04s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868]embeddings shape: torch.Size([32, 768, 1597])
Epoch 0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 301/573 [1:20:20<1:12:36, 16.01s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868]Epoch 0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 301/573 [1:20:20<1:12:36, 16.01s/it, loss=0.356, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.812]embeddings shape: torch.Size([32, 768, 1678])
Epoch 0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 302/573 [1:20:29<1:12:14, 15.99s/it, loss=0.356, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.812]Epoch 0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 302/573 [1:20:29<1:12:14, 15.99s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865]embeddings shape: torch.Size([32, 768, 1581])
Epoch 0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 303/573 [1:20:38<1:11:51, 15.97s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865]Epoch 0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 303/573 [1:20:38<1:11:51, 15.97s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.826]embeddings shape: torch.Size([32, 768, 1635])
Epoch 0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 304/573 [1:21:08<1:11:47, 16.01s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.826]Epoch 0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 304/573 [1:21:08<1:11:47, 16.01s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865]Generating train split: 1750 examples [1:19:49,  2.61s/ examples]embeddings shape: torch.Size([32, 768, 1660])
Epoch 0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 305/573 [1:21:39<1:11:44, 16.06s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865]Epoch 0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 305/573 [1:21:39<1:11:44, 16.06s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878]Generating train split: 2345 examples [1:21:37,  2.29s/ examples]embeddings shape: torch.Size([32, 768, 1547])
Epoch 0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 306/573 [1:21:51<1:11:25, 16.05s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878]Epoch 0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 306/573 [1:21:51<1:11:25, 16.05s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833]embeddings shape: torch.Size([32, 768, 1618])
Epoch 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 307/573 [1:22:02<1:11:04, 16.03s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833]Epoch 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 307/573 [1:22:02<1:11:04, 16.03s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854]Generating train split: 2196 examples [1:22:09,  2.24s/ examples]embeddings shape: torch.Size([32, 768, 1566])
Epoch 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 308/573 [1:22:31<1:11:00, 16.08s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854]Epoch 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 308/573 [1:22:31<1:11:00, 16.08s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.875]embeddings shape: torch.Size([32, 768, 1566])
Epoch 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 309/573 [1:22:40<1:10:38, 16.05s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.875]Epoch 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 309/573 [1:22:40<1:10:38, 16.05s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.858]embeddings shape: torch.Size([32, 768, 1561])
Epoch 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 310/573 [1:22:48<1:10:15, 16.03s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.858]Epoch 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 310/573 [1:22:48<1:10:15, 16.03s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844]embeddings shape: torch.Size([32, 768, 1653])
Epoch 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 311/573 [1:22:55<1:09:51, 16.00s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844]Epoch 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 311/573 [1:22:55<1:09:51, 16.00s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868]embeddings shape: torch.Size([32, 768, 1297])
Epoch 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 312/573 [1:23:08<1:09:32, 15.99s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868]Epoch 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 312/573 [1:23:08<1:09:32, 15.99s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.833]Generating train split: 1800 examples [1:22:09,  2.66s/ examples]embeddings shape: torch.Size([32, 768, 1590])
Epoch 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 313/573 [1:23:41<1:09:31, 16.04s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.833]Epoch 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 313/573 [1:23:41<1:09:31, 16.04s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868] Generating train split: 2412 examples [1:23:53,  2.21s/ examples]embeddings shape: torch.Size([32, 768, 1556])
Epoch 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 314/573 [1:24:03<1:09:20, 16.06s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868]Epoch 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 314/573 [1:24:03<1:09:20, 16.06s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.830]embeddings shape: torch.Size([32, 768, 1230])
Epoch 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 315/573 [1:24:11<1:08:57, 16.04s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.830]Epoch 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 315/573 [1:24:11<1:08:57, 16.04s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844]embeddings shape: torch.Size([32, 768, 1601])
Epoch 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 316/573 [1:24:30<1:08:43, 16.05s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844]Epoch 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 316/573 [1:24:30<1:08:43, 16.05s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.830]Generating train split: 2257 examples [1:24:26,  2.24s/ examples]embeddings shape: torch.Size([32, 768, 1351])
Epoch 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 317/573 [1:24:49<1:08:30, 16.05s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.830]Epoch 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 317/573 [1:24:49<1:08:30, 16.05s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.861] embeddings shape: torch.Size([32, 768, 1577])
Epoch 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 318/573 [1:24:58<1:08:08, 16.03s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.861]Epoch 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 318/573 [1:24:58<1:08:08, 16.03s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851]embeddings shape: torch.Size([32, 768, 1583])
Epoch 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 319/573 [1:25:06<1:07:46, 16.01s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851]Epoch 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 319/573 [1:25:06<1:07:46, 16.01s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847]embeddings shape: torch.Size([32, 768, 1551])
Epoch 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 320/573 [1:25:13<1:07:23, 15.98s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847]Epoch 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 320/573 [1:25:13<1:07:23, 15.98s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.865]embeddings shape: torch.Size([32, 768, 1593])
Epoch 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 321/573 [1:25:33<1:07:09, 15.99s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.865]Epoch 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 321/573 [1:25:33<1:07:09, 15.99s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.896]Generating train split: 1850 examples [1:24:24,  2.68s/ examples]embeddings shape: torch.Size([32, 768, 1580])
Epoch 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 322/573 [1:25:52<1:06:56, 16.00s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.896]Epoch 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 322/573 [1:25:52<1:06:56, 16.00s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858]embeddings shape: torch.Size([32, 768, 1484])
Epoch 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 323/573 [1:25:59<1:06:33, 15.97s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858]Epoch 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 323/573 [1:25:59<1:06:33, 15.97s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.878]embeddings shape: torch.Size([32, 768, 1636])
Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 324/573 [1:26:19<1:06:20, 15.99s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.878]Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 324/573 [1:26:19<1:06:20, 15.99s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.823]embeddings shape: torch.Size([32, 768, 1586])
Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 325/573 [1:26:52<1:06:17, 16.04s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.823]Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 325/573 [1:26:52<1:06:17, 16.04s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.861] Generating train split: 2479 examples [1:26:51,  2.35s/ examples]Generating train split: 2318 examples [1:26:49,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1534])
Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 326/573 [1:27:14<1:06:05, 16.06s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.861]Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 326/573 [1:27:14<1:06:05, 16.06s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.844]embeddings shape: torch.Size([32, 768, 1555])
Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 327/573 [1:27:20<1:05:42, 16.03s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.844]Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 327/573 [1:27:20<1:05:42, 16.03s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854] embeddings shape: torch.Size([32, 768, 1621])
Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 328/573 [1:27:30<1:05:21, 16.01s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854]Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 328/573 [1:27:30<1:05:21, 16.01s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840]embeddings shape: torch.Size([32, 768, 1437])
Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 329/573 [1:27:42<1:05:02, 16.00s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840]Epoch 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 329/573 [1:27:42<1:05:02, 16.00s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854]Generating train split: 1900 examples [1:26:43,  2.71s/ examples]embeddings shape: torch.Size([32, 768, 1624])
Epoch 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 330/573 [1:28:07<1:04:53, 16.02s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854]Epoch 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 330/573 [1:28:07<1:04:53, 16.02s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840]embeddings shape: torch.Size([32, 768, 1670])
Epoch 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 331/573 [1:28:15<1:04:31, 16.00s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840]Epoch 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 331/573 [1:28:15<1:04:31, 16.00s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.806] embeddings shape: torch.Size([32, 768, 1583])
Epoch 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 332/573 [1:28:25<1:04:11, 15.98s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.806]Epoch 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 332/573 [1:28:25<1:04:11, 15.98s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847]embeddings shape: torch.Size([32, 768, 1670])
Epoch 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 333/573 [1:28:33<1:03:49, 15.96s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847]Epoch 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 333/573 [1:28:33<1:03:49, 15.96s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.872]embeddings shape: torch.Size([32, 768, 1624])
Epoch 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 334/573 [1:29:03<1:03:43, 16.00s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.872]Epoch 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 334/573 [1:29:03<1:03:43, 16.00s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847]Generating train split: 2379 examples [1:28:44,  2.16s/ examples]embeddings shape: torch.Size([32, 768, 1589])
Epoch 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 335/573 [1:29:12<1:03:22, 15.98s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847]Epoch 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 335/573 [1:29:12<1:03:22, 15.98s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868]embeddings shape: torch.Size([32, 768, 1605])
Epoch 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 336/573 [1:29:35<1:03:11, 16.00s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868]Epoch 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 336/573 [1:29:35<1:03:11, 16.00s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868]Generating train split: 2546 examples [1:29:51,  2.45s/ examples]embeddings shape: torch.Size([32, 768, 1477])
Epoch 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 337/573 [1:29:59<1:03:01, 16.02s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868]Epoch 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 337/573 [1:29:59<1:03:01, 16.02s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858]embeddings shape: torch.Size([32, 768, 1623])
Epoch 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 338/573 [1:30:26<1:02:52, 16.06s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858]Epoch 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 338/573 [1:30:26<1:02:52, 16.06s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.875] Generating train split: 1950 examples [1:29:07,  2.76s/ examples]embeddings shape: torch.Size([32, 768, 1553])
Epoch 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 339/573 [1:30:36<1:02:32, 16.04s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.875]Epoch 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 339/573 [1:30:36<1:02:32, 16.04s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854]embeddings shape: torch.Size([32, 768, 1625])
Epoch 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 340/573 [1:30:46<1:02:12, 16.02s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854]Epoch 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 340/573 [1:30:46<1:02:12, 16.02s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844]embeddings shape: torch.Size([32, 768, 1613])
Epoch 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 341/573 [1:30:56<1:01:52, 16.00s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844]Epoch 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 341/573 [1:30:56<1:01:52, 16.00s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854]embeddings shape: torch.Size([32, 768, 1572])
Epoch 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 342/573 [1:31:20<1:01:41, 16.02s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854]Epoch 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 342/573 [1:31:20<1:01:41, 16.02s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.830]Generating train split: 2440 examples [1:31:10,  2.23s/ examples]embeddings shape: torch.Size([32, 768, 1668])
Epoch 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 343/573 [1:31:36<1:01:25, 16.03s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.830]Epoch 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 343/573 [1:31:36<1:01:25, 16.03s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.840] embeddings shape: torch.Size([32, 768, 1540])
Epoch 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 344/573 [1:31:43<1:01:03, 16.00s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.840]Epoch 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 344/573 [1:31:43<1:01:03, 16.00s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854]embeddings shape: torch.Size([32, 768, 1528])
Epoch 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 345/573 [1:31:50<1:00:41, 15.97s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854]Epoch 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 345/573 [1:31:50<1:00:41, 15.97s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.861]embeddings shape: torch.Size([32, 768, 1622])
Epoch 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 346/573 [1:32:13<1:00:30, 15.99s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.861]Epoch 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 346/573 [1:32:13<1:00:30, 15.99s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847]Generating train split: 2000 examples [1:31:03,  2.63s/ examples]embeddings shape: torch.Size([32, 768, 1609])
Epoch 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 347/573 [1:32:30<1:00:15, 16.00s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847]Epoch 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 347/573 [1:32:30<1:00:15, 16.00s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.889]embeddings shape: torch.Size([32, 768, 1596])
Epoch 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 348/573 [1:32:47<59:59, 16.00s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.889]  Epoch 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 348/573 [1:32:47<59:59, 16.00s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840]Generating train split: 2613 examples [1:33:12,  2.61s/ examples]embeddings shape: torch.Size([32, 768, 1608])
Epoch 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 349/573 [1:33:18<59:53, 16.04s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840]Epoch 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 349/573 [1:33:18<59:53, 16.04s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823]embeddings shape: torch.Size([32, 768, 1490])
Epoch 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 350/573 [1:33:27<59:32, 16.02s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823]Epoch 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 350/573 [1:33:27<59:32, 16.02s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837]embeddings shape: torch.Size([32, 768, 1653])
Epoch 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 351/573 [1:33:54<59:23, 16.05s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837]Epoch 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 351/573 [1:33:54<59:23, 16.05s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.788]Generating train split: 2501 examples [1:33:37,  2.28s/ examples]embeddings shape: torch.Size([32, 768, 1621])
Epoch 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 352/573 [1:34:04<59:03, 16.03s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.788]Epoch 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 352/573 [1:34:04<59:03, 16.03s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861] embeddings shape: torch.Size([32, 768, 1598])
Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 353/573 [1:34:12<58:42, 16.01s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861]Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 353/573 [1:34:12<58:42, 16.01s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868]embeddings shape: torch.Size([32, 768, 1579])
Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 354/573 [1:34:21<58:22, 15.99s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868]Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 354/573 [1:34:21<58:22, 15.99s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858]Generating train split: 2050 examples [1:33:27,  2.70s/ examples]embeddings shape: torch.Size([32, 768, 1607])
Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 355/573 [1:34:49<58:14, 16.03s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858]Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 355/573 [1:34:49<58:14, 16.03s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847]embeddings shape: torch.Size([32, 768, 1619])
Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 356/573 [1:34:59<57:54, 16.01s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847]Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 356/573 [1:34:59<57:54, 16.01s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.847]embeddings shape: torch.Size([32, 768, 1553])
Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 357/573 [1:35:07<57:33, 15.99s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.847]Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 357/573 [1:35:07<57:33, 15.99s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847] embeddings shape: torch.Size([32, 768, 1655])
Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 358/573 [1:35:15<57:12, 15.97s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847]Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 358/573 [1:35:15<57:12, 15.97s/it, loss=0.357, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837]embeddings shape: torch.Size([32, 768, 1622])
Epoch 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 359/573 [1:35:26<56:53, 15.95s/it, loss=0.357, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837]Epoch 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 359/573 [1:35:26<56:53, 15.95s/it, loss=0.357, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847]Generating train split: 2562 examples [1:35:30,  2.15s/ examples]embeddings shape: torch.Size([32, 768, 1581])
Epoch 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 360/573 [1:35:53<56:44, 15.98s/it, loss=0.357, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847]Epoch 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 360/573 [1:35:53<56:44, 15.98s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.875]embeddings shape: torch.Size([32, 768, 1598])
Epoch 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 361/573 [1:36:20<56:34, 16.01s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.875]Epoch 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 361/573 [1:36:20<56:34, 16.01s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851]Generating train split: 2680 examples [1:36:32,  2.73s/ examples]embeddings shape: torch.Size([32, 768, 1449])
Epoch 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 362/573 [1:36:42<56:22, 16.03s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851]Epoch 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 362/573 [1:36:42<56:22, 16.03s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851]embeddings shape: torch.Size([32, 768, 1550])
Epoch 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 363/573 [1:37:07<56:11, 16.05s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851]Epoch 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 363/573 [1:37:07<56:11, 16.05s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844] Generating train split: 2100 examples [1:35:53,  2.77s/ examples]embeddings shape: torch.Size([32, 768, 1627])
Epoch 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 364/573 [1:37:21<55:53, 16.05s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844]Epoch 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 364/573 [1:37:21<55:53, 16.05s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861]embeddings shape: torch.Size([32, 768, 1400])
Epoch 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 365/573 [1:37:28<55:33, 16.02s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861]Epoch 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 365/573 [1:37:28<55:33, 16.02s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875]embeddings shape: torch.Size([32, 768, 1572])
Epoch 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 366/573 [1:37:34<55:11, 16.00s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875]Epoch 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 366/573 [1:37:34<55:11, 16.00s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.812]embeddings shape: torch.Size([32, 768, 1628])
Epoch 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 367/573 [1:37:43<54:51, 15.98s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.812]Epoch 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 367/573 [1:37:43<54:51, 15.98s/it, loss=0.359, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837] embeddings shape: torch.Size([32, 768, 1671])
Epoch 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 368/573 [1:38:04<54:38, 15.99s/it, loss=0.359, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837]Epoch 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 368/573 [1:38:04<54:38, 15.99s/it, loss=0.358, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.840]Generating train split: 2623 examples [1:38:00,  2.24s/ examples]embeddings shape: torch.Size([32, 768, 1602])
Epoch 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 369/573 [1:38:24<54:24, 16.00s/it, loss=0.358, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.840]Epoch 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 369/573 [1:38:24<54:24, 16.00s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882]embeddings shape: torch.Size([32, 768, 1613])
Epoch 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 370/573 [1:38:32<54:03, 15.98s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882]Epoch 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 370/573 [1:38:32<54:03, 15.98s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.823]embeddings shape: torch.Size([32, 768, 1642])
Epoch 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 371/573 [1:38:44<53:45, 15.97s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.823]Epoch 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 371/573 [1:38:44<53:45, 15.97s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858] Generating train split: 2150 examples [1:37:48,  2.63s/ examples]embeddings shape: torch.Size([32, 768, 1683])
Epoch 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 372/573 [1:39:11<53:35, 16.00s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858]Epoch 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 372/573 [1:39:11<53:35, 16.00s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865]embeddings shape: torch.Size([32, 768, 1592])
Epoch 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 373/573 [1:39:40<53:26, 16.03s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865]Epoch 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 373/573 [1:39:40<53:26, 16.03s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875]Generating train split: 2747 examples [1:39:46,  2.78s/ examples]embeddings shape: torch.Size([32, 768, 1683])
Epoch 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 374/573 [1:39:57<53:11, 16.04s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875]Epoch 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 374/573 [1:39:57<53:11, 16.04s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865]embeddings shape: torch.Size([32, 768, 1608])
Epoch 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 375/573 [1:40:05<52:50, 16.01s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865]Epoch 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 375/573 [1:40:05<52:50, 16.01s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847]embeddings shape: torch.Size([32, 768, 1631])
Epoch 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 376/573 [1:40:13<52:30, 15.99s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847]Epoch 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 376/573 [1:40:13<52:30, 15.99s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.851]Generating train split: 2684 examples [1:40:21,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1625])
Epoch 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 377/573 [1:40:43<52:22, 16.03s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.851]Epoch 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 377/573 [1:40:43<52:22, 16.03s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]embeddings shape: torch.Size([32, 768, 1585])
Epoch 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 378/573 [1:40:52<52:02, 16.01s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]Epoch 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 378/573 [1:40:52<52:02, 16.01s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.816]embeddings shape: torch.Size([32, 768, 1584])
Epoch 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 379/573 [1:41:00<51:42, 15.99s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.816]Epoch 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 379/573 [1:41:00<51:42, 15.99s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861] Generating train split: 2200 examples [1:40:08,  2.68s/ examples]embeddings shape: torch.Size([32, 768, 1607])
Epoch 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 380/573 [1:41:29<51:32, 16.02s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861]Epoch 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 380/573 [1:41:29<51:32, 16.02s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861]embeddings shape: torch.Size([32, 768, 1509])
Epoch 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 381/573 [1:41:36<51:12, 16.00s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861]Epoch 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 381/573 [1:41:36<51:12, 16.00s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861]embeddings shape: torch.Size([32, 768, 1542])
Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 382/573 [1:41:45<50:52, 15.98s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861]Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 382/573 [1:41:45<50:52, 15.98s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.840]embeddings shape: torch.Size([32, 768, 1584])
Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 383/573 [1:41:54<50:33, 15.97s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.840]Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 383/573 [1:41:54<50:33, 15.97s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865]embeddings shape: torch.Size([32, 768, 1606])
Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 384/573 [1:42:03<50:13, 15.95s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865]Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 384/573 [1:42:03<50:13, 15.95s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858]embeddings shape: torch.Size([32, 768, 1262])
Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 385/573 [1:42:31<50:03, 15.98s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858]Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 385/573 [1:42:31<50:03, 15.98s/it, loss=0.356, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861]Generating train split: 2745 examples [1:42:14,  2.14s/ examples]embeddings shape: torch.Size([32, 768, 1620])
Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 386/573 [1:42:57<49:52, 16.00s/it, loss=0.356, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861]Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 386/573 [1:42:57<49:52, 16.00s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865]Generating train split: 2814 examples [1:43:08,  2.85s/ examples]embeddings shape: torch.Size([32, 768, 1589])
Epoch 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 387/573 [1:43:16<49:38, 16.01s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865]Epoch 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 387/573 [1:43:16<49:38, 16.01s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865] embeddings shape: torch.Size([32, 768, 1631])
Epoch 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 388/573 [1:43:30<49:21, 16.01s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865]Epoch 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 388/573 [1:43:30<49:21, 16.01s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854]Generating train split: 2250 examples [1:42:30,  2.72s/ examples]embeddings shape: torch.Size([32, 768, 1603])
Epoch 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 389/573 [1:43:54<49:09, 16.03s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854]Epoch 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 389/573 [1:43:54<49:09, 16.03s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847]embeddings shape: torch.Size([32, 768, 1602])
Epoch 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 390/573 [1:44:02<48:49, 16.01s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847]Epoch 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 390/573 [1:44:02<48:49, 16.01s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.875]embeddings shape: torch.Size([32, 768, 1385])
Epoch 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 391/573 [1:44:08<48:28, 15.98s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.875]Epoch 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 391/573 [1:44:08<48:28, 15.98s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.840]embeddings shape: torch.Size([32, 768, 1582])
Epoch 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 392/573 [1:44:18<48:09, 15.96s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.840]Epoch 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 392/573 [1:44:18<48:09, 15.96s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.847]embeddings shape: torch.Size([32, 768, 1630])
Epoch 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 393/573 [1:44:29<47:51, 15.95s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.847]Epoch 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 393/573 [1:44:29<47:51, 15.95s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.837] Generating train split: 2806 examples [1:44:34,  2.19s/ examples]embeddings shape: torch.Size([32, 768, 1594])
Epoch 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 394/573 [1:44:59<47:41, 15.99s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.837]Epoch 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 394/573 [1:44:59<47:41, 15.99s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.826]embeddings shape: torch.Size([32, 768, 1622])
Epoch 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 395/573 [1:45:07<47:22, 15.97s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.826]Epoch 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 395/573 [1:45:07<47:22, 15.97s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844]embeddings shape: torch.Size([32, 768, 1405])
Epoch 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 396/573 [1:45:13<47:02, 15.94s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844]Epoch 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 396/573 [1:45:13<47:02, 15.94s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875]Generating train split: 2300 examples [1:44:20,  2.57s/ examples]embeddings shape: torch.Size([32, 768, 1562])
Epoch 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 397/573 [1:45:42<46:51, 15.98s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875]Epoch 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 397/573 [1:45:42<46:51, 15.98s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854]embeddings shape: torch.Size([32, 768, 1588])
Epoch 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 398/573 [1:45:49<46:32, 15.95s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854]Epoch 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 398/573 [1:45:49<46:32, 15.95s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837]embeddings shape: torch.Size([32, 768, 1634])
Epoch 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 399/573 [1:45:58<46:12, 15.94s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837]Epoch 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 399/573 [1:45:58<46:12, 15.94s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833] embeddings shape: torch.Size([32, 768, 1628])
Epoch 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 400/573 [1:46:11<45:55, 15.93s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833]Epoch 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 400/573 [1:46:11<45:55, 15.93s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844]embeddings shape: torch.Size([32, 768, 1606])
Epoch 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 401/573 [1:46:45<45:47, 15.97s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844]Epoch 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 401/573 [1:46:45<45:47, 15.97s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837]Generating train split: 2881 examples [1:46:47,  2.97s/ examples]embeddings shape: torch.Size([32, 768, 1604])
Epoch 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 402/573 [1:47:14<45:37, 16.01s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837]Epoch 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 402/573 [1:47:14<45:37, 16.01s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833]Generating train split: 2867 examples [1:47:04,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1597])
Epoch 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 403/573 [1:47:29<45:20, 16.00s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833]Epoch 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 403/573 [1:47:29<45:20, 16.00s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.903] embeddings shape: torch.Size([32, 768, 1597])
Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 404/573 [1:47:38<45:01, 15.99s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.903]Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 404/573 [1:47:38<45:01, 15.99s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847]embeddings shape: torch.Size([32, 768, 1457])
Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 405/573 [1:47:44<44:41, 15.96s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847]Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 405/573 [1:47:44<44:41, 15.96s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872] embeddings shape: torch.Size([32, 768, 1459])
Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 406/573 [1:48:08<44:29, 15.98s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872]Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 406/573 [1:48:08<44:29, 15.98s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.830]Generating train split: 2350 examples [1:46:53,  2.72s/ examples]embeddings shape: torch.Size([32, 768, 1573])
Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 407/573 [1:48:20<44:11, 15.97s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.830]Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 407/573 [1:48:20<44:11, 15.97s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854]embeddings shape: torch.Size([32, 768, 1604])
Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 408/573 [1:48:27<43:51, 15.95s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854]Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 408/573 [1:48:27<43:51, 15.95s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.882]embeddings shape: torch.Size([32, 768, 1593])
Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 409/573 [1:48:35<43:32, 15.93s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.882]Epoch 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 409/573 [1:48:35<43:32, 15.93s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.882]embeddings shape: torch.Size([32, 768, 1597])
Epoch 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 410/573 [1:48:44<43:14, 15.91s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.882]Epoch 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 410/573 [1:48:44<43:14, 15.91s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868] embeddings shape: torch.Size([32, 768, 1614])
Epoch 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 411/573 [1:49:05<42:59, 15.93s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868]Epoch 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 411/573 [1:49:05<42:59, 15.93s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865]Generating train split: 2928 examples [1:48:57,  2.14s/ examples]embeddings shape: torch.Size([32, 768, 1632])
Epoch 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 412/573 [1:49:22<42:44, 15.93s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865]Epoch 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 412/573 [1:49:22<42:44, 15.93s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.858]embeddings shape: torch.Size([32, 768, 1645])
Epoch 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 413/573 [1:49:29<42:25, 15.91s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.858]Epoch 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 413/573 [1:49:29<42:25, 15.91s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.861]embeddings shape: torch.Size([32, 768, 1654])
Epoch 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 414/573 [1:49:40<42:07, 15.89s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.861]Epoch 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 414/573 [1:49:40<42:07, 15.89s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872]Generating train split: 2400 examples [1:48:42,  2.56s/ examples]embeddings shape: torch.Size([32, 768, 1575])
Epoch 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 415/573 [1:50:15<41:58, 15.94s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872]Epoch 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 415/573 [1:50:15<41:58, 15.94s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865]embeddings shape: torch.Size([32, 768, 1653])
Epoch 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 416/573 [1:50:51<41:50, 15.99s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865]Epoch 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 416/573 [1:50:51<41:50, 15.99s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833]Generating train split: 2948 examples [1:50:47,  3.15s/ examples]embeddings shape: torch.Size([32, 768, 1549])
Epoch 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 417/573 [1:51:01<41:32, 15.97s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833]Epoch 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 417/573 [1:51:01<41:32, 15.97s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858]embeddings shape: torch.Size([32, 768, 1618])
Epoch 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 418/573 [1:51:10<41:13, 15.96s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858]Epoch 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 418/573 [1:51:10<41:13, 15.96s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.892]embeddings shape: torch.Size([32, 768, 1591])
Epoch 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 419/573 [1:51:16<40:54, 15.94s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.892]Epoch 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 419/573 [1:51:16<40:54, 15.94s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872] embeddings shape: torch.Size([32, 768, 1616])
Epoch 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 420/573 [1:51:44<40:42, 15.96s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872]Epoch 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 420/573 [1:51:44<40:42, 15.96s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885]Generating train split: 2989 examples [1:51:28,  2.24s/ examples]embeddings shape: torch.Size([32, 768, 1406])
Epoch 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 421/573 [1:51:54<40:24, 15.95s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885]Epoch 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 421/573 [1:51:54<40:24, 15.95s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858]embeddings shape: torch.Size([32, 768, 1582])
Epoch 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 422/573 [1:52:02<40:05, 15.93s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858]Epoch 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 422/573 [1:52:02<40:05, 15.93s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875]embeddings shape: torch.Size([32, 768, 1587])
Epoch 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 423/573 [1:52:31<39:54, 15.96s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875]Epoch 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 423/573 [1:52:31<39:54, 15.96s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865]Generating train split: 2450 examples [1:51:14,  2.70s/ examples]embeddings shape: torch.Size([32, 768, 1396])
Epoch 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 424/573 [1:52:41<39:36, 15.95s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865]Epoch 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 424/573 [1:52:41<39:36, 15.95s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844]embeddings shape: torch.Size([32, 768, 1609])
Epoch 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 425/573 [1:52:50<39:17, 15.93s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844]Epoch 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 425/573 [1:52:50<39:17, 15.93s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878]embeddings shape: torch.Size([32, 768, 1643])
Epoch 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 426/573 [1:52:56<38:58, 15.91s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878]Epoch 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 426/573 [1:52:56<38:58, 15.91s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854]embeddings shape: torch.Size([32, 768, 1630])
Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 427/573 [1:53:06<38:40, 15.89s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854]Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 427/573 [1:53:06<38:40, 15.89s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865]embeddings shape: torch.Size([32, 768, 1458])
Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 428/573 [1:53:15<38:22, 15.88s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865]Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 428/573 [1:53:15<38:22, 15.88s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847]Generating train split: 3050 examples [1:53:22,  2.13s/ examples]embeddings shape: torch.Size([32, 768, 1595])
Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 429/573 [1:53:44<38:10, 15.91s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847]Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 429/573 [1:53:44<38:10, 15.91s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865]embeddings shape: torch.Size([32, 768, 1573])
Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 430/573 [1:53:51<37:51, 15.89s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865]Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 430/573 [1:53:51<37:51, 15.89s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.826]embeddings shape: torch.Size([32, 768, 1217])
Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 431/573 [1:54:12<37:37, 15.90s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.826]Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 431/573 [1:54:12<37:37, 15.90s/it, loss=0.341, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833]embeddings shape: torch.Size([32, 768, 1621])
Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 432/573 [1:54:42<37:26, 15.93s/it, loss=0.341, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833]Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 432/573 [1:54:42<37:26, 15.93s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.816]embeddings shape: torch.Size([32, 768, 1588])
Epoch 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 433/573 [1:55:04<37:12, 15.95s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.816]Epoch 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 433/573 [1:55:04<37:12, 15.95s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.844]Generating train split: 2500 examples [1:53:45,  2.79s/ examples]Generating train split: 3015 examples [1:55:00,  3.34s/ examples]embeddings shape: torch.Size([32, 768, 1545])
Epoch 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 434/573 [1:55:15<36:54, 15.93s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.844]Epoch 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 434/573 [1:55:15<36:54, 15.93s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]embeddings shape: torch.Size([32, 768, 1605])
Epoch 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 435/573 [1:55:21<36:35, 15.91s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]Epoch 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 435/573 [1:55:21<36:35, 15.91s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872]embeddings shape: torch.Size([32, 768, 1571])
Epoch 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 436/573 [1:55:30<36:17, 15.89s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872]Epoch 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 436/573 [1:55:30<36:17, 15.89s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840]embeddings shape: torch.Size([32, 768, 1502])
Epoch 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 437/573 [1:55:41<36:00, 15.88s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840]Epoch 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 437/573 [1:55:41<36:00, 15.88s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833] Generating train split: 3111 examples [1:55:46,  2.20s/ examples]embeddings shape: torch.Size([32, 768, 1667])
Epoch 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 438/573 [1:56:09<35:47, 15.91s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833]Epoch 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 438/573 [1:56:09<35:47, 15.91s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875]embeddings shape: torch.Size([32, 768, 1596])
Epoch 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 439/573 [1:56:16<35:29, 15.89s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875]Epoch 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 439/573 [1:56:16<35:29, 15.89s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844]embeddings shape: torch.Size([32, 768, 1607])
Epoch 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 440/573 [1:56:25<35:11, 15.88s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844]Epoch 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 440/573 [1:56:25<35:11, 15.88s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847]embeddings shape: torch.Size([32, 768, 1599])
Epoch 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 441/573 [1:56:34<34:53, 15.86s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847]Epoch 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 441/573 [1:56:34<34:53, 15.86s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.892]Generating train split: 2550 examples [1:55:39,  2.64s/ examples]embeddings shape: torch.Size([32, 768, 1576])
Epoch 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 442/573 [1:57:00<34:40, 15.88s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.892]Epoch 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 442/573 [1:57:00<34:40, 15.88s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851]embeddings shape: torch.Size([32, 768, 1589])
Epoch 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 443/573 [1:57:08<34:22, 15.87s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851]Epoch 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 443/573 [1:57:08<34:22, 15.87s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.865]embeddings shape: torch.Size([32, 768, 1577])
Epoch 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 444/573 [1:57:15<34:04, 15.85s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.865]Epoch 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 444/573 [1:57:15<34:04, 15.85s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854]embeddings shape: torch.Size([32, 768, 1588])
Epoch 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 445/573 [1:57:22<33:45, 15.83s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854]Epoch 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 445/573 [1:57:22<33:45, 15.83s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878]embeddings shape: torch.Size([32, 768, 1617])
Epoch 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 446/573 [1:57:35<33:29, 15.82s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878]Epoch 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 446/573 [1:57:35<33:29, 15.82s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.875]Generating train split: 3172 examples [1:57:37,  2.09s/ examples]embeddings shape: torch.Size([32, 768, 1653])
Epoch 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 447/573 [1:58:01<33:15, 15.84s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.875]Epoch 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 447/573 [1:58:01<33:15, 15.84s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.854]embeddings shape: torch.Size([32, 768, 1576])
Epoch 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 448/573 [1:58:13<32:59, 15.83s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.854]Epoch 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 448/573 [1:58:13<32:59, 15.83s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840]embeddings shape: torch.Size([32, 768, 1567])
Epoch 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 449/573 [1:58:41<32:46, 15.86s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840]Epoch 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 449/573 [1:58:41<32:46, 15.86s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868]Generating train split: 3082 examples [1:58:50,  3.37s/ examples]embeddings shape: torch.Size([32, 768, 1566])
Epoch 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 450/573 [1:59:00<32:31, 15.87s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868]Epoch 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 450/573 [1:59:00<32:31, 15.87s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844]embeddings shape: torch.Size([32, 768, 1598])
Epoch 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 451/573 [1:59:21<32:17, 15.88s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844]Epoch 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 451/573 [1:59:21<32:17, 15.88s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840]Generating train split: 2600 examples [1:58:12,  2.77s/ examples]embeddings shape: torch.Size([32, 768, 1599])
Epoch 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 452/573 [1:59:37<32:01, 15.88s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840]Epoch 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 452/573 [1:59:37<32:01, 15.88s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837]embeddings shape: torch.Size([32, 768, 1626])
Epoch 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 453/573 [1:59:46<31:43, 15.86s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837]Epoch 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 453/573 [1:59:46<31:43, 15.86s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861]embeddings shape: torch.Size([32, 768, 1605])
Epoch 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 454/573 [1:59:56<31:26, 15.85s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861]Epoch 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 454/573 [1:59:56<31:26, 15.85s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]embeddings shape: torch.Size([32, 768, 1600])
Epoch 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 455/573 [2:00:21<31:12, 15.87s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840]Epoch 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 455/573 [2:00:21<31:12, 15.87s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858]Generating train split: 3233 examples [2:00:09,  2.21s/ examples]embeddings shape: torch.Size([32, 768, 1647])
Epoch 0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 456/573 [2:00:36<30:56, 15.87s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858]Epoch 0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 456/573 [2:00:36<30:56, 15.87s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.847]embeddings shape: torch.Size([32, 768, 1674])
Epoch 0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 457/573 [2:00:46<30:39, 15.86s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.847]Epoch 0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 457/573 [2:00:46<30:39, 15.86s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868]embeddings shape: torch.Size([26, 768, 1645])
Epoch 0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 458/573 [2:00:52<30:21, 15.84s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868]Epoch 0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 458/573 [2:00:52<30:21, 15.84s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]
Validation: 0it [00:00, ?it/s][A
Validation:   0%|          | 0/115 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/115 [00:00<?, ?it/s][Aembeddings shape: torch.Size([32, 768, 1569])

Validation DataLoader 0:   1%|          | 1/115 [00:00<00:00, 176.73it/s][AEpoch 0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 459/573 [2:01:08<30:05, 15.83s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 2650 examples [2:00:05,  2.62s/ examples]embeddings shape: torch.Size([32, 768, 1635])

Validation DataLoader 0:   2%|‚ñè         | 2/115 [00:24<23:26, 12.45s/it] [AEpoch 0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 460/573 [2:01:33<29:51, 15.85s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1614])

Validation DataLoader 0:   3%|‚ñé         | 3/115 [00:34<21:25, 11.48s/it][AEpoch 0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 461/573 [2:01:42<29:34, 15.84s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1551])

Validation DataLoader 0:   3%|‚ñé         | 4/115 [00:51<23:40, 12.80s/it][AEpoch 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 462/573 [2:01:59<29:18, 15.84s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1601])

Validation DataLoader 0:   4%|‚ñç         | 5/115 [01:20<29:35, 16.14s/it][AEpoch 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 463/573 [2:02:28<29:05, 15.87s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1631])

Validation DataLoader 0:   5%|‚ñå         | 6/115 [01:48<32:55, 18.13s/it][AEpoch 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 464/573 [2:02:56<28:52, 15.90s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3149 examples [2:02:52,  3.44s/ examples]Generating train split: 3294 examples [2:02:38,  2.28s/ examples]embeddings shape: torch.Size([32, 768, 1677])

Validation DataLoader 0:   6%|‚ñå         | 7/115 [01:57<30:09, 16.75s/it][AEpoch 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 465/573 [2:03:05<28:35, 15.88s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1594])

Validation DataLoader 0:   7%|‚ñã         | 8/115 [02:04<27:46, 15.58s/it][AEpoch 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 466/573 [2:03:12<28:17, 15.86s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1383])

Validation DataLoader 0:   8%|‚ñä         | 9/115 [02:12<25:58, 14.71s/it][AEpoch 0:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 467/573 [2:03:20<27:59, 15.85s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1634])

Validation DataLoader 0:   9%|‚ñä         | 10/115 [02:34<27:05, 15.48s/it][AEpoch 0:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 468/573 [2:03:42<27:45, 15.86s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 2700 examples [2:02:29,  2.69s/ examples]embeddings shape: torch.Size([32, 768, 1573])

Validation DataLoader 0:  10%|‚ñâ         | 11/115 [02:46<26:13, 15.13s/it][AEpoch 0:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 469/573 [2:03:54<27:28, 15.85s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1607])

Validation DataLoader 0:  10%|‚ñà         | 12/115 [02:54<24:56, 14.53s/it][AEpoch 0:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 470/573 [2:04:02<27:11, 15.84s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1446])

Validation DataLoader 0:  11%|‚ñà‚ñè        | 13/115 [03:01<23:41, 13.94s/it][AEpoch 0:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 471/573 [2:04:09<26:53, 15.82s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1323])

Validation DataLoader 0:  12%|‚ñà‚ñè        | 14/115 [03:08<22:40, 13.47s/it][AEpoch 0:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 472/573 [2:04:16<26:35, 15.80s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1219])

Validation DataLoader 0:  13%|‚ñà‚ñé        | 15/115 [03:13<21:29, 12.90s/it][AEpoch 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 473/573 [2:04:21<26:17, 15.78s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1092])

Validation DataLoader 0:  14%|‚ñà‚ñç        | 16/115 [03:24<21:07, 12.80s/it][AEpoch 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 474/573 [2:04:32<26:00, 15.77s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3355 examples [2:04:31,  2.15s/ examples]embeddings shape: torch.Size([32, 768, 1197])

Validation DataLoader 0:  15%|‚ñà‚ñç        | 17/115 [03:44<21:31, 13.18s/it][AEpoch 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 475/573 [2:04:52<25:45, 15.77s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1569])

Validation DataLoader 0:  16%|‚ñà‚ñå        | 18/115 [03:50<20:41, 12.80s/it][AEpoch 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 476/573 [2:04:58<25:28, 15.75s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1234])

Validation DataLoader 0:  17%|‚ñà‚ñã        | 19/115 [03:56<19:56, 12.46s/it][AEpoch 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 477/573 [2:05:04<25:10, 15.73s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1597])

Validation DataLoader 0:  17%|‚ñà‚ñã        | 20/115 [04:18<20:26, 12.91s/it][AEpoch 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 478/573 [2:05:26<24:55, 15.75s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 2750 examples [2:04:24,  2.58s/ examples]embeddings shape: torch.Size([32, 768, 1641])

Validation DataLoader 0:  18%|‚ñà‚ñä        | 21/115 [05:09<23:05, 14.74s/it][AEpoch 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 479/573 [2:06:17<24:47, 15.82s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3216 examples [2:06:19,  3.34s/ examples]embeddings shape: torch.Size([32, 768, 1672])

Validation DataLoader 0:  19%|‚ñà‚ñâ        | 22/115 [05:27<23:05, 14.90s/it][AEpoch 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 480/573 [2:06:35<24:31, 15.82s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1613])

Validation DataLoader 0:  20%|‚ñà‚ñà        | 23/115 [05:38<22:35, 14.73s/it][AEpoch 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 481/573 [2:06:46<24:14, 15.81s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3416 examples [2:06:56,  2.22s/ examples]embeddings shape: torch.Size([32, 768, 1631])

Validation DataLoader 0:  21%|‚ñà‚ñà        | 24/115 [06:12<23:30, 15.50s/it][AEpoch 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 482/573 [2:07:20<24:02, 15.85s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1631])

Validation DataLoader 0:  22%|‚ñà‚ñà‚ñè       | 25/115 [06:25<23:07, 15.42s/it][AEpoch 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 483/573 [2:07:33<23:46, 15.85s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 2800 examples [2:06:42,  2.63s/ examples]embeddings shape: torch.Size([32, 768, 1614])

Validation DataLoader 0:  23%|‚ñà‚ñà‚ñé       | 26/115 [06:57<23:47, 16.04s/it][AEpoch 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 484/573 [2:08:05<23:33, 15.88s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1592])

Validation DataLoader 0:  23%|‚ñà‚ñà‚ñé       | 27/115 [07:08<23:16, 15.87s/it][AEpoch 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 485/573 [2:08:16<23:16, 15.87s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1681])

Validation DataLoader 0:  24%|‚ñà‚ñà‚ñç       | 28/115 [07:21<22:50, 15.76s/it][AEpoch 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 486/573 [2:08:29<23:00, 15.86s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1618])

Validation DataLoader 0:  25%|‚ñà‚ñà‚ñå       | 29/115 [07:33<22:25, 15.65s/it][AEpoch 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 487/573 [2:08:42<22:43, 15.86s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3477 examples [2:08:48,  2.10s/ examples]embeddings shape: torch.Size([32, 768, 1618])

Validation DataLoader 0:  26%|‚ñà‚ñà‚ñå       | 30/115 [08:02<22:47, 16.09s/it][AEpoch 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 488/573 [2:09:10<22:30, 15.88s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1540])

Validation DataLoader 0:  27%|‚ñà‚ñà‚ñã       | 31/115 [08:11<22:11, 15.85s/it][AEpoch 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 489/573 [2:09:19<22:12, 15.87s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1574])

Validation DataLoader 0:  28%|‚ñà‚ñà‚ñä       | 32/115 [08:30<22:03, 15.95s/it][AEpoch 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 490/573 [2:09:38<21:57, 15.87s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1577])

Validation DataLoader 0:  29%|‚ñà‚ñà‚ñä       | 33/115 [09:04<22:32, 16.49s/it][AEpoch 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 491/573 [2:10:12<21:44, 15.91s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1574])

Validation DataLoader 0:  30%|‚ñà‚ñà‚ñâ       | 34/115 [09:26<22:28, 16.65s/it][AEpoch 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 492/573 [2:10:34<21:29, 15.92s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3283 examples [2:10:38,  3.49s/ examples]Generating train split: 2850 examples [2:09:25,  2.82s/ examples]embeddings shape: torch.Size([32, 768, 1677])

Validation DataLoader 0:  30%|‚ñà‚ñà‚ñà       | 35/115 [09:42<22:10, 16.63s/it][AEpoch 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 493/573 [2:10:50<21:13, 15.92s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1615])

Validation DataLoader 0:  31%|‚ñà‚ñà‚ñà‚ñè      | 36/115 [09:50<21:34, 16.39s/it][AEpoch 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 494/573 [2:10:58<20:56, 15.91s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1582])

Validation DataLoader 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 37/115 [10:01<21:07, 16.24s/it][AEpoch 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 495/573 [2:11:09<20:39, 15.90s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3538 examples [2:11:21,  2.22s/ examples]embeddings shape: torch.Size([32, 768, 1602])

Validation DataLoader 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 38/115 [10:35<21:27, 16.72s/it][AEpoch 0:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 496/573 [2:11:43<20:26, 15.93s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1545])

Validation DataLoader 0:  34%|‚ñà‚ñà‚ñà‚ñç      | 39/115 [10:44<20:56, 16.53s/it][AEpoch 0:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 497/573 [2:11:52<20:09, 15.92s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1615])

Validation DataLoader 0:  35%|‚ñà‚ñà‚ñà‚ñç      | 40/115 [10:52<20:22, 16.30s/it][AEpoch 0:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 498/573 [2:12:00<19:52, 15.90s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1602])

Validation DataLoader 0:  36%|‚ñà‚ñà‚ñà‚ñå      | 41/115 [10:59<19:51, 16.10s/it][AEpoch 0:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 499/573 [2:12:08<19:35, 15.89s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1656])

Validation DataLoader 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 42/115 [11:20<19:42, 16.20s/it][AEpoch 0:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 500/573 [2:12:28<19:20, 15.90s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 2900 examples [2:11:26,  2.70s/ examples]embeddings shape: torch.Size([32, 768, 1654])

Validation DataLoader 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 43/115 [11:44<19:40, 16.39s/it][AEpoch 0:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 501/573 [2:12:53<19:05, 15.91s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1570])

Validation DataLoader 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 44/115 [11:53<19:11, 16.22s/it][AEpoch 0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 502/573 [2:13:01<18:48, 15.90s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1627])

Validation DataLoader 0:  39%|‚ñà‚ñà‚ñà‚ñâ      | 45/115 [12:02<18:44, 16.06s/it][AEpoch 0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 503/573 [2:13:10<18:32, 15.89s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1646])

Validation DataLoader 0:  40%|‚ñà‚ñà‚ñà‚ñà      | 46/115 [12:32<18:48, 16.36s/it][AEpoch 0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 504/573 [2:13:40<18:18, 15.91s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3599 examples [2:13:21,  2.15s/ examples]embeddings shape: torch.Size([32, 768, 1492])

Validation DataLoader 0:  41%|‚ñà‚ñà‚ñà‚ñà      | 47/115 [12:39<18:18, 16.15s/it][AEpoch 0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 505/573 [2:13:47<18:00, 15.90s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1575])

Validation DataLoader 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 48/115 [12:47<17:51, 15.99s/it][AEpoch 0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 506/573 [2:13:55<17:44, 15.88s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1594])

Validation DataLoader 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 49/115 [12:53<17:22, 15.79s/it][AEpoch 0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 507/573 [2:14:01<17:26, 15.86s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1549])

Validation DataLoader 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 50/115 [13:00<16:54, 15.61s/it][AEpoch 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 508/573 [2:14:08<17:09, 15.84s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1532])

Validation DataLoader 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51/115 [13:16<16:39, 15.61s/it][AEpoch 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 509/573 [2:14:24<16:53, 15.84s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1573])

Validation DataLoader 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 52/115 [13:44<16:38, 15.86s/it][AEpoch 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 510/573 [2:14:52<16:39, 15.87s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1604])

Validation DataLoader 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 53/115 [14:06<16:30, 15.98s/it][AEpoch 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 511/573 [2:15:15<16:24, 15.88s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 2950 examples [2:14:13,  2.89s/ examples]Generating train split: 3350 examples [2:15:31,  3.76s/ examples]embeddings shape: torch.Size([32, 768, 1557])

Validation DataLoader 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 54/115 [14:30<16:22, 16.11s/it][AEpoch 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 512/573 [2:15:38<16:09, 15.89s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1535])

Validation DataLoader 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 55/115 [14:36<15:56, 15.95s/it][AEpoch 0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 513/573 [2:15:45<15:52, 15.88s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 802])

Validation DataLoader 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 56/115 [14:46<15:34, 15.83s/it][AEpoch 0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 514/573 [2:15:54<15:36, 15.87s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3660 examples [2:15:57,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1446])

Validation DataLoader 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 57/115 [15:09<15:25, 15.96s/it][AEpoch 0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 515/573 [2:16:18<15:21, 15.88s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1251])

Validation DataLoader 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 58/115 [15:16<15:00, 15.80s/it][AEpoch 0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 516/573 [2:16:24<15:04, 15.86s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1607])

Validation DataLoader 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 59/115 [15:22<14:35, 15.63s/it][AEpoch 0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 517/573 [2:16:30<14:47, 15.84s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1191])

Validation DataLoader 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 60/115 [15:28<14:11, 15.48s/it][AEpoch 0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 518/573 [2:16:37<14:30, 15.82s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1343])

Validation DataLoader 0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 61/115 [15:35<13:47, 15.33s/it][AEpoch 0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 519/573 [2:16:43<14:13, 15.81s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 985])

Validation DataLoader 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 62/115 [15:40<13:23, 15.17s/it][AEpoch 0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 520/573 [2:16:48<13:56, 15.79s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1337])

Validation DataLoader 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 63/115 [15:45<13:00, 15.00s/it][AEpoch 0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 521/573 [2:16:53<13:39, 15.76s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1107])

Validation DataLoader 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64/115 [15:49<12:36, 14.83s/it][AEpoch 0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 522/573 [2:16:57<13:22, 15.74s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1142])

Validation DataLoader 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 65/115 [15:53<12:13, 14.67s/it][AEpoch 0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 523/573 [2:17:01<13:06, 15.72s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1570])

Validation DataLoader 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 66/115 [16:06<11:57, 14.64s/it][AEpoch 0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 524/573 [2:17:14<12:49, 15.71s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3000 examples [2:16:20,  2.79s/ examples]embeddings shape: torch.Size([32, 768, 1587])

Validation DataLoader 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 67/115 [16:36<11:54, 14.88s/it][AEpoch 0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 525/573 [2:17:44<12:35, 15.74s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1609])

Validation DataLoader 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 68/115 [17:00<11:45, 15.00s/it][AEpoch 0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 526/573 [2:18:08<12:20, 15.76s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3721 examples [2:18:03,  2.21s/ examples]embeddings shape: torch.Size([32, 768, 1595])

Validation DataLoader 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 69/115 [17:20<11:33, 15.08s/it][AEpoch 0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 527/573 [2:18:28<12:05, 15.77s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1593])

Validation DataLoader 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 70/115 [17:31<11:15, 15.02s/it][AEpoch 0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 528/573 [2:18:39<11:49, 15.76s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1575])

Validation DataLoader 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 71/115 [17:59<11:09, 15.21s/it][AEpoch 0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 529/573 [2:19:07<11:34, 15.78s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3417 examples [2:19:31,  3.71s/ examples]embeddings shape: torch.Size([32, 768, 1684])

Validation DataLoader 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 72/115 [18:35<11:06, 15.49s/it][AEpoch 0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 530/573 [2:19:43<11:20, 15.82s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1662])

Validation DataLoader 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 73/115 [18:47<10:48, 15.45s/it][AEpoch 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 531/573 [2:19:55<11:04, 15.81s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1617])

Validation DataLoader 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 74/115 [19:17<10:41, 15.65s/it][AEpoch 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 532/573 [2:20:26<10:49, 15.84s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3050 examples [2:19:15,  3.00s/ examples]embeddings shape: torch.Size([32, 768, 1639])

Validation DataLoader 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 75/115 [19:50<10:34, 15.87s/it][AEpoch 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 533/573 [2:20:58<10:34, 15.87s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3782 examples [2:20:51,  2.37s/ examples]embeddings shape: torch.Size([32, 768, 1612])

Validation DataLoader 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 76/115 [20:10<10:21, 15.93s/it][AEpoch 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 534/573 [2:21:18<10:19, 15.88s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1581])

Validation DataLoader 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 77/115 [20:18<10:01, 15.82s/it][AEpoch 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 535/573 [2:21:26<10:02, 15.86s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1159])

Validation DataLoader 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 78/115 [20:25<09:41, 15.71s/it][AEpoch 0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 536/573 [2:21:33<09:46, 15.85s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1591])

Validation DataLoader 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 79/115 [20:32<09:21, 15.60s/it][AEpoch 0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 537/573 [2:21:40<09:29, 15.83s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1455])

Validation DataLoader 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 80/115 [20:42<09:03, 15.53s/it][AEpoch 0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 538/573 [2:21:50<09:13, 15.82s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1566])

Validation DataLoader 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 81/115 [21:08<08:52, 15.66s/it][AEpoch 0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 539/573 [2:22:16<08:58, 15.84s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3484 examples [2:22:11,  3.31s/ examples]embeddings shape: torch.Size([32, 768, 1571])

Validation DataLoader 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 82/115 [21:15<08:33, 15.55s/it][AEpoch 0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 540/573 [2:22:23<08:42, 15.82s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1568])

Validation DataLoader 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 83/115 [21:22<08:14, 15.45s/it][AEpoch 0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 541/573 [2:22:30<08:25, 15.81s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3100 examples [2:21:39,  2.96s/ examples]embeddings shape: torch.Size([32, 768, 1600])

Validation DataLoader 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 84/115 [21:54<08:05, 15.65s/it][AEpoch 0:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 542/573 [2:23:02<08:10, 15.83s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3843 examples [2:23:15,  2.37s/ examples]embeddings shape: torch.Size([32, 768, 1569])

Validation DataLoader 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 85/115 [22:27<07:55, 15.85s/it][AEpoch 0:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 543/573 [2:23:35<07:55, 15.87s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1626])

Validation DataLoader 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 86/115 [22:36<07:37, 15.77s/it][AEpoch 0:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 544/573 [2:23:44<07:39, 15.85s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1599])

Validation DataLoader 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 87/115 [22:48<07:20, 15.73s/it][AEpoch 0:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 545/573 [2:23:56<07:23, 15.85s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1553])

Validation DataLoader 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 88/115 [22:57<07:02, 15.65s/it][AEpoch 0:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 546/573 [2:24:05<07:07, 15.83s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1636])

Validation DataLoader 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 89/115 [23:04<06:44, 15.56s/it][AEpoch 0:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 547/573 [2:24:13<06:51, 15.82s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1600])

Validation DataLoader 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 90/115 [23:22<06:29, 15.58s/it][AEpoch 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 548/573 [2:24:30<06:35, 15.82s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3551 examples [2:24:40,  2.98s/ examples]embeddings shape: torch.Size([32, 768, 1475])

Validation DataLoader 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 91/115 [23:41<06:14, 15.62s/it][AEpoch 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 549/573 [2:24:49<06:19, 15.83s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1533])

Validation DataLoader 0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 92/115 [23:44<05:56, 15.49s/it][AEpoch 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 550/573 [2:24:53<06:03, 15.81s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1593])

Validation DataLoader 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 93/115 [24:10<05:43, 15.60s/it][AEpoch 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 551/573 [2:25:19<05:48, 15.82s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 48])

Validation DataLoader 0:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 94/115 [24:12<05:24, 15.45s/it][AEpoch 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 552/573 [2:25:20<05:31, 15.80s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3150 examples [2:24:00,  2.92s/ examples]embeddings shape: torch.Size([32, 768, 1074])

Validation DataLoader 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 95/115 [24:17<05:06, 15.34s/it][AEpoch 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 553/573 [2:25:25<05:15, 15.78s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3904 examples [2:25:34,  2.34s/ examples]embeddings shape: torch.Size([32, 768, 1660])

Validation DataLoader 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 96/115 [24:51<04:55, 15.54s/it][AEpoch 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 554/573 [2:25:59<05:00, 15.81s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1624])

Validation DataLoader 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 97/115 [25:02<04:38, 15.49s/it][AEpoch 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 555/573 [2:26:10<04:44, 15.80s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1568])

Validation DataLoader 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 98/115 [25:08<04:21, 15.39s/it][AEpoch 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 556/573 [2:26:16<04:28, 15.79s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1581])

Validation DataLoader 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 99/115 [25:18<04:05, 15.34s/it][AEpoch 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 557/573 [2:26:26<04:12, 15.77s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1602])

Validation DataLoader 0:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 100/115 [25:26<03:48, 15.26s/it][AEpoch 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 558/573 [2:26:34<03:56, 15.76s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1598])

Validation DataLoader 0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 101/115 [25:32<03:32, 15.17s/it][AEpoch 0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 559/573 [2:26:40<03:40, 15.74s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1603])

Validation DataLoader 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 102/115 [25:41<03:16, 15.11s/it][AEpoch 0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 560/573 [2:26:49<03:24, 15.73s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1564])

Validation DataLoader 0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 103/115 [26:13<03:03, 15.28s/it][AEpoch 0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 561/573 [2:27:21<03:09, 15.76s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 1615])

Validation DataLoader 0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 104/115 [26:27<02:47, 15.27s/it][AEpoch 0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 562/573 [2:27:35<02:53, 15.76s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Generating train split: 3200 examples [2:26:20,  2.88s/ examples]embeddings shape: torch.Size([32, 768, 1659])

Validation DataLoader 0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 105/115 [27:06<02:34, 15.49s/it][AEpoch 0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 563/573 [2:28:14<02:37, 15.80s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 34])

Validation DataLoader 0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 106/115 [27:07<02:18, 15.35s/it][AEpoch 0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 564/573 [2:28:15<02:21, 15.77s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 30])

Validation DataLoader 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 107/115 [27:08<02:01, 15.22s/it][AEpoch 0:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 565/573 [2:28:16<02:05, 15.75s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 25])

Validation DataLoader 0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 108/115 [27:08<01:45, 15.08s/it][AEpoch 0:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 566/573 [2:28:17<01:50, 15.72s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 109/115 [27:09<01:29, 14.95s/it][AEpoch 0:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 567/573 [2:28:17<01:34, 15.69s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 110/115 [27:09<01:14, 14.81s/it][AEpoch 0:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 568/573 [2:28:17<01:18, 15.66s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 111/115 [27:09<00:58, 14.68s/it][AEpoch 0:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 569/573 [2:28:17<01:02, 15.64s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 112/115 [27:09<00:43, 14.55s/it][AEpoch 0:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 570/573 [2:28:18<00:46, 15.61s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 113/115 [27:10<00:28, 14.43s/it][AEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 571/573 [2:28:18<00:31, 15.58s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([32, 768, 6])

Validation DataLoader 0:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 114/115 [27:10<00:14, 14.30s/it][AEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 572/573 [2:28:18<00:15, 15.56s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]embeddings shape: torch.Size([22, 768, 22])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [27:10<00:00, 14.18s/it][AEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 573/573 [2:28:18<00:00, 15.53s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 573/573 [2:28:18<00:00, 15.53s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838]
                                                                          [A/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('train categorical_accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('train categorical_accuracy_strict', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/sxr280/miniconda3/envs/deeploc_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('train binary_accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
Metric val_loss improved. New best score: 0.496
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 573/573 [2:28:18<00:00, 15.53s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3618 examples [2:28:13,  3.04s/ examples]Epoch 0, global step 458: 'val_loss' reached 0.49590 (best 0.49590), saving model to '/home/sxr280/BERTLocRNA/output/RNAlocalization/DNABERT2/checkpoints/checkpoints_best-v6.ckpt' as top 1
Epoch duration: 8899.70 seconds
Epoch 0:   0%|          | 0/573 [00:00<?, ?it/s, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]            Epoch 1:   0%|          | 0/573 [00:00<?, ?it/s, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3965 examples [2:28:03,  2.37s/ examples]embeddings shape: torch.Size([32, 768, 1560])
Epoch 1:   0%|          | 1/573 [00:09<1:27:29,  9.18s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.538, train categorical_accuracy_strict_step=0.385, train binary_accuracy_step=0.893, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   0%|          | 1/573 [00:09<1:27:30,  9.18s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1591])
Epoch 1:   0%|          | 2/573 [00:14<1:08:14,  7.17s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   0%|          | 2/573 [00:14<1:08:14,  7.17s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1608])
Epoch 1:   1%|          | 3/573 [00:19<1:00:59,  6.42s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   1%|          | 3/573 [00:19<1:00:59,  6.42s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1571])
Epoch 1:   1%|          | 4/573 [00:25<1:01:02,  6.44s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   1%|          | 4/573 [00:25<1:01:02,  6.44s/it, loss=0.341, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1613])
Epoch 1:   1%|          | 5/573 [00:34<1:04:39,  6.83s/it, loss=0.341, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   1%|          | 5/573 [00:34<1:04:39,  6.83s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1649])
Epoch 1:   1%|          | 6/573 [00:42<1:06:43,  7.06s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   1%|          | 6/573 [00:42<1:06:43,  7.06s/it, loss=0.341, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1619])
Epoch 1:   1%|          | 7/573 [00:50<1:07:33,  7.16s/it, loss=0.341, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   1%|          | 7/573 [00:50<1:07:33,  7.16s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1590])
Epoch 1:   1%|‚ñè         | 8/573 [00:54<1:03:44,  6.77s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   1%|‚ñè         | 8/573 [00:54<1:03:44,  6.77s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1590])
Epoch 1:   2%|‚ñè         | 9/573 [01:05<1:08:09,  7.25s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   2%|‚ñè         | 9/573 [01:05<1:08:09,  7.25s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3250 examples [2:28:26,  2.78s/ examples]embeddings shape: torch.Size([32, 768, 1573])
Epoch 1:   2%|‚ñè         | 10/573 [01:28<1:22:38,  8.81s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   2%|‚ñè         | 10/573 [01:28<1:22:38,  8.81s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1591])
Epoch 1:   2%|‚ñè         | 11/573 [01:41<1:26:23,  9.22s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   2%|‚ñè         | 11/573 [01:41<1:26:23,  9.22s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4026 examples [2:29:57,  2.22s/ examples]embeddings shape: torch.Size([32, 768, 1588])
Epoch 1:   2%|‚ñè         | 12/573 [01:58<1:32:33,  9.90s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   2%|‚ñè         | 12/573 [01:58<1:32:33,  9.90s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1581])
Epoch 1:   2%|‚ñè         | 13/573 [02:05<1:30:09,  9.66s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   2%|‚ñè         | 13/573 [02:05<1:30:09,  9.66s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1568])
Epoch 1:   2%|‚ñè         | 14/573 [02:22<1:34:46, 10.17s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   2%|‚ñè         | 14/573 [02:22<1:34:46, 10.17s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1553])
Epoch 1:   3%|‚ñé         | 15/573 [02:40<1:39:29, 10.70s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   3%|‚ñé         | 15/573 [02:40<1:39:29, 10.70s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3685 examples [2:30:55,  2.86s/ examples]embeddings shape: torch.Size([32, 768, 1630])
Epoch 1:   3%|‚ñé         | 16/573 [02:47<1:37:09, 10.47s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   3%|‚ñé         | 16/573 [02:47<1:37:09, 10.47s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1477])
Epoch 1:   3%|‚ñé         | 17/573 [02:53<1:34:18, 10.18s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   3%|‚ñé         | 17/573 [02:53<1:34:18, 10.18s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1572])
Epoch 1:   3%|‚ñé         | 18/573 [03:01<1:33:11, 10.07s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   3%|‚ñé         | 18/573 [03:01<1:33:11, 10.07s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1601])
Epoch 1:   3%|‚ñé         | 19/573 [03:08<1:31:22,  9.90s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   3%|‚ñé         | 19/573 [03:08<1:31:22,  9.90s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1618])
Epoch 1:   3%|‚ñé         | 20/573 [03:14<1:29:31,  9.71s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   3%|‚ñé         | 20/573 [03:14<1:29:31,  9.71s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1630])
Epoch 1:   4%|‚ñé         | 21/573 [03:20<1:27:53,  9.55s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   4%|‚ñé         | 21/573 [03:20<1:27:54,  9.55s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1540])
Epoch 1:   4%|‚ñç         | 22/573 [03:40<1:31:54, 10.01s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   4%|‚ñç         | 22/573 [03:40<1:31:54, 10.01s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3300 examples [2:30:58,  2.85s/ examples]embeddings shape: torch.Size([32, 768, 1599])
Epoch 1:   4%|‚ñç         | 23/573 [04:05<1:37:50, 10.67s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   4%|‚ñç         | 23/573 [04:05<1:37:50, 10.67s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4087 examples [2:32:18,  2.25s/ examples]embeddings shape: torch.Size([32, 768, 1628])
Epoch 1:   4%|‚ñç         | 24/573 [04:22<1:40:01, 10.93s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   4%|‚ñç         | 24/573 [04:22<1:40:01, 10.93s/it, loss=0.356, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1568])
Epoch 1:   4%|‚ñç         | 25/573 [04:29<1:38:37, 10.80s/it, loss=0.356, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   4%|‚ñç         | 25/573 [04:29<1:38:37, 10.80s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1485])
Epoch 1:   5%|‚ñç         | 26/573 [04:44<1:39:35, 10.92s/it, loss=0.354, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   5%|‚ñç         | 26/573 [04:44<1:39:35, 10.92s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1432])
Epoch 1:   5%|‚ñç         | 27/573 [05:03<1:42:24, 11.25s/it, loss=0.355, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   5%|‚ñç         | 27/573 [05:03<1:42:24, 11.25s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3752 examples [2:33:18,  2.64s/ examples]embeddings shape: torch.Size([32, 768, 1609])
Epoch 1:   5%|‚ñç         | 28/573 [05:12<1:41:25, 11.17s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   5%|‚ñç         | 28/573 [05:12<1:41:25, 11.17s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1554])
Epoch 1:   5%|‚ñå         | 29/573 [05:18<1:39:25, 10.97s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   5%|‚ñå         | 29/573 [05:18<1:39:25, 10.97s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1622])
Epoch 1:   5%|‚ñå         | 30/573 [05:25<1:38:12, 10.85s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   5%|‚ñå         | 30/573 [05:25<1:38:12, 10.85s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1602])
Epoch 1:   5%|‚ñå         | 31/573 [05:33<1:37:11, 10.76s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   5%|‚ñå         | 31/573 [05:33<1:37:11, 10.76s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1622])
Epoch 1:   6%|‚ñå         | 32/573 [05:39<1:35:38, 10.61s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   6%|‚ñå         | 32/573 [05:39<1:35:38, 10.61s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1593])
Epoch 1:   6%|‚ñå         | 33/573 [05:51<1:35:44, 10.64s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   6%|‚ñå         | 33/573 [05:51<1:35:44, 10.64s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1683])
Epoch 1:   6%|‚ñå         | 34/573 [06:16<1:39:29, 11.07s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   6%|‚ñå         | 34/573 [06:16<1:39:29, 11.07s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4148 examples [2:34:26,  2.20s/ examples]embeddings shape: torch.Size([32, 768, 1638])
Epoch 1:   6%|‚ñå         | 35/573 [06:45<1:43:50, 11.58s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   6%|‚ñå         | 35/573 [06:45<1:43:50, 11.58s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3350 examples [2:33:45,  3.00s/ examples]embeddings shape: torch.Size([32, 768, 1335])
Epoch 1:   6%|‚ñã         | 36/573 [06:50<1:42:07, 11.41s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   6%|‚ñã         | 36/573 [06:50<1:42:07, 11.41s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1518])
Epoch 1:   6%|‚ñã         | 37/573 [06:57<1:40:48, 11.28s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   6%|‚ñã         | 37/573 [06:57<1:40:48, 11.28s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1616])
Epoch 1:   7%|‚ñã         | 38/573 [07:02<1:39:13, 11.13s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   7%|‚ñã         | 38/573 [07:02<1:39:13, 11.13s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1609])
Epoch 1:   7%|‚ñã         | 39/573 [07:08<1:37:47, 10.99s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   7%|‚ñã         | 39/573 [07:08<1:37:47, 10.99s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1605])
Epoch 1:   7%|‚ñã         | 40/573 [07:15<1:36:44, 10.89s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   7%|‚ñã         | 40/573 [07:15<1:36:44, 10.89s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1574])
Epoch 1:   7%|‚ñã         | 41/573 [07:33<1:38:09, 11.07s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   7%|‚ñã         | 41/573 [07:33<1:38:09, 11.07s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3819 examples [2:36:05,  2.59s/ examples]embeddings shape: torch.Size([32, 768, 1573])
Epoch 1:   7%|‚ñã         | 42/573 [07:53<1:39:46, 11.27s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   7%|‚ñã         | 42/573 [07:53<1:39:46, 11.27s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1624])
Epoch 1:   8%|‚ñä         | 43/573 [08:00<1:38:36, 11.16s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   8%|‚ñä         | 43/573 [08:00<1:38:36, 11.16s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1566])
Epoch 1:   8%|‚ñä         | 44/573 [08:05<1:37:16, 11.03s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   8%|‚ñä         | 44/573 [08:05<1:37:16, 11.03s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1623])
Epoch 1:   8%|‚ñä         | 45/573 [08:11<1:36:11, 10.93s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   8%|‚ñä         | 45/573 [08:11<1:36:11, 10.93s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1674])
Epoch 1:   8%|‚ñä         | 46/573 [08:34<1:38:13, 11.18s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   8%|‚ñä         | 46/573 [08:34<1:38:13, 11.18s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4209 examples [2:36:40,  2.20s/ examples]embeddings shape: torch.Size([32, 768, 1621])
Epoch 1:   8%|‚ñä         | 47/573 [08:49<1:38:43, 11.26s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   8%|‚ñä         | 47/573 [08:49<1:38:43, 11.26s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3400 examples [2:36:10,  2.97s/ examples]embeddings shape: torch.Size([32, 768, 1612])
Epoch 1:   8%|‚ñä         | 48/573 [09:13<1:40:53, 11.53s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   8%|‚ñä         | 48/573 [09:13<1:40:53, 11.53s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1600])
Epoch 1:   9%|‚ñä         | 49/573 [09:21<1:40:04, 11.46s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   9%|‚ñä         | 49/573 [09:21<1:40:04, 11.46s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1562])
Epoch 1:   9%|‚ñä         | 50/573 [09:26<1:38:48, 11.33s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   9%|‚ñä         | 50/573 [09:26<1:38:48, 11.33s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1428])
Epoch 1:   9%|‚ñâ         | 51/573 [09:33<1:37:52, 11.25s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   9%|‚ñâ         | 51/573 [09:33<1:37:52, 11.25s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1668])
Epoch 1:   9%|‚ñâ         | 52/573 [09:41<1:37:08, 11.19s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   9%|‚ñâ         | 52/573 [09:41<1:37:08, 11.19s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1406])
Epoch 1:   9%|‚ñâ         | 53/573 [09:48<1:36:09, 11.09s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   9%|‚ñâ         | 53/573 [09:48<1:36:09, 11.09s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1592])
Epoch 1:   9%|‚ñâ         | 54/573 [09:52<1:34:56, 10.97s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:   9%|‚ñâ         | 54/573 [09:52<1:34:56, 10.97s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1596])
Epoch 1:  10%|‚ñâ         | 55/573 [10:13<1:36:22, 11.16s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  10%|‚ñâ         | 55/573 [10:13<1:36:22, 11.16s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3886 examples [2:38:42,  2.52s/ examples]embeddings shape: torch.Size([32, 768, 1578])
Epoch 1:  10%|‚ñâ         | 56/573 [10:36<1:38:00, 11.37s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  10%|‚ñâ         | 56/573 [10:36<1:38:00, 11.37s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4270 examples [2:38:53,  2.19s/ examples]embeddings shape: torch.Size([32, 768, 1567])
Epoch 1:  10%|‚ñâ         | 57/573 [10:55<1:38:57, 11.51s/it, loss=0.349, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  10%|‚ñâ         | 57/573 [10:55<1:38:57, 11.51s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1612])
Epoch 1:  10%|‚ñà         | 58/573 [11:19<1:40:36, 11.72s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  10%|‚ñà         | 58/573 [11:19<1:40:36, 11.72s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3450 examples [2:38:22,  2.87s/ examples]embeddings shape: torch.Size([32, 768, 1596])
Epoch 1:  10%|‚ñà         | 59/573 [11:27<1:39:49, 11.65s/it, loss=0.348, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  10%|‚ñà         | 59/573 [11:27<1:39:49, 11.65s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1608])
Epoch 1:  10%|‚ñà         | 60/573 [11:33<1:38:46, 11.55s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  10%|‚ñà         | 60/573 [11:33<1:38:46, 11.55s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1617])
Epoch 1:  11%|‚ñà         | 61/573 [11:37<1:37:37, 11.44s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  11%|‚ñà         | 61/573 [11:37<1:37:37, 11.44s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1616])
Epoch 1:  11%|‚ñà         | 62/573 [11:43<1:36:35, 11.34s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  11%|‚ñà         | 62/573 [11:43<1:36:35, 11.34s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1613])
Epoch 1:  11%|‚ñà         | 63/573 [11:48<1:35:35, 11.25s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  11%|‚ñà         | 63/573 [11:48<1:35:35, 11.25s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1631])
Epoch 1:  11%|‚ñà         | 64/573 [11:53<1:34:32, 11.15s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  11%|‚ñà         | 64/573 [11:53<1:34:32, 11.15s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1575])
Epoch 1:  11%|‚ñà‚ñè        | 65/573 [11:58<1:33:38, 11.06s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  11%|‚ñà‚ñè        | 65/573 [11:58<1:33:38, 11.06s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1553])
Epoch 1:  12%|‚ñà‚ñè        | 66/573 [12:05<1:32:52, 10.99s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  12%|‚ñà‚ñè        | 66/573 [12:05<1:32:52, 10.99s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1443])
Epoch 1:  12%|‚ñà‚ñè        | 67/573 [12:12<1:32:10, 10.93s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  12%|‚ñà‚ñè        | 67/573 [12:12<1:32:10, 10.93s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1598])
Epoch 1:  12%|‚ñà‚ñè        | 68/573 [12:32<1:33:09, 11.07s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  12%|‚ñà‚ñè        | 68/573 [12:32<1:33:09, 11.07s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4331 examples [2:40:38,  2.06s/ examples]embeddings shape: torch.Size([32, 768, 1647])
Epoch 1:  12%|‚ñà‚ñè        | 69/573 [12:50<1:33:44, 11.16s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  12%|‚ñà‚ñè        | 69/573 [12:50<1:33:44, 11.16s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1608])
Epoch 1:  12%|‚ñà‚ñè        | 70/573 [13:13<1:35:04, 11.34s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  12%|‚ñà‚ñè        | 70/573 [13:13<1:35:04, 11.34s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3500 examples [2:40:26,  2.75s/ examples]embeddings shape: torch.Size([32, 768, 1594])
Epoch 1:  12%|‚ñà‚ñè        | 71/573 [13:31<1:35:40, 11.44s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  12%|‚ñà‚ñè        | 71/573 [13:31<1:35:40, 11.44s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] Generating train split: 3953 examples [2:41:58,  2.64s/ examples]embeddings shape: torch.Size([32, 768, 1590])
Epoch 1:  13%|‚ñà‚ñé        | 72/573 [13:46<1:35:51, 11.48s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  13%|‚ñà‚ñé        | 72/573 [13:46<1:35:51, 11.48s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1570])
Epoch 1:  13%|‚ñà‚ñé        | 73/573 [13:51<1:34:55, 11.39s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  13%|‚ñà‚ñé        | 73/573 [13:51<1:34:55, 11.39s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1653])
Epoch 1:  13%|‚ñà‚ñé        | 74/573 [13:59<1:34:17, 11.34s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  13%|‚ñà‚ñé        | 74/573 [13:59<1:34:17, 11.34s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1645])
Epoch 1:  13%|‚ñà‚ñé        | 75/573 [14:07<1:33:45, 11.30s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  13%|‚ñà‚ñé        | 75/573 [14:07<1:33:45, 11.30s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1598])
Epoch 1:  13%|‚ñà‚ñé        | 76/573 [14:13<1:33:04, 11.24s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  13%|‚ñà‚ñé        | 76/573 [14:13<1:33:04, 11.24s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1651])
Epoch 1:  13%|‚ñà‚ñé        | 77/573 [14:20<1:32:25, 11.18s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  13%|‚ñà‚ñé        | 77/573 [14:20<1:32:25, 11.18s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4392 examples [2:42:50,  2.09s/ examples]embeddings shape: torch.Size([32, 768, 1615])
Epoch 1:  14%|‚ñà‚ñé        | 78/573 [14:51<1:34:15, 11.43s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  14%|‚ñà‚ñé        | 78/573 [14:51<1:34:15, 11.43s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.788, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1588])
Epoch 1:  14%|‚ñà‚ñç        | 79/573 [14:58<1:33:36, 11.37s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.788, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  14%|‚ñà‚ñç        | 79/573 [14:58<1:33:36, 11.37s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1566])
Epoch 1:  14%|‚ñà‚ñç        | 80/573 [15:26<1:35:10, 11.58s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  14%|‚ñà‚ñç        | 80/573 [15:26<1:35:10, 11.58s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3550 examples [2:42:29,  2.67s/ examples]embeddings shape: torch.Size([32, 768, 1608])
Epoch 1:  14%|‚ñà‚ñç        | 81/573 [15:36<1:34:45, 11.56s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  14%|‚ñà‚ñç        | 81/573 [15:36<1:34:45, 11.56s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1630])
Epoch 1:  14%|‚ñà‚ñç        | 82/573 [15:41<1:33:59, 11.49s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  14%|‚ñà‚ñç        | 82/573 [15:41<1:33:59, 11.49s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1660])
Epoch 1:  14%|‚ñà‚ñç        | 83/573 [16:03<1:34:49, 11.61s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  14%|‚ñà‚ñç        | 83/573 [16:03<1:34:49, 11.61s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4020 examples [2:44:29,  2.52s/ examples]embeddings shape: torch.Size([32, 768, 1571])
Epoch 1:  15%|‚ñà‚ñç        | 84/573 [16:19<1:35:01, 11.66s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  15%|‚ñà‚ñç        | 84/573 [16:19<1:35:01, 11.66s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1506])
Epoch 1:  15%|‚ñà‚ñç        | 85/573 [16:25<1:34:18, 11.59s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  15%|‚ñà‚ñç        | 85/573 [16:25<1:34:18, 11.59s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1625])
Epoch 1:  15%|‚ñà‚ñå        | 86/573 [16:31<1:33:32, 11.53s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  15%|‚ñà‚ñå        | 86/573 [16:31<1:33:32, 11.53s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1591])
Epoch 1:  15%|‚ñà‚ñå        | 87/573 [16:37<1:32:50, 11.46s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  15%|‚ñà‚ñå        | 87/573 [16:37<1:32:50, 11.46s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1579])
Epoch 1:  15%|‚ñà‚ñå        | 88/573 [17:00<1:33:41, 11.59s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  15%|‚ñà‚ñå        | 88/573 [17:00<1:33:41, 11.59s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4453 examples [2:45:10,  2.15s/ examples]embeddings shape: torch.Size([32, 768, 1588])
Epoch 1:  16%|‚ñà‚ñå        | 89/573 [17:16<1:33:57, 11.65s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  16%|‚ñà‚ñå        | 89/573 [17:16<1:33:57, 11.65s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1621])
Epoch 1:  16%|‚ñà‚ñå        | 90/573 [17:28<1:33:49, 11.66s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  16%|‚ñà‚ñå        | 90/573 [17:28<1:33:49, 11.66s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3600 examples [2:44:53,  2.73s/ examples]embeddings shape: torch.Size([32, 768, 1409])
Epoch 1:  16%|‚ñà‚ñå        | 91/573 [17:56<1:35:01, 11.83s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  16%|‚ñà‚ñå        | 91/573 [17:56<1:35:01, 11.83s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1599])
Epoch 1:  16%|‚ñà‚ñå        | 92/573 [18:03<1:34:25, 11.78s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  16%|‚ñà‚ñå        | 92/573 [18:03<1:34:25, 11.78s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1625])
Epoch 1:  16%|‚ñà‚ñå        | 93/573 [18:11<1:33:51, 11.73s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  16%|‚ñà‚ñå        | 93/573 [18:11<1:33:51, 11.73s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1564])
Epoch 1:  16%|‚ñà‚ñã        | 94/573 [18:17<1:33:12, 11.68s/it, loss=0.347, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  16%|‚ñà‚ñã        | 94/573 [18:17<1:33:12, 11.68s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.500, train binary_accuracy_step=0.913, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1613])
Epoch 1:  17%|‚ñà‚ñã        | 95/573 [18:37<1:33:43, 11.76s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.500, train binary_accuracy_step=0.913, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  17%|‚ñà‚ñã        | 95/573 [18:37<1:33:43, 11.76s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4087 examples [2:47:07,  2.47s/ examples]embeddings shape: torch.Size([32, 768, 1560])
Epoch 1:  17%|‚ñà‚ñã        | 96/573 [18:58<1:34:14, 11.85s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  17%|‚ñà‚ñã        | 96/573 [18:58<1:34:14, 11.85s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1571])
Epoch 1:  17%|‚ñà‚ñã        | 97/573 [19:05<1:33:40, 11.81s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  17%|‚ñà‚ñã        | 97/573 [19:05<1:33:40, 11.81s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] Generating train split: 4514 examples [2:47:35,  2.22s/ examples]embeddings shape: torch.Size([32, 768, 1603])
Epoch 1:  17%|‚ñà‚ñã        | 98/573 [19:36<1:35:04, 12.01s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  17%|‚ñà‚ñã        | 98/573 [19:36<1:35:04, 12.01s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1591])
Epoch 1:  17%|‚ñà‚ñã        | 99/573 [19:42<1:34:19, 11.94s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  17%|‚ñà‚ñã        | 99/573 [19:42<1:34:19, 11.94s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1598])
Epoch 1:  17%|‚ñà‚ñã        | 100/573 [19:53<1:34:03, 11.93s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  17%|‚ñà‚ñã        | 100/573 [19:53<1:34:03, 11.93s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3650 examples [2:47:12,  2.75s/ examples]embeddings shape: torch.Size([32, 768, 1596])
Epoch 1:  18%|‚ñà‚ñä        | 101/573 [20:15<1:34:40, 12.04s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  18%|‚ñà‚ñä        | 101/573 [20:15<1:34:40, 12.04s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1459])
Epoch 1:  18%|‚ñà‚ñä        | 102/573 [20:22<1:34:06, 11.99s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  18%|‚ñà‚ñä        | 102/573 [20:22<1:34:06, 11.99s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1571])
Epoch 1:  18%|‚ñà‚ñä        | 103/573 [20:29<1:33:29, 11.93s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  18%|‚ñà‚ñä        | 103/573 [20:29<1:33:29, 11.93s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1632])
Epoch 1:  18%|‚ñà‚ñä        | 104/573 [20:36<1:32:56, 11.89s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  18%|‚ñà‚ñä        | 104/573 [20:36<1:32:56, 11.89s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1602])
Epoch 1:  18%|‚ñà‚ñä        | 105/573 [20:43<1:32:22, 11.84s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  18%|‚ñà‚ñä        | 105/573 [20:43<1:32:22, 11.84s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1671])
Epoch 1:  18%|‚ñà‚ñä        | 106/573 [20:51<1:31:52, 11.80s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  18%|‚ñà‚ñä        | 106/573 [20:51<1:31:52, 11.80s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1595])
Epoch 1:  19%|‚ñà‚ñä        | 107/573 [21:10<1:32:13, 11.87s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  19%|‚ñà‚ñä        | 107/573 [21:10<1:32:13, 11.87s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1694])
Epoch 1:  19%|‚ñà‚ñâ        | 108/573 [21:41<1:33:24, 12.05s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  19%|‚ñà‚ñâ        | 108/573 [21:41<1:33:24, 12.05s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.906, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4575 examples [2:49:50,  2.22s/ examples]Generating train split: 4154 examples [2:50:08,  2.55s/ examples]embeddings shape: torch.Size([32, 768, 1698])
Epoch 1:  19%|‚ñà‚ñâ        | 109/573 [21:57<1:33:27, 12.09s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.906, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  19%|‚ñà‚ñâ        | 109/573 [21:57<1:33:27, 12.09s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1624])
Epoch 1:  19%|‚ñà‚ñâ        | 110/573 [22:17<1:33:50, 12.16s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  19%|‚ñà‚ñâ        | 110/573 [22:17<1:33:50, 12.16s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3700 examples [2:49:28,  2.74s/ examples]embeddings shape: torch.Size([32, 768, 1548])
Epoch 1:  19%|‚ñà‚ñâ        | 111/573 [22:33<1:33:54, 12.20s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  19%|‚ñà‚ñâ        | 111/573 [22:33<1:33:54, 12.20s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 970])
Epoch 1:  20%|‚ñà‚ñâ        | 112/573 [22:38<1:33:12, 12.13s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  20%|‚ñà‚ñâ        | 112/573 [22:38<1:33:12, 12.13s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1609])
Epoch 1:  20%|‚ñà‚ñâ        | 113/573 [22:47<1:32:44, 12.10s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  20%|‚ñà‚ñâ        | 113/573 [22:47<1:32:44, 12.10s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1007])
Epoch 1:  20%|‚ñà‚ñâ        | 114/573 [22:51<1:32:02, 12.03s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  20%|‚ñà‚ñâ        | 114/573 [22:51<1:32:02, 12.03s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1585])
Epoch 1:  20%|‚ñà‚ñà        | 115/573 [22:58<1:31:28, 11.98s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  20%|‚ñà‚ñà        | 115/573 [22:58<1:31:28, 11.98s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1624])
Epoch 1:  20%|‚ñà‚ñà        | 116/573 [23:04<1:30:55, 11.94s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  20%|‚ñà‚ñà        | 116/573 [23:04<1:30:55, 11.94s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.799, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1535])
Epoch 1:  20%|‚ñà‚ñà        | 117/573 [23:10<1:30:20, 11.89s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.799, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  20%|‚ñà‚ñà        | 117/573 [23:10<1:30:20, 11.89s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1602])
Epoch 1:  21%|‚ñà‚ñà        | 118/573 [23:16<1:29:44, 11.83s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  21%|‚ñà‚ñà        | 118/573 [23:16<1:29:44, 11.83s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1611])
Epoch 1:  21%|‚ñà‚ñà        | 119/573 [23:37<1:30:08, 11.91s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  21%|‚ñà‚ñà        | 119/573 [23:37<1:30:08, 11.91s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4636 examples [2:51:45,  2.12s/ examples]embeddings shape: torch.Size([32, 768, 1628])
Epoch 1:  21%|‚ñà‚ñà        | 120/573 [23:49<1:29:57, 11.91s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  21%|‚ñà‚ñà        | 120/573 [23:49<1:29:57, 11.91s/it, loss=0.341, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1645])
Epoch 1:  21%|‚ñà‚ñà        | 121/573 [24:18<1:30:47, 12.05s/it, loss=0.341, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  21%|‚ñà‚ñà        | 121/573 [24:18<1:30:47, 12.05s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3750 examples [2:51:18,  2.57s/ examples]embeddings shape: torch.Size([32, 768, 1601])
Epoch 1:  21%|‚ñà‚ñà‚ñè       | 122/573 [24:26<1:30:20, 12.02s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  21%|‚ñà‚ñà‚ñè       | 122/573 [24:26<1:30:20, 12.02s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1636])
Epoch 1:  21%|‚ñà‚ñà‚ñè       | 123/573 [24:38<1:30:09, 12.02s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  21%|‚ñà‚ñà‚ñè       | 123/573 [24:38<1:30:09, 12.02s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1578])
Epoch 1:  22%|‚ñà‚ñà‚ñè       | 124/573 [25:10<1:31:08, 12.18s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  22%|‚ñà‚ñà‚ñè       | 124/573 [25:10<1:31:08, 12.18s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4221 examples [2:53:29,  2.68s/ examples]embeddings shape: torch.Size([32, 768, 1599])
Epoch 1:  22%|‚ñà‚ñà‚ñè       | 125/573 [25:22<1:30:55, 12.18s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  22%|‚ñà‚ñà‚ñè       | 125/573 [25:22<1:30:55, 12.18s/it, loss=0.341, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1572])
Epoch 1:  22%|‚ñà‚ñà‚ñè       | 126/573 [25:28<1:30:23, 12.13s/it, loss=0.341, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  22%|‚ñà‚ñà‚ñè       | 126/573 [25:28<1:30:23, 12.13s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1592])
Epoch 1:  22%|‚ñà‚ñà‚ñè       | 127/573 [25:36<1:29:55, 12.10s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  22%|‚ñà‚ñà‚ñè       | 127/573 [25:36<1:29:55, 12.10s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1599])
Epoch 1:  22%|‚ñà‚ñà‚ñè       | 128/573 [25:50<1:29:50, 12.11s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  22%|‚ñà‚ñà‚ñè       | 128/573 [25:50<1:29:50, 12.11s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4697 examples [2:54:09,  2.19s/ examples]embeddings shape: torch.Size([32, 768, 1589])
Epoch 1:  23%|‚ñà‚ñà‚ñé       | 129/573 [26:12<1:30:11, 12.19s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  23%|‚ñà‚ñà‚ñé       | 129/573 [26:12<1:30:11, 12.19s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1115])
Epoch 1:  23%|‚ñà‚ñà‚ñé       | 130/573 [26:25<1:30:02, 12.20s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  23%|‚ñà‚ñà‚ñé       | 130/573 [26:25<1:30:02, 12.20s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3800 examples [2:53:43,  2.67s/ examples]embeddings shape: torch.Size([32, 768, 1561])
Epoch 1:  23%|‚ñà‚ñà‚ñé       | 131/573 [26:46<1:30:21, 12.26s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  23%|‚ñà‚ñà‚ñé       | 131/573 [26:46<1:30:21, 12.26s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1610])
Epoch 1:  23%|‚ñà‚ñà‚ñé       | 132/573 [26:54<1:29:53, 12.23s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  23%|‚ñà‚ñà‚ñé       | 132/573 [26:54<1:29:53, 12.23s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1631])
Epoch 1:  23%|‚ñà‚ñà‚ñé       | 133/573 [27:01<1:29:24, 12.19s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  23%|‚ñà‚ñà‚ñé       | 133/573 [27:01<1:29:24, 12.19s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1561])
Epoch 1:  23%|‚ñà‚ñà‚ñé       | 134/573 [27:08<1:28:54, 12.15s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  23%|‚ñà‚ñà‚ñé       | 134/573 [27:08<1:28:54, 12.15s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1462])
Epoch 1:  24%|‚ñà‚ñà‚ñé       | 135/573 [27:15<1:28:26, 12.12s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  24%|‚ñà‚ñà‚ñé       | 135/573 [27:15<1:28:26, 12.12s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1653])
Epoch 1:  24%|‚ñà‚ñà‚ñé       | 136/573 [27:23<1:28:01, 12.09s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  24%|‚ñà‚ñà‚ñé       | 136/573 [27:23<1:28:01, 12.09s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1621])
Epoch 1:  24%|‚ñà‚ñà‚ñç       | 137/573 [27:57<1:28:58, 12.24s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  24%|‚ñà‚ñà‚ñç       | 137/573 [27:57<1:28:58, 12.24s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4288 examples [2:56:19,  2.64s/ examples]embeddings shape: torch.Size([32, 768, 1540])
Epoch 1:  24%|‚ñà‚ñà‚ñç       | 138/573 [28:27<1:29:43, 12.38s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  24%|‚ñà‚ñà‚ñç       | 138/573 [28:27<1:29:43, 12.38s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4758 examples [2:56:37,  2.26s/ examples]embeddings shape: torch.Size([32, 768, 1606])
Epoch 1:  24%|‚ñà‚ñà‚ñç       | 139/573 [28:52<1:30:08, 12.46s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  24%|‚ñà‚ñà‚ñç       | 139/573 [28:52<1:30:08, 12.46s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3850 examples [2:56:16,  2.79s/ examples]embeddings shape: torch.Size([32, 768, 1576])
Epoch 1:  24%|‚ñà‚ñà‚ñç       | 140/573 [29:20<1:30:44, 12.57s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  24%|‚ñà‚ñà‚ñç       | 140/573 [29:20<1:30:44, 12.57s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1672])
Epoch 1:  25%|‚ñà‚ñà‚ñç       | 141/573 [29:27<1:30:16, 12.54s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  25%|‚ñà‚ñà‚ñç       | 141/573 [29:27<1:30:16, 12.54s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1192])
Epoch 1:  25%|‚ñà‚ñà‚ñç       | 142/573 [29:34<1:29:46, 12.50s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  25%|‚ñà‚ñà‚ñç       | 142/573 [29:34<1:29:46, 12.50s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1642])
Epoch 1:  25%|‚ñà‚ñà‚ñç       | 143/573 [29:42<1:29:21, 12.47s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  25%|‚ñà‚ñà‚ñç       | 143/573 [29:42<1:29:21, 12.47s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1592])
Epoch 1:  25%|‚ñà‚ñà‚ñå       | 144/573 [29:50<1:28:52, 12.43s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  25%|‚ñà‚ñà‚ñå       | 144/573 [29:50<1:28:52, 12.43s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1593])
Epoch 1:  25%|‚ñà‚ñà‚ñå       | 145/573 [29:58<1:28:30, 12.41s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  25%|‚ñà‚ñà‚ñå       | 145/573 [29:58<1:28:30, 12.41s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1654])
Epoch 1:  25%|‚ñà‚ñà‚ñå       | 146/573 [30:07<1:28:05, 12.38s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  25%|‚ñà‚ñà‚ñå       | 146/573 [30:07<1:28:05, 12.38s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1643])
Epoch 1:  26%|‚ñà‚ñà‚ñå       | 147/573 [30:32<1:28:29, 12.46s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  26%|‚ñà‚ñà‚ñå       | 147/573 [30:32<1:28:29, 12.46s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4819 examples [2:58:43,  2.20s/ examples]embeddings shape: torch.Size([32, 768, 1663])
Epoch 1:  26%|‚ñà‚ñà‚ñå       | 148/573 [30:51<1:28:36, 12.51s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  26%|‚ñà‚ñà‚ñå       | 148/573 [30:51<1:28:36, 12.51s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3900 examples [2:58:16,  2.67s/ examples]embeddings shape: torch.Size([32, 768, 1599])
Epoch 1:  26%|‚ñà‚ñà‚ñå       | 149/573 [31:17<1:29:02, 12.60s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  26%|‚ñà‚ñà‚ñå       | 149/573 [31:17<1:29:02, 12.60s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1570])
Epoch 1:  26%|‚ñà‚ñà‚ñå       | 150/573 [31:23<1:28:32, 12.56s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  26%|‚ñà‚ñà‚ñå       | 150/573 [31:23<1:28:32, 12.56s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1574])
Epoch 1:  26%|‚ñà‚ñà‚ñã       | 151/573 [31:49<1:28:55, 12.64s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  26%|‚ñà‚ñà‚ñã       | 151/573 [31:49<1:28:55, 12.64s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4355 examples [3:00:24,  2.94s/ examples]embeddings shape: torch.Size([32, 768, 1557])
Epoch 1:  27%|‚ñà‚ñà‚ñã       | 152/573 [32:13<1:29:15, 12.72s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  27%|‚ñà‚ñà‚ñã       | 152/573 [32:13<1:29:15, 12.72s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1607])
Epoch 1:  27%|‚ñà‚ñà‚ñã       | 153/573 [32:19<1:28:43, 12.67s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  27%|‚ñà‚ñà‚ñã       | 153/573 [32:19<1:28:43, 12.67s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1600])
Epoch 1:  27%|‚ñà‚ñà‚ñã       | 154/573 [32:28<1:28:20, 12.65s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  27%|‚ñà‚ñà‚ñã       | 154/573 [32:28<1:28:20, 12.65s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1458])
Epoch 1:  27%|‚ñà‚ñà‚ñã       | 155/573 [32:36<1:27:57, 12.62s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  27%|‚ñà‚ñà‚ñã       | 155/573 [32:36<1:27:57, 12.62s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1678])
Epoch 1:  27%|‚ñà‚ñà‚ñã       | 156/573 [32:53<1:27:56, 12.65s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  27%|‚ñà‚ñà‚ñã       | 156/573 [32:53<1:27:56, 12.65s/it, loss=0.31, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] Generating train split: 4880 examples [3:01:13,  2.28s/ examples]embeddings shape: torch.Size([32, 768, 1640])
Epoch 1:  27%|‚ñà‚ñà‚ñã       | 157/573 [33:16<1:28:09, 12.72s/it, loss=0.31, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  27%|‚ñà‚ñà‚ñã       | 157/573 [33:16<1:28:09, 12.72s/it, loss=0.31, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1546])
Epoch 1:  28%|‚ñà‚ñà‚ñä       | 158/573 [33:40<1:28:27, 12.79s/it, loss=0.31, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  28%|‚ñà‚ñà‚ñä       | 158/573 [33:40<1:28:27, 12.79s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 3950 examples [3:00:46,  2.77s/ examples]embeddings shape: torch.Size([32, 768, 1623])
Epoch 1:  28%|‚ñà‚ñà‚ñä       | 159/573 [33:54<1:28:16, 12.79s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  28%|‚ñà‚ñà‚ñä       | 159/573 [33:54<1:28:16, 12.79s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1573])
Epoch 1:  28%|‚ñà‚ñà‚ñä       | 160/573 [34:01<1:27:49, 12.76s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  28%|‚ñà‚ñà‚ñä       | 160/573 [34:01<1:27:49, 12.76s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1586])
Epoch 1:  28%|‚ñà‚ñà‚ñä       | 161/573 [34:08<1:27:21, 12.72s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  28%|‚ñà‚ñà‚ñä       | 161/573 [34:08<1:27:21, 12.72s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1573])
Epoch 1:  28%|‚ñà‚ñà‚ñä       | 162/573 [34:13<1:26:50, 12.68s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  28%|‚ñà‚ñà‚ñä       | 162/573 [34:13<1:26:50, 12.68s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1410])
Epoch 1:  28%|‚ñà‚ñà‚ñä       | 163/573 [34:21<1:26:25, 12.65s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  28%|‚ñà‚ñà‚ñä       | 163/573 [34:21<1:26:25, 12.65s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1592])
Epoch 1:  29%|‚ñà‚ñà‚ñä       | 164/573 [34:39<1:26:26, 12.68s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  29%|‚ñà‚ñà‚ñä       | 164/573 [34:39<1:26:26, 12.68s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1694])
Epoch 1:  29%|‚ñà‚ñà‚ñâ       | 165/573 [35:18<1:27:19, 12.84s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  29%|‚ñà‚ñà‚ñâ       | 165/573 [35:18<1:27:19, 12.84s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4422 examples [3:03:43,  2.95s/ examples]Generating train split: 4941 examples [3:03:44,  2.34s/ examples]embeddings shape: torch.Size([32, 768, 1618])
Epoch 1:  29%|‚ñà‚ñà‚ñâ       | 166/573 [35:47<1:27:46, 12.94s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  29%|‚ñà‚ñà‚ñâ       | 166/573 [35:47<1:27:46, 12.94s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1597])
Epoch 1:  29%|‚ñà‚ñà‚ñâ       | 167/573 [36:16<1:28:10, 13.03s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  29%|‚ñà‚ñà‚ñâ       | 167/573 [36:16<1:28:10, 13.03s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4000 examples [3:03:17,  2.85s/ examples]embeddings shape: torch.Size([32, 768, 1625])
Epoch 1:  29%|‚ñà‚ñà‚ñâ       | 168/573 [36:26<1:27:50, 13.01s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  29%|‚ñà‚ñà‚ñâ       | 168/573 [36:26<1:27:50, 13.01s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1609])
Epoch 1:  29%|‚ñà‚ñà‚ñâ       | 169/573 [36:33<1:27:23, 12.98s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  29%|‚ñà‚ñà‚ñâ       | 169/573 [36:33<1:27:23, 12.98s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1563])
Epoch 1:  30%|‚ñà‚ñà‚ñâ       | 170/573 [36:41<1:26:57, 12.95s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  30%|‚ñà‚ñà‚ñâ       | 170/573 [36:41<1:26:57, 12.95s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1570])
Epoch 1:  30%|‚ñà‚ñà‚ñâ       | 171/573 [36:51<1:26:39, 12.93s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  30%|‚ñà‚ñà‚ñâ       | 171/573 [36:51<1:26:39, 12.93s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1594])
Epoch 1:  30%|‚ñà‚ñà‚ñà       | 172/573 [37:00<1:26:15, 12.91s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  30%|‚ñà‚ñà‚ñà       | 172/573 [37:00<1:26:15, 12.91s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1567])
Epoch 1:  30%|‚ñà‚ñà‚ñà       | 173/573 [37:07<1:25:49, 12.87s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  30%|‚ñà‚ñà‚ñà       | 173/573 [37:07<1:25:49, 12.87s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5002 examples [3:05:42,  2.21s/ examples]embeddings shape: torch.Size([32, 768, 1610])
Epoch 1:  30%|‚ñà‚ñà‚ñà       | 174/573 [37:43<1:26:29, 13.01s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  30%|‚ñà‚ñà‚ñà       | 174/573 [37:43<1:26:29, 13.01s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1622])
Epoch 1:  31%|‚ñà‚ñà‚ñà       | 175/573 [38:13<1:26:55, 13.10s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  31%|‚ñà‚ñà‚ñà       | 175/573 [38:13<1:26:55, 13.10s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4050 examples [3:05:16,  2.71s/ examples]embeddings shape: torch.Size([32, 768, 1573])
Epoch 1:  31%|‚ñà‚ñà‚ñà       | 176/573 [38:27<1:26:44, 13.11s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  31%|‚ñà‚ñà‚ñà       | 176/573 [38:27<1:26:44, 13.11s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1621])
Epoch 1:  31%|‚ñà‚ñà‚ñà       | 177/573 [38:59<1:27:14, 13.22s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  31%|‚ñà‚ñà‚ñà       | 177/573 [38:59<1:27:14, 13.22s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4489 examples [3:07:30,  3.08s/ examples]embeddings shape: torch.Size([32, 768, 1473])
Epoch 1:  31%|‚ñà‚ñà‚ñà       | 178/573 [39:22<1:27:22, 13.27s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  31%|‚ñà‚ñà‚ñà       | 178/573 [39:22<1:27:22, 13.27s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1623])
Epoch 1:  31%|‚ñà‚ñà‚ñà       | 179/573 [39:31<1:27:00, 13.25s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  31%|‚ñà‚ñà‚ñà       | 179/573 [39:31<1:27:00, 13.25s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1582])
Epoch 1:  31%|‚ñà‚ñà‚ñà‚ñè      | 180/573 [39:42<1:26:41, 13.23s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  31%|‚ñà‚ñà‚ñà‚ñè      | 180/573 [39:42<1:26:41, 13.23s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5063 examples [3:08:19,  2.32s/ examples]embeddings shape: torch.Size([32, 768, 1562])
Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 181/573 [40:20<1:27:21, 13.37s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 181/573 [40:20<1:27:21, 13.37s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1584])
Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 182/573 [40:44<1:27:32, 13.43s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 182/573 [40:44<1:27:32, 13.43s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4100 examples [3:07:57,  2.86s/ examples]embeddings shape: torch.Size([32, 768, 1569])
Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 183/573 [41:03<1:27:29, 13.46s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 183/573 [41:03<1:27:29, 13.46s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1587])
Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 184/573 [41:11<1:27:04, 13.43s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 184/573 [41:11<1:27:04, 13.43s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1588])
Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 185/573 [41:18<1:26:38, 13.40s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 185/573 [41:18<1:26:38, 13.40s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1658])
Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 186/573 [41:26<1:26:14, 13.37s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 186/573 [41:26<1:26:14, 13.37s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1632])
Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 187/573 [41:49<1:26:19, 13.42s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 187/573 [41:49<1:26:19, 13.42s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4556 examples [3:10:26,  2.94s/ examples]embeddings shape: torch.Size([32, 768, 1616])
Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 188/573 [42:15<1:26:32, 13.49s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 188/573 [42:15<1:26:32, 13.49s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.500, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1595])
Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 189/573 [42:40<1:26:42, 13.55s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.500, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 189/573 [42:40<1:26:42, 13.55s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5124 examples [3:10:51,  2.37s/ examples]embeddings shape: torch.Size([32, 768, 1572])
Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 190/573 [43:05<1:26:52, 13.61s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 190/573 [43:05<1:26:52, 13.61s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4150 examples [3:10:30,  2.92s/ examples]embeddings shape: torch.Size([32, 768, 1589])
Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 191/573 [43:36<1:27:13, 13.70s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 191/573 [43:36<1:27:13, 13.70s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1602])
Epoch 1:  34%|‚ñà‚ñà‚ñà‚ñé      | 192/573 [43:45<1:26:49, 13.67s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  34%|‚ñà‚ñà‚ñà‚ñé      | 192/573 [43:45<1:26:49, 13.67s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1685])
Epoch 1:  34%|‚ñà‚ñà‚ñà‚ñé      | 193/573 [43:55<1:26:28, 13.65s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  34%|‚ñà‚ñà‚ñà‚ñé      | 193/573 [43:55<1:26:28, 13.65s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1532])
Epoch 1:  34%|‚ñà‚ñà‚ñà‚ñç      | 194/573 [44:02<1:26:02, 13.62s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  34%|‚ñà‚ñà‚ñà‚ñç      | 194/573 [44:02<1:26:02, 13.62s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1445])
Epoch 1:  34%|‚ñà‚ñà‚ñà‚ñç      | 195/573 [44:09<1:25:36, 13.59s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  34%|‚ñà‚ñà‚ñà‚ñç      | 195/573 [44:09<1:25:36, 13.59s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1545])
Epoch 1:  34%|‚ñà‚ñà‚ñà‚ñç      | 196/573 [44:19<1:25:14, 13.57s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  34%|‚ñà‚ñà‚ñà‚ñç      | 196/573 [44:19<1:25:14, 13.57s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1655])
Epoch 1:  34%|‚ñà‚ñà‚ñà‚ñç      | 197/573 [44:52<1:25:38, 13.67s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  34%|‚ñà‚ñà‚ñà‚ñç      | 197/573 [44:52<1:25:38, 13.67s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5185 examples [3:12:53,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1414])
Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñç      | 198/573 [45:12<1:25:37, 13.70s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñç      | 198/573 [45:12<1:25:37, 13.70s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1569])
Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñç      | 199/573 [45:56<1:26:21, 13.85s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñç      | 199/573 [45:56<1:26:21, 13.85s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4200 examples [3:13:02,  2.95s/ examples]Generating train split: 4623 examples [3:14:30,  3.15s/ examples]embeddings shape: torch.Size([32, 768, 1588])
Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñç      | 200/573 [46:20<1:26:26, 13.90s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñç      | 200/573 [46:20<1:26:26, 13.90s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1371])
Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñå      | 201/573 [46:27<1:25:58, 13.87s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñå      | 201/573 [46:27<1:25:58, 13.87s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1567])
Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñå      | 202/573 [46:34<1:25:33, 13.84s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñå      | 202/573 [46:34<1:25:33, 13.84s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1572])
Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñå      | 203/573 [46:43<1:25:09, 13.81s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñå      | 203/573 [46:43<1:25:09, 13.81s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1593])
Epoch 1:  36%|‚ñà‚ñà‚ñà‚ñå      | 204/573 [46:52<1:24:48, 13.79s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  36%|‚ñà‚ñà‚ñà‚ñå      | 204/573 [46:52<1:24:48, 13.79s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1366])
Epoch 1:  36%|‚ñà‚ñà‚ñà‚ñå      | 205/573 [47:23<1:25:04, 13.87s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  36%|‚ñà‚ñà‚ñà‚ñå      | 205/573 [47:23<1:25:04, 13.87s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5246 examples [3:15:28,  2.34s/ examples]embeddings shape: torch.Size([32, 768, 1564])
Epoch 1:  36%|‚ñà‚ñà‚ñà‚ñå      | 206/573 [47:34<1:24:45, 13.86s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  36%|‚ñà‚ñà‚ñà‚ñå      | 206/573 [47:34<1:24:45, 13.86s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1573])
Epoch 1:  36%|‚ñà‚ñà‚ñà‚ñå      | 207/573 [47:50<1:24:35, 13.87s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  36%|‚ñà‚ñà‚ñà‚ñå      | 207/573 [47:50<1:24:35, 13.87s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] Generating train split: 4250 examples [3:15:11,  2.84s/ examples]embeddings shape: torch.Size([32, 768, 1587])
Epoch 1:  36%|‚ñà‚ñà‚ñà‚ñã      | 208/573 [48:17<1:24:45, 13.93s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  36%|‚ñà‚ñà‚ñà‚ñã      | 208/573 [48:17<1:24:45, 13.93s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1561])
Epoch 1:  36%|‚ñà‚ñà‚ñà‚ñã      | 209/573 [48:25<1:24:20, 13.90s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  36%|‚ñà‚ñà‚ñà‚ñã      | 209/573 [48:25<1:24:20, 13.90s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1603])
Epoch 1:  37%|‚ñà‚ñà‚ñà‚ñã      | 210/573 [48:42<1:24:12, 13.92s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  37%|‚ñà‚ñà‚ñà‚ñã      | 210/573 [48:42<1:24:12, 13.92s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4690 examples [3:17:26,  3.00s/ examples]embeddings shape: torch.Size([32, 768, 1659])
Epoch 1:  37%|‚ñà‚ñà‚ñà‚ñã      | 211/573 [49:15<1:24:30, 14.01s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  37%|‚ñà‚ñà‚ñà‚ñã      | 211/573 [49:15<1:24:30, 14.01s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1585])
Epoch 1:  37%|‚ñà‚ñà‚ñà‚ñã      | 212/573 [49:23<1:24:06, 13.98s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  37%|‚ñà‚ñà‚ñà‚ñã      | 212/573 [49:23<1:24:06, 13.98s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1545])
Epoch 1:  37%|‚ñà‚ñà‚ñà‚ñã      | 213/573 [49:41<1:23:58, 14.00s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  37%|‚ñà‚ñà‚ñà‚ñã      | 213/573 [49:41<1:23:58, 14.00s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5307 examples [3:17:58,  2.38s/ examples]embeddings shape: torch.Size([32, 768, 1632])
Epoch 1:  37%|‚ñà‚ñà‚ñà‚ñã      | 214/573 [50:01<1:23:55, 14.03s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  37%|‚ñà‚ñà‚ñà‚ñã      | 214/573 [50:01<1:23:55, 14.03s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1564])
Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 215/573 [50:10<1:23:32, 14.00s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 215/573 [50:10<1:23:32, 14.00s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4300 examples [3:17:40,  2.89s/ examples]embeddings shape: torch.Size([32, 768, 1566])
Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 216/573 [50:44<1:23:51, 14.09s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 216/573 [50:44<1:23:51, 14.09s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1178])
Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 217/573 [50:51<1:23:26, 14.06s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 217/573 [50:51<1:23:26, 14.06s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1627])
Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 218/573 [50:59<1:23:02, 14.04s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 218/573 [50:59<1:23:02, 14.04s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1577])
Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 219/573 [51:16<1:22:53, 14.05s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 219/573 [51:16<1:22:53, 14.05s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4757 examples [3:19:55,  2.76s/ examples]embeddings shape: torch.Size([32, 768, 1631])
Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 220/573 [51:44<1:23:01, 14.11s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 220/573 [51:44<1:23:01, 14.11s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1594])
Epoch 1:  39%|‚ñà‚ñà‚ñà‚ñä      | 221/573 [51:52<1:22:37, 14.08s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  39%|‚ñà‚ñà‚ñà‚ñä      | 221/573 [51:52<1:22:37, 14.08s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5368 examples [3:20:24,  2.38s/ examples]embeddings shape: torch.Size([32, 768, 1597])
Epoch 1:  39%|‚ñà‚ñà‚ñà‚ñä      | 222/573 [52:26<1:22:55, 14.17s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  39%|‚ñà‚ñà‚ñà‚ñä      | 222/573 [52:26<1:22:55, 14.17s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1621])
Epoch 1:  39%|‚ñà‚ñà‚ñà‚ñâ      | 223/573 [52:36<1:22:33, 14.15s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  39%|‚ñà‚ñà‚ñà‚ñâ      | 223/573 [52:36<1:22:33, 14.15s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4350 examples [3:20:06,  2.90s/ examples]embeddings shape: torch.Size([32, 768, 1566])
Epoch 1:  39%|‚ñà‚ñà‚ñà‚ñâ      | 224/573 [53:08<1:22:48, 14.24s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  39%|‚ñà‚ñà‚ñà‚ñâ      | 224/573 [53:08<1:22:48, 14.24s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1566])
Epoch 1:  39%|‚ñà‚ñà‚ñà‚ñâ      | 225/573 [53:17<1:22:25, 14.21s/it, loss=0.343, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  39%|‚ñà‚ñà‚ñà‚ñâ      | 225/573 [53:17<1:22:25, 14.21s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1597])
Epoch 1:  39%|‚ñà‚ñà‚ñà‚ñâ      | 226/573 [53:27<1:22:05, 14.19s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  39%|‚ñà‚ñà‚ñà‚ñâ      | 226/573 [53:27<1:22:05, 14.19s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1575])
Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñâ      | 227/573 [53:36<1:21:42, 14.17s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñâ      | 227/573 [53:36<1:21:42, 14.17s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1552])
Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñâ      | 228/573 [53:46<1:21:21, 14.15s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñâ      | 228/573 [53:46<1:21:21, 14.15s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4824 examples [3:22:31,  2.64s/ examples]embeddings shape: torch.Size([32, 768, 1622])
Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñâ      | 229/573 [54:29<1:21:52, 14.28s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñâ      | 229/573 [54:29<1:21:52, 14.28s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5429 examples [3:22:53,  2.40s/ examples]embeddings shape: torch.Size([32, 768, 1341])
Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñà      | 230/573 [54:56<1:21:56, 14.33s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñà      | 230/573 [54:56<1:21:56, 14.33s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1557])
Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñà      | 231/573 [55:04<1:21:32, 14.30s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñà      | 231/573 [55:04<1:21:32, 14.30s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1568])
Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñà      | 232/573 [55:31<1:21:36, 14.36s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñà      | 232/573 [55:31<1:21:36, 14.36s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.913, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] Generating train split: 4400 examples [3:22:38,  2.94s/ examples]embeddings shape: torch.Size([32, 768, 1563])
Epoch 1:  41%|‚ñà‚ñà‚ñà‚ñà      | 233/573 [55:45<1:21:21, 14.36s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.913, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  41%|‚ñà‚ñà‚ñà‚ñà      | 233/573 [55:45<1:21:21, 14.36s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1307])
Epoch 1:  41%|‚ñà‚ñà‚ñà‚ñà      | 234/573 [55:52<1:20:57, 14.33s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  41%|‚ñà‚ñà‚ñà‚ñà      | 234/573 [55:52<1:20:57, 14.33s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1565])
Epoch 1:  41%|‚ñà‚ñà‚ñà‚ñà      | 235/573 [56:00<1:20:33, 14.30s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  41%|‚ñà‚ñà‚ñà‚ñà      | 235/573 [56:00<1:20:33, 14.30s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1604])
Epoch 1:  41%|‚ñà‚ñà‚ñà‚ñà      | 236/573 [56:10<1:20:13, 14.28s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  41%|‚ñà‚ñà‚ñà‚ñà      | 236/573 [56:10<1:20:13, 14.28s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1602])
Epoch 1:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 237/573 [56:19<1:19:50, 14.26s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 237/573 [56:19<1:19:50, 14.26s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1588])
Epoch 1:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 238/573 [56:50<1:20:00, 14.33s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 238/573 [56:50<1:20:00, 14.33s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4891 examples [3:25:12,  2.56s/ examples]Generating train split: 5490 examples [3:25:17,  2.39s/ examples]embeddings shape: torch.Size([32, 768, 1072])
Epoch 1:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 239/573 [57:19<1:20:06, 14.39s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 239/573 [57:19<1:20:06, 14.39s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1567])
Epoch 1:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 240/573 [57:36<1:19:55, 14.40s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 240/573 [57:36<1:19:55, 14.40s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4450 examples [3:24:51,  2.85s/ examples]embeddings shape: torch.Size([32, 768, 1594])
Epoch 1:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 241/573 [57:57<1:19:50, 14.43s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 241/573 [57:57<1:19:50, 14.43s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1540])
Epoch 1:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 242/573 [58:06<1:19:28, 14.41s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 242/573 [58:06<1:19:28, 14.41s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1461])
Epoch 1:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 243/573 [58:12<1:19:03, 14.37s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 243/573 [58:12<1:19:03, 14.37s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1423])
Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 244/573 [58:20<1:18:39, 14.35s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 244/573 [58:20<1:18:39, 14.35s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1555])
Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 245/573 [58:28<1:18:16, 14.32s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 245/573 [58:28<1:18:16, 14.32s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1606])
Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 246/573 [58:38<1:17:56, 14.30s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 246/573 [58:38<1:17:56, 14.30s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1561])
Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 247/573 [58:59<1:17:51, 14.33s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 247/573 [58:59<1:17:51, 14.33s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] Generating train split: 5551 examples [3:27:18,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1599])
Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 248/573 [59:28<1:17:56, 14.39s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 248/573 [59:28<1:17:56, 14.39s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1551])
Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 249/573 [1:00:03<1:18:09, 14.47s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 249/573 [1:00:03<1:18:09, 14.47s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4958 examples [3:28:36,  2.71s/ examples]Generating train split: 4500 examples [3:27:26,  2.93s/ examples]embeddings shape: torch.Size([32, 768, 1579])
Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 250/573 [1:00:30<1:18:10, 14.52s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 250/573 [1:00:30<1:18:10, 14.52s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1551])
Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 251/573 [1:00:38<1:17:47, 14.50s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 251/573 [1:00:38<1:17:47, 14.50s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1514])
Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 252/573 [1:00:45<1:17:23, 14.47s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 252/573 [1:00:45<1:17:23, 14.47s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1634])
Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 253/573 [1:00:52<1:16:59, 14.44s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 253/573 [1:00:52<1:16:59, 14.44s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1573])
Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 254/573 [1:00:59<1:16:36, 14.41s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 254/573 [1:00:59<1:16:36, 14.41s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1561])
Epoch 1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 255/573 [1:01:08<1:16:14, 14.39s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 255/573 [1:01:08<1:16:14, 14.39s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5612 examples [3:29:39,  2.28s/ examples]embeddings shape: torch.Size([32, 768, 1609])
Epoch 1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 256/573 [1:01:41<1:16:23, 14.46s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 256/573 [1:01:41<1:16:23, 14.46s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1535])
Epoch 1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 257/573 [1:01:50<1:16:01, 14.44s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 257/573 [1:01:50<1:16:01, 14.44s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4550 examples [3:29:20,  2.73s/ examples]embeddings shape: torch.Size([32, 768, 1581])
Epoch 1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 258/573 [1:02:22<1:16:09, 14.51s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 258/573 [1:02:22<1:16:09, 14.51s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1647])
Epoch 1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 259/573 [1:02:45<1:16:05, 14.54s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 259/573 [1:02:45<1:16:05, 14.54s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5025 examples [3:31:15,  2.61s/ examples]embeddings shape: torch.Size([32, 768, 1549])
Epoch 1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 260/573 [1:03:05<1:15:56, 14.56s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 260/573 [1:03:05<1:15:56, 14.56s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1564])
Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 261/573 [1:03:14<1:15:35, 14.54s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 261/573 [1:03:14<1:15:35, 14.54s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1608])
Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 262/573 [1:03:24<1:15:15, 14.52s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 262/573 [1:03:24<1:15:15, 14.52s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1558])
Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 263/573 [1:03:31<1:14:52, 14.49s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 263/573 [1:03:31<1:14:52, 14.49s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1566])
Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 264/573 [1:03:58<1:14:52, 14.54s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 264/573 [1:03:58<1:14:52, 14.54s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.812, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5673 examples [3:32:03,  2.31s/ examples]embeddings shape: torch.Size([32, 768, 1619])
Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 265/573 [1:04:12<1:14:37, 14.54s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.812, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 265/573 [1:04:12<1:14:37, 14.54s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1574])
Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 266/573 [1:04:30<1:14:27, 14.55s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 266/573 [1:04:30<1:14:27, 14.55s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4600 examples [3:31:45,  2.79s/ examples]embeddings shape: torch.Size([32, 768, 1618])
Epoch 1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 267/573 [1:04:52<1:14:21, 14.58s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 267/573 [1:04:52<1:14:21, 14.58s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1667])
Epoch 1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 268/573 [1:05:01<1:13:59, 14.56s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 268/573 [1:05:01<1:13:59, 14.56s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1612])
Epoch 1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 269/573 [1:05:09<1:13:38, 14.53s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 269/573 [1:05:09<1:13:38, 14.53s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1618])
Epoch 1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 270/573 [1:05:16<1:13:14, 14.50s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 270/573 [1:05:16<1:13:14, 14.50s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1565])
Epoch 1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 271/573 [1:05:50<1:13:22, 14.58s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 271/573 [1:05:50<1:13:22, 14.58s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5092 examples [3:34:11,  2.61s/ examples]embeddings shape: torch.Size([32, 768, 1617])
Epoch 1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 272/573 [1:06:03<1:13:06, 14.57s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 272/573 [1:06:03<1:13:06, 14.57s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5734 examples [3:34:32,  2.34s/ examples]embeddings shape: torch.Size([32, 768, 1620])
Epoch 1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 273/573 [1:06:34<1:13:09, 14.63s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 273/573 [1:06:34<1:13:09, 14.63s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1299])
Epoch 1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 274/573 [1:06:42<1:12:48, 14.61s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 274/573 [1:06:42<1:12:48, 14.61s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4650 examples [3:34:11,  2.82s/ examples]embeddings shape: torch.Size([32, 768, 1575])
Epoch 1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 275/573 [1:07:12<1:12:49, 14.66s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 275/573 [1:07:12<1:12:49, 14.66s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1593])
Epoch 1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 276/573 [1:07:20<1:12:28, 14.64s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 276/573 [1:07:20<1:12:28, 14.64s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1627])
Epoch 1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 277/573 [1:07:27<1:12:04, 14.61s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 277/573 [1:07:27<1:12:04, 14.61s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.910, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1659])
Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 278/573 [1:07:38<1:11:46, 14.60s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.910, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 278/573 [1:07:38<1:11:46, 14.60s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1554])
Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 279/573 [1:07:48<1:11:27, 14.58s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 279/573 [1:07:48<1:11:27, 14.58s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1236])
Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 280/573 [1:08:13<1:11:23, 14.62s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 280/573 [1:08:13<1:11:23, 14.62s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5159 examples [3:36:28,  2.44s/ examples]embeddings shape: torch.Size([32, 768, 1619])
Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 281/573 [1:08:36<1:11:17, 14.65s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 281/573 [1:08:36<1:11:17, 14.65s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5795 examples [3:36:49,  2.32s/ examples]embeddings shape: torch.Size([32, 768, 1620])
Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 282/573 [1:08:56<1:11:08, 14.67s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 282/573 [1:08:56<1:11:08, 14.67s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1549])
Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 283/573 [1:09:24<1:11:07, 14.72s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 283/573 [1:09:24<1:11:07, 14.72s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4700 examples [3:36:25,  2.78s/ examples]embeddings shape: torch.Size([32, 768, 1650])
Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 284/573 [1:09:34<1:10:48, 14.70s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 284/573 [1:09:34<1:10:48, 14.70s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1561])
Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 285/573 [1:09:40<1:10:24, 14.67s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 285/573 [1:09:40<1:10:24, 14.67s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1547])
Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 286/573 [1:09:47<1:10:01, 14.64s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 286/573 [1:09:47<1:10:01, 14.64s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1580])
Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 287/573 [1:09:54<1:09:40, 14.62s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 287/573 [1:09:54<1:09:40, 14.62s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1624])
Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 288/573 [1:10:02<1:09:18, 14.59s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 288/573 [1:10:02<1:09:18, 14.59s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1596])
Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 289/573 [1:10:10<1:08:57, 14.57s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 289/573 [1:10:10<1:08:57, 14.57s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1419])
Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 290/573 [1:10:37<1:08:54, 14.61s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 290/573 [1:10:37<1:08:54, 14.61s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5226 examples [3:39:23,  2.50s/ examples]Generating train split: 5856 examples [3:39:12,  2.32s/ examples]embeddings shape: torch.Size([32, 768, 1598])
Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 291/573 [1:11:13<1:09:01, 14.69s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 291/573 [1:11:13<1:09:01, 14.69s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4750 examples [3:38:41,  2.76s/ examples]embeddings shape: torch.Size([32, 768, 1589])
Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 292/573 [1:11:44<1:09:02, 14.74s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 292/573 [1:11:44<1:09:02, 14.74s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1575])
Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 293/573 [1:11:53<1:08:41, 14.72s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 293/573 [1:11:53<1:08:41, 14.72s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1566])
Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 294/573 [1:11:59<1:08:19, 14.69s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 294/573 [1:11:59<1:08:19, 14.69s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1631])
Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 295/573 [1:12:09<1:08:00, 14.68s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 295/573 [1:12:09<1:08:00, 14.68s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1582])
Epoch 1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 296/573 [1:12:18<1:07:40, 14.66s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 296/573 [1:12:18<1:07:40, 14.66s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1597])
Epoch 1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 297/573 [1:12:27<1:07:20, 14.64s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 297/573 [1:12:27<1:07:20, 14.64s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1598])
Epoch 1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 298/573 [1:12:35<1:06:59, 14.62s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 298/573 [1:12:35<1:06:59, 14.62s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1605])
Epoch 1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 299/573 [1:13:00<1:06:53, 14.65s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 299/573 [1:13:00<1:06:53, 14.65s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5917 examples [3:41:11,  2.21s/ examples]embeddings shape: torch.Size([32, 768, 1589])
Epoch 1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 300/573 [1:13:42<1:07:04, 14.74s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 300/573 [1:13:42<1:07:04, 14.74s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4800 examples [3:40:46,  2.68s/ examples]Generating train split: 5293 examples [3:42:25,  2.56s/ examples]embeddings shape: torch.Size([32, 768, 1598])
Epoch 1:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 301/573 [1:14:12<1:07:03, 14.79s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 301/573 [1:14:12<1:07:03, 14.79s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1578])
Epoch 1:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 302/573 [1:14:20<1:06:42, 14.77s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 302/573 [1:14:20<1:06:42, 14.77s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1585])
Epoch 1:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 303/573 [1:14:27<1:06:21, 14.74s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 303/573 [1:14:27<1:06:21, 14.74s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1561])
Epoch 1:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 304/573 [1:14:34<1:05:59, 14.72s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 304/573 [1:14:34<1:05:59, 14.72s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1635])
Epoch 1:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 305/573 [1:14:43<1:05:39, 14.70s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 305/573 [1:14:43<1:05:39, 14.70s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1577])
Epoch 1:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 306/573 [1:14:52<1:05:19, 14.68s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 306/573 [1:14:52<1:05:19, 14.68s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1530])
Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 307/573 [1:15:03<1:05:01, 14.67s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 307/573 [1:15:03<1:05:01, 14.67s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5978 examples [3:43:30,  2.23s/ examples]embeddings shape: torch.Size([32, 768, 1644])
Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 308/573 [1:15:43<1:05:09, 14.75s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 308/573 [1:15:43<1:05:09, 14.75s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4850 examples [3:43:01,  2.69s/ examples]embeddings shape: torch.Size([32, 768, 1617])
Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 309/573 [1:16:06<1:05:01, 14.78s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 309/573 [1:16:06<1:05:01, 14.78s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1552])
Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 310/573 [1:16:36<1:04:59, 14.83s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 310/573 [1:16:36<1:04:59, 14.83s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5360 examples [3:44:57,  2.47s/ examples]embeddings shape: torch.Size([32, 768, 1614])
Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 311/573 [1:16:51<1:04:45, 14.83s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 311/573 [1:16:51<1:04:45, 14.83s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1594])
Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 312/573 [1:16:59<1:04:24, 14.81s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 312/573 [1:16:59<1:04:24, 14.81s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1575])
Epoch 1:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 313/573 [1:17:06<1:04:03, 14.78s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 313/573 [1:17:06<1:04:03, 14.78s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1239])
Epoch 1:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 314/573 [1:17:14<1:03:42, 14.76s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 314/573 [1:17:14<1:03:42, 14.76s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1557])
Epoch 1:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 315/573 [1:17:34<1:03:32, 14.78s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 315/573 [1:17:34<1:03:32, 14.78s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6039 examples [3:45:50,  2.25s/ examples]embeddings shape: torch.Size([32, 768, 1546])
Epoch 1:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 316/573 [1:18:05<1:03:30, 14.83s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 316/573 [1:18:05<1:03:30, 14.83s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4900 examples [3:45:20,  2.71s/ examples]embeddings shape: torch.Size([32, 768, 1586])
Epoch 1:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 317/573 [1:18:26<1:03:20, 14.85s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 317/573 [1:18:26<1:03:20, 14.85s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1634])
Epoch 1:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 318/573 [1:18:35<1:03:01, 14.83s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 318/573 [1:18:35<1:03:01, 14.83s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1604])
Epoch 1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 319/573 [1:18:47<1:02:44, 14.82s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 319/573 [1:18:47<1:02:44, 14.82s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1570])
Epoch 1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 320/573 [1:19:16<1:02:40, 14.86s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 320/573 [1:19:16<1:02:40, 14.86s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5427 examples [3:47:35,  2.44s/ examples]embeddings shape: torch.Size([32, 768, 1613])
Epoch 1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 321/573 [1:19:27<1:02:22, 14.85s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 321/573 [1:19:27<1:02:22, 14.85s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1568])
Epoch 1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 322/573 [1:19:36<1:02:03, 14.83s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 322/573 [1:19:36<1:02:03, 14.83s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1585])
Epoch 1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 323/573 [1:19:43<1:01:42, 14.81s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 323/573 [1:19:43<1:01:42, 14.81s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1573])
Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 324/573 [1:20:15<1:01:40, 14.86s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 324/573 [1:20:15<1:01:40, 14.86s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6100 examples [3:48:17,  2.30s/ examples]embeddings shape: torch.Size([32, 768, 1573])
Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 325/573 [1:20:48<1:01:39, 14.92s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 325/573 [1:20:48<1:01:39, 14.92s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 4950 examples [3:47:48,  2.79s/ examples]embeddings shape: torch.Size([32, 768, 1540])
Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 326/573 [1:20:56<1:01:19, 14.90s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 326/573 [1:20:56<1:01:19, 14.90s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1589])
Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 327/573 [1:21:04<1:00:59, 14.88s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 327/573 [1:21:04<1:00:59, 14.88s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1645])
Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 328/573 [1:21:12<1:00:39, 14.85s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 328/573 [1:21:12<1:00:39, 14.85s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1592])
Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 329/573 [1:21:20<1:00:19, 14.84s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 329/573 [1:21:20<1:00:19, 14.84s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1394])
Epoch 1:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 330/573 [1:21:28<59:59, 14.81s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]  Epoch 1:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 330/573 [1:21:28<59:59, 14.81s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1561])
Epoch 1:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 331/573 [1:22:05<1:00:00, 14.88s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 331/573 [1:22:05<1:00:00, 14.88s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5494 examples [3:50:20,  2.44s/ examples]embeddings shape: torch.Size([32, 768, 1595])
Epoch 1:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 332/573 [1:22:18<59:44, 14.87s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]  Epoch 1:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 332/573 [1:22:18<59:44, 14.87s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6161 examples [3:50:40,  2.31s/ examples]embeddings shape: torch.Size([32, 768, 1670])
Epoch 1:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 333/573 [1:22:49<59:41, 14.92s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 333/573 [1:22:49<59:41, 14.92s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5000 examples [3:50:09,  2.80s/ examples]embeddings shape: torch.Size([32, 768, 1635])
Epoch 1:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 334/573 [1:23:14<59:33, 14.95s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 334/573 [1:23:14<59:33, 14.95s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1595])
Epoch 1:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 335/573 [1:23:22<59:13, 14.93s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 335/573 [1:23:22<59:13, 14.93s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1464])
Epoch 1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 336/573 [1:23:29<58:53, 14.91s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 336/573 [1:23:29<58:53, 14.91s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1590])
Epoch 1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 337/573 [1:23:37<58:33, 14.89s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 337/573 [1:23:37<58:33, 14.89s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1390])
Epoch 1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 338/573 [1:23:44<58:13, 14.87s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 338/573 [1:23:44<58:13, 14.87s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1559])
Epoch 1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 339/573 [1:23:54<57:55, 14.85s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 339/573 [1:23:54<57:55, 14.85s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1590])
Epoch 1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 340/573 [1:24:05<57:37, 14.84s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 340/573 [1:24:05<57:37, 14.84s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1568])
Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 341/573 [1:24:42<57:38, 14.91s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 341/573 [1:24:42<57:38, 14.91s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6222 examples [3:52:46,  2.24s/ examples]embeddings shape: torch.Size([32, 768, 1664])
Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 342/573 [1:25:15<57:35, 14.96s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 342/573 [1:25:15<57:35, 14.96s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5050 examples [3:52:29,  2.80s/ examples]Generating train split: 5561 examples [3:53:46,  2.64s/ examples]embeddings shape: torch.Size([32, 768, 1628])
Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 343/573 [1:25:38<57:25, 14.98s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 343/573 [1:25:38<57:25, 14.98s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1584])
Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 344/573 [1:25:47<57:06, 14.96s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 344/573 [1:25:47<57:06, 14.96s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1585])
Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 345/573 [1:25:54<56:46, 14.94s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 345/573 [1:25:54<56:46, 14.94s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1623])
Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 346/573 [1:26:03<56:27, 14.92s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 346/573 [1:26:03<56:27, 14.92s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1588])
Epoch 1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 347/573 [1:26:12<56:08, 14.91s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 347/573 [1:26:12<56:08, 14.91s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1637])
Epoch 1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 348/573 [1:26:21<55:49, 14.89s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 348/573 [1:26:21<55:49, 14.89s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1628])
Epoch 1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 349/573 [1:26:46<55:41, 14.92s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 349/573 [1:26:46<55:41, 14.92s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6283 examples [3:54:50,  2.18s/ examples]embeddings shape: torch.Size([32, 768, 1687])
Epoch 1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 350/573 [1:27:07<55:30, 14.93s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 350/573 [1:27:07<55:30, 14.93s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5100 examples [3:54:22,  2.64s/ examples]embeddings shape: torch.Size([32, 768, 1608])
Epoch 1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 351/573 [1:27:27<55:18, 14.95s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 351/573 [1:27:27<55:18, 14.95s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1585])
Epoch 1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 352/573 [1:27:34<54:58, 14.93s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 352/573 [1:27:34<54:58, 14.93s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1372])
Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 353/573 [1:27:57<54:49, 14.95s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 353/573 [1:27:57<54:49, 14.95s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5628 examples [3:56:26,  2.56s/ examples]embeddings shape: torch.Size([32, 768, 1671])
Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 354/573 [1:28:17<54:37, 14.96s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 354/573 [1:28:17<54:37, 14.96s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1644])
Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 355/573 [1:28:24<54:17, 14.94s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 355/573 [1:28:24<54:17, 14.94s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1603])
Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 356/573 [1:28:33<53:58, 14.92s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 356/573 [1:28:33<53:58, 14.92s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1624])
Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 357/573 [1:28:44<53:41, 14.92s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 357/573 [1:28:44<53:41, 14.92s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6344 examples [3:57:10,  2.21s/ examples]embeddings shape: torch.Size([32, 768, 1501])
Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 358/573 [1:29:13<53:34, 14.95s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 358/573 [1:29:13<53:34, 14.95s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1597])
Epoch 1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 359/573 [1:29:39<53:26, 14.99s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 359/573 [1:29:39<53:26, 14.99s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5150 examples [3:56:43,  2.70s/ examples]embeddings shape: torch.Size([32, 768, 1575])
Epoch 1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 360/573 [1:29:51<53:09, 14.98s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 360/573 [1:29:51<53:09, 14.98s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1620])
Epoch 1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 361/573 [1:29:58<52:50, 14.95s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 361/573 [1:29:58<52:50, 14.95s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1670])
Epoch 1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 362/573 [1:30:07<52:31, 14.94s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 362/573 [1:30:07<52:31, 14.94s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1595])
Epoch 1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 363/573 [1:30:14<52:12, 14.92s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 363/573 [1:30:14<52:12, 14.92s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1600])
Epoch 1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 364/573 [1:30:23<51:53, 14.90s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 364/573 [1:30:23<51:53, 14.90s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1582])
Epoch 1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 365/573 [1:30:32<51:35, 14.88s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 365/573 [1:30:32<51:35, 14.88s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1614])
Epoch 1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 366/573 [1:31:03<51:30, 14.93s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 366/573 [1:31:03<51:30, 14.93s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6405 examples [3:59:30,  2.23s/ examples]embeddings shape: torch.Size([32, 768, 1649])
Epoch 1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 367/573 [1:31:32<51:23, 14.97s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 367/573 [1:31:32<51:23, 14.97s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5695 examples [4:00:04,  2.77s/ examples]Generating train split: 5200 examples [3:59:04,  2.73s/ examples]embeddings shape: torch.Size([32, 768, 1593])
Epoch 1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 368/573 [1:32:05<51:18, 15.02s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 368/573 [1:32:05<51:18, 15.02s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1653])
Epoch 1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 369/573 [1:32:13<50:59, 15.00s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 369/573 [1:32:13<50:59, 15.00s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1456])
Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 370/573 [1:32:22<50:40, 14.98s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 370/573 [1:32:22<50:40, 14.98s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1687])
Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 371/573 [1:32:30<50:21, 14.96s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 371/573 [1:32:30<50:21, 14.96s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1537])
Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 372/573 [1:32:36<50:02, 14.94s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 372/573 [1:32:36<50:02, 14.94s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.906, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1601])
Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 373/573 [1:32:44<49:43, 14.92s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.906, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 373/573 [1:32:44<49:43, 14.92s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1581])
Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 374/573 [1:32:52<49:24, 14.90s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 374/573 [1:32:52<49:24, 14.90s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1194])
Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 375/573 [1:33:03<49:07, 14.89s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 375/573 [1:33:03<49:07, 14.89s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6466 examples [4:01:26,  2.14s/ examples]embeddings shape: torch.Size([32, 768, 1306])
Epoch 1:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 376/573 [1:33:31<49:00, 14.93s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 376/573 [1:33:31<49:00, 14.93s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5250 examples [4:00:56,  2.58s/ examples]embeddings shape: torch.Size([32, 768, 1590])
Epoch 1:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 377/573 [1:34:01<48:52, 14.96s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 377/573 [1:34:01<48:52, 14.96s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1594])
Epoch 1:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 378/573 [1:34:22<48:41, 14.98s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 378/573 [1:34:22<48:41, 14.98s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] Generating train split: 5762 examples [4:02:52,  2.69s/ examples]embeddings shape: torch.Size([32, 768, 1645])
Epoch 1:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 379/573 [1:34:42<48:28, 14.99s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 379/573 [1:34:42<48:28, 14.99s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1551])
Epoch 1:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 380/573 [1:34:49<48:09, 14.97s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 380/573 [1:34:49<48:09, 14.97s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1399])
Epoch 1:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 381/573 [1:34:57<47:50, 14.95s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 381/573 [1:34:57<47:50, 14.95s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1581])
Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 382/573 [1:35:05<47:32, 14.94s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 382/573 [1:35:05<47:32, 14.94s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1554])
Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 383/573 [1:35:13<47:14, 14.92s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 383/573 [1:35:13<47:14, 14.92s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6527 examples [4:03:43,  2.17s/ examples]embeddings shape: torch.Size([32, 768, 1619])
Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 384/573 [1:35:45<47:07, 14.96s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 384/573 [1:35:45<47:07, 14.96s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5300 examples [4:03:10,  2.61s/ examples]embeddings shape: torch.Size([32, 768, 1598])
Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 385/573 [1:36:13<46:59, 15.00s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 385/573 [1:36:13<46:59, 15.00s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1648])
Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 386/573 [1:36:22<46:41, 14.98s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 386/573 [1:36:22<46:41, 14.98s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.812, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1468])
Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 387/573 [1:36:30<46:22, 14.96s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.812, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 387/573 [1:36:30<46:22, 14.96s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1596])
Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 388/573 [1:36:50<46:10, 14.97s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 388/573 [1:36:50<46:10, 14.97s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5829 examples [4:05:20,  2.55s/ examples]embeddings shape: torch.Size([32, 768, 1179])
Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 389/573 [1:37:09<45:57, 14.99s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 389/573 [1:37:09<45:57, 14.99s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1626])
Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 390/573 [1:37:19<45:40, 14.97s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 390/573 [1:37:19<45:40, 14.97s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1660])
Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 391/573 [1:37:27<45:21, 14.95s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 391/573 [1:37:27<45:21, 14.95s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1608])
Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 392/573 [1:37:51<45:10, 14.98s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 392/573 [1:37:51<45:10, 14.98s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] Generating train split: 6588 examples [4:06:11,  2.25s/ examples]embeddings shape: torch.Size([32, 768, 1584])
Epoch 1:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 393/573 [1:38:26<45:05, 15.03s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 393/573 [1:38:26<45:05, 15.03s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5350 examples [4:05:28,  2.66s/ examples]embeddings shape: torch.Size([32, 768, 1531])
Epoch 1:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 394/573 [1:38:37<44:48, 15.02s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 394/573 [1:38:37<44:48, 15.02s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.812, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1572])
Epoch 1:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 395/573 [1:38:45<44:30, 15.00s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.812, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 395/573 [1:38:45<44:30, 15.00s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1585])
Epoch 1:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 396/573 [1:39:02<44:15, 15.01s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 396/573 [1:39:02<44:15, 15.01s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5896 examples [4:07:31,  2.37s/ examples]embeddings shape: torch.Size([32, 768, 1566])
Epoch 1:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 397/573 [1:39:21<44:03, 15.02s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 397/573 [1:39:21<44:03, 15.02s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1605])
Epoch 1:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 398/573 [1:39:29<43:44, 15.00s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 398/573 [1:39:29<43:44, 15.00s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.799, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1565])
Epoch 1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 399/573 [1:39:38<43:27, 14.98s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.799, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 399/573 [1:39:38<43:27, 14.98s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1583])
Epoch 1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 400/573 [1:39:45<43:08, 14.96s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 400/573 [1:39:45<43:08, 14.96s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1598])
Epoch 1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 401/573 [1:39:54<42:51, 14.95s/it, loss=0.35, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 401/573 [1:39:54<42:51, 14.95s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1518])
Epoch 1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 402/573 [1:40:29<42:44, 15.00s/it, loss=0.353, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 402/573 [1:40:29<42:44, 15.00s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.688, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6649 examples [4:08:35,  2.28s/ examples]Generating train split: 5400 examples [4:07:58,  2.76s/ examples]embeddings shape: torch.Size([32, 768, 1195])
Epoch 1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 403/573 [1:41:01<42:37, 15.04s/it, loss=0.352, v_num=hitj, train categorical_accuracy_step=0.688, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 403/573 [1:41:01<42:37, 15.04s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1606])
Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 404/573 [1:41:17<42:22, 15.04s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 404/573 [1:41:17<42:22, 15.04s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5963 examples [4:09:50,  2.28s/ examples]embeddings shape: torch.Size([32, 768, 1558])
Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 405/573 [1:41:39<42:10, 15.06s/it, loss=0.351, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 405/573 [1:41:39<42:10, 15.06s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1493])
Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 406/573 [1:41:46<41:51, 15.04s/it, loss=0.346, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 406/573 [1:41:46<41:51, 15.04s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1598])
Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 407/573 [1:41:53<41:33, 15.02s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 407/573 [1:41:53<41:33, 15.02s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1576])
Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 408/573 [1:42:01<41:15, 15.00s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 408/573 [1:42:01<41:15, 15.00s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1560])
Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 409/573 [1:42:07<40:57, 14.98s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 409/573 [1:42:07<40:57, 14.98s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1569])
Epoch 1:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 410/573 [1:42:15<40:39, 14.96s/it, loss=0.342, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 410/573 [1:42:15<40:39, 14.96s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1567])
Epoch 1:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 411/573 [1:42:46<40:30, 15.00s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 411/573 [1:42:46<40:30, 15.00s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6710 examples [4:11:12,  2.37s/ examples]Generating train split: 5450 examples [4:10:19,  2.78s/ examples]embeddings shape: torch.Size([32, 768, 1657])
Epoch 1:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 412/573 [1:43:21<40:23, 15.05s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 412/573 [1:43:21<40:23, 15.05s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1613])
Epoch 1:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 413/573 [1:43:27<40:04, 15.03s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 413/573 [1:43:27<40:04, 15.03s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1444])
Epoch 1:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 414/573 [1:43:34<39:46, 15.01s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 414/573 [1:43:34<39:46, 15.01s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1599])
Epoch 1:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 415/573 [1:44:02<39:36, 15.04s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 415/573 [1:44:02<39:36, 15.04s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] Generating train split: 6030 examples [4:12:24,  2.29s/ examples]embeddings shape: torch.Size([32, 768, 1618])
Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 416/573 [1:44:18<39:21, 15.04s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 416/573 [1:44:18<39:21, 15.04s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1605])
Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 417/573 [1:44:27<39:04, 15.03s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 417/573 [1:44:27<39:04, 15.03s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1674])
Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 418/573 [1:44:37<38:47, 15.02s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 418/573 [1:44:37<38:47, 15.02s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1575])
Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 419/573 [1:44:45<38:30, 15.00s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 419/573 [1:44:45<38:30, 15.00s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1618])
Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 420/573 [1:45:13<38:19, 15.03s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 420/573 [1:45:13<38:19, 15.03s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5500 examples [4:12:41,  2.79s/ examples]Generating train split: 6771 examples [4:13:48,  2.43s/ examples]embeddings shape: torch.Size([32, 768, 1606])
Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 421/573 [1:45:49<38:12, 15.08s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 421/573 [1:45:49<38:12, 15.08s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1572])
Epoch 1:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 422/573 [1:45:56<37:54, 15.06s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 422/573 [1:45:56<37:54, 15.06s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1683])
Epoch 1:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 423/573 [1:46:05<37:37, 15.05s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 423/573 [1:46:05<37:37, 15.05s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1582])
Epoch 1:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 424/573 [1:46:21<37:22, 15.05s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 424/573 [1:46:21<37:22, 15.05s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6097 examples [4:14:54,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1613])
Epoch 1:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 425/573 [1:46:44<37:10, 15.07s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 425/573 [1:46:44<37:10, 15.07s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1576])
Epoch 1:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 426/573 [1:46:52<36:52, 15.05s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 426/573 [1:46:52<36:52, 15.05s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1573])
Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 427/573 [1:47:00<36:35, 15.04s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 427/573 [1:47:00<36:35, 15.04s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1584])
Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 428/573 [1:47:06<36:17, 15.02s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 428/573 [1:47:06<36:17, 15.02s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1583])
Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 429/573 [1:47:13<35:59, 15.00s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 429/573 [1:47:13<35:59, 15.00s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1603])
Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 430/573 [1:47:40<35:48, 15.02s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 430/573 [1:47:40<35:48, 15.02s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] Generating train split: 6832 examples [4:16:08,  2.39s/ examples]Generating train split: 5550 examples [4:15:08,  2.84s/ examples]embeddings shape: torch.Size([32, 768, 1262])
Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 431/573 [1:48:09<35:38, 15.06s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 431/573 [1:48:09<35:38, 15.06s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1580])
Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 432/573 [1:48:19<35:21, 15.05s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 432/573 [1:48:19<35:21, 15.05s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1587])
Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 433/573 [1:48:30<35:05, 15.04s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 433/573 [1:48:30<35:05, 15.04s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6164 examples [4:17:09,  2.20s/ examples]embeddings shape: torch.Size([32, 768, 1320])
Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 434/573 [1:48:55<34:53, 15.06s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 434/573 [1:48:55<34:53, 15.06s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1585])
Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 435/573 [1:49:05<34:36, 15.05s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 435/573 [1:49:05<34:36, 15.05s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1602])
Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 436/573 [1:49:15<34:19, 15.04s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 436/573 [1:49:15<34:19, 15.04s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1584])
Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 437/573 [1:49:22<34:02, 15.02s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 437/573 [1:49:22<34:02, 15.02s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1593])
Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 438/573 [1:49:30<33:45, 15.00s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 438/573 [1:49:30<33:45, 15.00s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842] embeddings shape: torch.Size([32, 768, 1653])
Epoch 1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 439/573 [1:50:03<33:35, 15.04s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 439/573 [1:50:03<33:35, 15.04s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5600 examples [4:17:20,  2.78s/ examples]Generating train split: 6893 examples [4:18:24,  2.34s/ examples]embeddings shape: torch.Size([32, 768, 1610])
Epoch 1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 440/573 [1:50:26<33:23, 15.06s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 440/573 [1:50:26<33:23, 15.06s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1551])
Epoch 1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 441/573 [1:50:36<33:06, 15.05s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 441/573 [1:50:36<33:06, 15.05s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6231 examples [4:19:13,  2.09s/ examples]embeddings shape: torch.Size([32, 768, 1586])
Epoch 1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 442/573 [1:51:00<32:54, 15.07s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 442/573 [1:51:00<32:54, 15.07s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1579])
Epoch 1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 443/573 [1:51:07<32:36, 15.05s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 443/573 [1:51:07<32:36, 15.05s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1606])
Epoch 1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 444/573 [1:51:15<32:19, 15.03s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 444/573 [1:51:15<32:19, 15.03s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1595])
Epoch 1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 445/573 [1:51:24<32:02, 15.02s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 445/573 [1:51:24<32:02, 15.02s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1556])
Epoch 1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 446/573 [1:51:32<31:45, 15.01s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 446/573 [1:51:32<31:45, 15.01s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1591])
Epoch 1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 447/573 [1:51:41<31:29, 14.99s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 447/573 [1:51:41<31:29, 14.99s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1582])
Epoch 1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 448/573 [1:51:47<31:11, 14.97s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 448/573 [1:51:47<31:11, 14.97s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1571])
Epoch 1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 449/573 [1:52:17<31:00, 15.01s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 449/573 [1:52:17<31:00, 15.01s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6954 examples [4:20:40,  2.30s/ examples]embeddings shape: torch.Size([32, 768, 1607])
Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 450/573 [1:52:47<30:49, 15.04s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 450/573 [1:52:47<30:49, 15.04s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5650 examples [4:19:49,  2.84s/ examples]Generating train split: 6298 examples [4:21:25,  2.06s/ examples]embeddings shape: torch.Size([32, 768, 1645])
Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 451/573 [1:53:13<30:37, 15.06s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 451/573 [1:53:13<30:37, 15.06s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1671])
Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 452/573 [1:53:20<30:20, 15.04s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 452/573 [1:53:20<30:20, 15.04s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1579])
Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 453/573 [1:53:27<30:03, 15.03s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 453/573 [1:53:27<30:03, 15.03s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1618])
Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 454/573 [1:53:35<29:46, 15.01s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 454/573 [1:53:35<29:46, 15.01s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1469])
Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 455/573 [1:53:43<29:29, 15.00s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 455/573 [1:53:43<29:29, 15.00s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1556])
Epoch 1:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 456/573 [1:53:52<29:13, 14.98s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 456/573 [1:53:52<29:13, 14.98s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1644])
Epoch 1:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 457/573 [1:54:00<28:56, 14.97s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 457/573 [1:54:00<28:56, 14.97s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([26, 768, 1604])
Epoch 1:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 458/573 [1:54:07<28:39, 14.95s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 458/573 [1:54:07<28:39, 14.95s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]
Validation: 0it [00:00, ?it/s][A
Validation:   0%|          | 0/115 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/115 [00:00<?, ?it/s][Aembeddings shape: torch.Size([32, 768, 1569])

Validation DataLoader 0:   1%|          | 1/115 [00:00<00:00, 172.91it/s][AEpoch 1:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 459/573 [1:54:41<28:29, 14.99s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 7015 examples [4:23:07,  2.34s/ examples]Generating train split: 5700 examples [4:22:08,  2.82s/ examples]embeddings shape: torch.Size([32, 768, 1635])

Validation DataLoader 0:   2%|‚ñè         | 2/115 [00:33<31:08, 16.54s/it] [AEpoch 1:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 460/573 [1:55:14<28:18, 15.03s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1614])

Validation DataLoader 0:   3%|‚ñé         | 3/115 [01:17<48:01, 25.73s/it][AEpoch 1:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 461/573 [1:55:58<28:10, 15.10s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6365 examples [4:24:13,  2.19s/ examples]embeddings shape: torch.Size([32, 768, 1551])

Validation DataLoader 0:   3%|‚ñé         | 4/115 [01:27<40:16, 21.77s/it][AEpoch 1:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 462/573 [1:56:08<27:54, 15.08s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1601])

Validation DataLoader 0:   4%|‚ñç         | 5/115 [01:34<34:48, 18.99s/it][AEpoch 1:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 463/573 [1:56:16<27:37, 15.07s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1631])

Validation DataLoader 0:   5%|‚ñå         | 6/115 [01:42<30:55, 17.02s/it][AEpoch 1:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 464/573 [1:56:23<27:20, 15.05s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1677])

Validation DataLoader 0:   6%|‚ñå         | 7/115 [01:48<27:52, 15.49s/it][AEpoch 1:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 465/573 [1:56:30<27:03, 15.03s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1594])

Validation DataLoader 0:   7%|‚ñã         | 8/115 [01:54<25:32, 14.32s/it][AEpoch 1:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 466/573 [1:56:36<26:46, 15.01s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1383])

Validation DataLoader 0:   8%|‚ñä         | 9/115 [02:02<23:57, 13.56s/it][AEpoch 1:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 467/573 [1:56:43<26:29, 15.00s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1634])

Validation DataLoader 0:   9%|‚ñä         | 10/115 [02:22<24:52, 14.22s/it][AEpoch 1:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 468/573 [1:57:03<26:15, 15.01s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1573])

Validation DataLoader 0:  10%|‚ñâ         | 11/115 [02:48<26:29, 15.28s/it][AEpoch 1:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 469/573 [1:57:29<26:03, 15.03s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 7076 examples [4:25:38,  2.38s/ examples]Generating train split: 5750 examples [4:24:42,  2.90s/ examples]embeddings shape: torch.Size([32, 768, 1607])

Validation DataLoader 0:  10%|‚ñà         | 12/115 [03:05<26:34, 15.48s/it][AEpoch 1:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 470/573 [1:57:47<25:48, 15.04s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1446])

Validation DataLoader 0:  11%|‚ñà‚ñè        | 13/115 [03:30<27:28, 16.16s/it][AEpoch 1:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 471/573 [1:58:11<25:35, 15.06s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6432 examples [4:26:36,  2.17s/ examples]embeddings shape: torch.Size([32, 768, 1323])

Validation DataLoader 0:  12%|‚ñà‚ñè        | 14/115 [03:44<26:58, 16.03s/it][AEpoch 1:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 472/573 [1:58:26<25:20, 15.06s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1219])

Validation DataLoader 0:  13%|‚ñà‚ñé        | 15/115 [03:48<25:24, 15.25s/it][AEpoch 1:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 473/573 [1:58:30<25:03, 15.03s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1092])

Validation DataLoader 0:  14%|‚ñà‚ñç        | 16/115 [03:54<24:09, 14.64s/it][AEpoch 1:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 474/573 [1:58:36<24:46, 15.01s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1197])

Validation DataLoader 0:  15%|‚ñà‚ñç        | 17/115 [03:59<23:02, 14.10s/it][AEpoch 1:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 475/573 [1:58:41<24:29, 14.99s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1569])

Validation DataLoader 0:  16%|‚ñà‚ñå        | 18/115 [04:06<22:05, 13.67s/it][AEpoch 1:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 476/573 [1:58:47<24:12, 14.97s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1234])

Validation DataLoader 0:  17%|‚ñà‚ñã        | 19/115 [04:12<21:13, 13.27s/it][AEpoch 1:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 477/573 [1:58:53<23:55, 14.96s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1597])

Validation DataLoader 0:  17%|‚ñà‚ñã        | 20/115 [04:24<20:58, 13.24s/it][AEpoch 1:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 478/573 [1:59:06<23:40, 14.95s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1641])

Validation DataLoader 0:  18%|‚ñà‚ñä        | 21/115 [05:11<23:16, 14.85s/it][AEpoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 479/573 [1:59:53<23:31, 15.02s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 7137 examples [4:28:08,  2.40s/ examples]Generating train split: 5800 examples [4:27:16,  2.96s/ examples]Generating train split: 6499 examples [4:28:46,  2.11s/ examples]embeddings shape: torch.Size([32, 768, 1672])

Validation DataLoader 0:  19%|‚ñà‚ñâ        | 22/115 [05:53<24:52, 16.05s/it][AEpoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 480/573 [2:00:34<23:21, 15.07s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1613])

Validation DataLoader 0:  20%|‚ñà‚ñà        | 23/115 [06:03<24:13, 15.80s/it][AEpoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 481/573 [2:00:45<23:05, 15.06s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1631])

Validation DataLoader 0:  21%|‚ñà‚ñà        | 24/115 [06:14<23:39, 15.60s/it][AEpoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 482/573 [2:00:56<22:49, 15.05s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1631])

Validation DataLoader 0:  22%|‚ñà‚ñà‚ñè       | 25/115 [06:25<23:08, 15.43s/it][AEpoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 483/573 [2:01:07<22:34, 15.05s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1614])

Validation DataLoader 0:  23%|‚ñà‚ñà‚ñé       | 26/115 [06:38<22:43, 15.32s/it][AEpoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 484/573 [2:01:20<22:18, 15.04s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1592])

Validation DataLoader 0:  23%|‚ñà‚ñà‚ñé       | 27/115 [07:08<23:18, 15.89s/it][AEpoch 1:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 485/573 [2:01:50<22:06, 15.07s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 7198 examples [4:30:12,  2.29s/ examples]Generating train split: 5850 examples [4:29:30,  2.87s/ examples]embeddings shape: torch.Size([32, 768, 1681])

Validation DataLoader 0:  24%|‚ñà‚ñà‚ñç       | 28/115 [08:00<24:51, 17.15s/it][AEpoch 1:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 486/573 [2:02:41<21:57, 15.15s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6566 examples [4:31:11,  2.12s/ examples]embeddings shape: torch.Size([32, 768, 1618])

Validation DataLoader 0:  25%|‚ñà‚ñà‚ñå       | 29/115 [08:21<24:46, 17.28s/it][AEpoch 1:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 487/573 [2:03:02<21:43, 15.16s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1618])

Validation DataLoader 0:  26%|‚ñà‚ñà‚ñå       | 30/115 [08:29<24:03, 16.98s/it][AEpoch 1:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 488/573 [2:03:11<21:27, 15.15s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1540])

Validation DataLoader 0:  27%|‚ñà‚ñà‚ñã       | 31/115 [08:37<23:21, 16.68s/it][AEpoch 1:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 489/573 [2:03:18<21:10, 15.13s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1574])

Validation DataLoader 0:  28%|‚ñà‚ñà‚ñä       | 32/115 [08:44<22:41, 16.40s/it][AEpoch 1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 490/573 [2:03:26<20:54, 15.12s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1577])

Validation DataLoader 0:  29%|‚ñà‚ñà‚ñä       | 33/115 [08:53<22:05, 16.17s/it][AEpoch 1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 491/573 [2:03:35<20:38, 15.10s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1574])

Validation DataLoader 0:  30%|‚ñà‚ñà‚ñâ       | 34/115 [08:59<21:25, 15.87s/it][AEpoch 1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 492/573 [2:03:41<20:21, 15.08s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1677])

Validation DataLoader 0:  30%|‚ñà‚ñà‚ñà       | 35/115 [09:15<21:09, 15.86s/it][AEpoch 1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 493/573 [2:03:57<20:06, 15.09s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 7259 examples [4:32:24,  2.25s/ examples]embeddings shape: torch.Size([32, 768, 1615])

Validation DataLoader 0:  31%|‚ñà‚ñà‚ñà‚ñè      | 36/115 [09:50<21:35, 16.40s/it][AEpoch 1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 494/573 [2:04:32<19:54, 15.13s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5900 examples [4:31:45,  2.82s/ examples]embeddings shape: torch.Size([32, 768, 1582])

Validation DataLoader 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 37/115 [10:13<21:34, 16.59s/it][AEpoch 1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 495/573 [2:04:55<19:41, 15.14s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1602])

Validation DataLoader 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 38/115 [10:40<21:37, 16.85s/it][AEpoch 1:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 496/573 [2:05:21<19:27, 15.17s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6633 examples [4:33:40,  2.15s/ examples]embeddings shape: torch.Size([32, 768, 1545])

Validation DataLoader 0:  34%|‚ñà‚ñà‚ñà‚ñç      | 39/115 [10:52<21:11, 16.73s/it][AEpoch 1:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 497/573 [2:05:34<19:12, 15.16s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1615])

Validation DataLoader 0:  35%|‚ñà‚ñà‚ñà‚ñç      | 40/115 [10:59<20:36, 16.49s/it][AEpoch 1:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 498/573 [2:05:41<18:55, 15.14s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1602])

Validation DataLoader 0:  36%|‚ñà‚ñà‚ñà‚ñå      | 41/115 [11:06<20:02, 16.25s/it][AEpoch 1:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 499/573 [2:05:48<18:39, 15.13s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1656])

Validation DataLoader 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 42/115 [11:14<19:32, 16.07s/it][AEpoch 1:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 500/573 [2:05:56<18:23, 15.11s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1654])

Validation DataLoader 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 43/115 [11:38<19:30, 16.25s/it][AEpoch 1:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 501/573 [2:06:20<18:09, 15.13s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 7320 examples [4:34:37,  2.23s/ examples]embeddings shape: torch.Size([32, 768, 1570])

Validation DataLoader 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 44/115 [12:12<19:42, 16.65s/it][AEpoch 1:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 502/573 [2:06:54<17:56, 15.17s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 5950 examples [4:33:59,  2.78s/ examples]embeddings shape: torch.Size([32, 768, 1627])

Validation DataLoader 0:  39%|‚ñà‚ñà‚ñà‚ñâ      | 45/115 [12:24<19:17, 16.54s/it][AEpoch 1:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 503/573 [2:07:06<17:41, 15.16s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1646])

Validation DataLoader 0:  40%|‚ñà‚ñà‚ñà‚ñà      | 46/115 [12:31<18:47, 16.33s/it][AEpoch 1:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 504/573 [2:07:13<17:25, 15.15s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1492])

Validation DataLoader 0:  41%|‚ñà‚ñà‚ñà‚ñà      | 47/115 [12:36<18:15, 16.11s/it][AEpoch 1:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 505/573 [2:07:18<17:08, 15.13s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1575])

Validation DataLoader 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 48/115 [12:47<17:51, 15.99s/it][AEpoch 1:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 506/573 [2:07:29<16:52, 15.12s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1594])

Validation DataLoader 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 49/115 [13:15<17:51, 16.24s/it][AEpoch 1:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 507/573 [2:07:57<16:39, 15.14s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6700 examples [4:36:19,  2.22s/ examples]embeddings shape: torch.Size([32, 768, 1549])

Validation DataLoader 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 50/115 [13:27<17:30, 16.16s/it][AEpoch 1:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 508/573 [2:08:09<16:23, 15.14s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1532])

Validation DataLoader 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51/115 [13:33<17:01, 15.95s/it][AEpoch 1:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 509/573 [2:08:15<16:07, 15.12s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1573])

Validation DataLoader 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 52/115 [13:40<16:34, 15.78s/it][AEpoch 1:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 510/573 [2:08:22<15:51, 15.10s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1604])

Validation DataLoader 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 53/115 [13:58<16:21, 15.83s/it][AEpoch 1:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 511/573 [2:08:40<15:36, 15.11s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 7381 examples [4:37:02,  2.28s/ examples]embeddings shape: torch.Size([32, 768, 1557])

Validation DataLoader 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 54/115 [14:30<16:22, 16.11s/it][AEpoch 1:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 512/573 [2:09:11<15:23, 15.14s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6000 examples [4:36:25,  2.82s/ examples]embeddings shape: torch.Size([32, 768, 1535])

Validation DataLoader 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 55/115 [14:47<16:08, 16.14s/it][AEpoch 1:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 513/573 [2:09:29<15:08, 15.14s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 802])

Validation DataLoader 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 56/115 [14:51<15:39, 15.92s/it][AEpoch 1:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 514/573 [2:09:33<14:52, 15.12s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1446])

Validation DataLoader 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 57/115 [14:58<15:14, 15.76s/it][AEpoch 1:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 515/573 [2:09:40<14:36, 15.11s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1251])

Validation DataLoader 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 58/115 [15:03<14:47, 15.58s/it][AEpoch 1:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 516/573 [2:09:45<14:20, 15.09s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1607])

Validation DataLoader 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 59/115 [15:09<14:23, 15.41s/it][AEpoch 1:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 517/573 [2:09:51<14:03, 15.07s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1191])

Validation DataLoader 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 60/115 [15:15<13:58, 15.25s/it][AEpoch 1:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 518/573 [2:09:56<13:47, 15.05s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1343])

Validation DataLoader 0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 61/115 [15:21<13:35, 15.11s/it][AEpoch 1:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 519/573 [2:10:03<13:31, 15.04s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 985])

Validation DataLoader 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 62/115 [15:44<13:27, 15.23s/it][AEpoch 1:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 520/573 [2:10:26<13:17, 15.05s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6767 examples [4:38:52,  2.24s/ examples]embeddings shape: torch.Size([32, 768, 1337])

Validation DataLoader 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 63/115 [15:58<13:11, 15.22s/it][AEpoch 1:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 521/573 [2:10:40<13:02, 15.05s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1107])

Validation DataLoader 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64/115 [16:02<12:46, 15.04s/it][AEpoch 1:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 522/573 [2:10:44<12:46, 15.03s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1142])

Validation DataLoader 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 65/115 [16:05<12:22, 14.86s/it][AEpoch 1:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 523/573 [2:10:47<12:30, 15.01s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1570])

Validation DataLoader 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 66/115 [16:26<12:12, 14.94s/it][AEpoch 1:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 524/573 [2:11:08<12:15, 15.02s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 7442 examples [4:39:31,  2.32s/ examples]embeddings shape: torch.Size([32, 768, 1587])

Validation DataLoader 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 67/115 [17:03<12:12, 15.27s/it][AEpoch 1:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 525/573 [2:11:44<12:02, 15.06s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6050 examples [4:38:47,  2.83s/ examples]embeddings shape: torch.Size([32, 768, 1609])

Validation DataLoader 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 68/115 [17:14<11:54, 15.21s/it][AEpoch 1:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 526/573 [2:11:56<11:47, 15.05s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1595])

Validation DataLoader 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 69/115 [17:22<11:35, 15.11s/it][AEpoch 1:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 527/573 [2:12:04<11:31, 15.04s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1593])

Validation DataLoader 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 70/115 [17:32<11:16, 15.04s/it][AEpoch 1:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 528/573 [2:12:14<11:16, 15.03s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1575])

Validation DataLoader 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 71/115 [17:40<10:57, 14.94s/it][AEpoch 1:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 529/573 [2:12:22<11:00, 15.01s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1684])

Validation DataLoader 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 72/115 [17:52<10:40, 14.89s/it][AEpoch 1:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 530/573 [2:12:33<10:45, 15.01s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6834 examples [4:41:25,  2.25s/ examples]embeddings shape: torch.Size([32, 768, 1662])

Validation DataLoader 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 73/115 [18:31<10:39, 15.22s/it][AEpoch 1:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 531/573 [2:13:12<10:32, 15.05s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1617])

Validation DataLoader 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 74/115 [19:09<10:36, 15.54s/it][AEpoch 1:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 532/573 [2:13:51<10:18, 15.10s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 7503 examples [4:42:00,  2.36s/ examples]Generating train split: 6100 examples [4:41:07,  2.82s/ examples]embeddings shape: torch.Size([32, 768, 1639])

Validation DataLoader 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 75/115 [19:32<10:25, 15.63s/it][AEpoch 1:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 533/573 [2:14:14<10:04, 15.11s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1612])

Validation DataLoader 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 76/115 [19:45<10:08, 15.59s/it][AEpoch 1:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 534/573 [2:14:27<09:49, 15.11s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1581])

Validation DataLoader 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 77/115 [19:51<09:48, 15.47s/it][AEpoch 1:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 535/573 [2:14:33<09:33, 15.09s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1159])

Validation DataLoader 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 78/115 [19:57<09:28, 15.36s/it][AEpoch 1:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 536/573 [2:14:39<09:17, 15.07s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1591])

Validation DataLoader 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 79/115 [20:04<09:08, 15.25s/it][AEpoch 1:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 537/573 [2:14:46<09:02, 15.06s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1455])

Validation DataLoader 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 80/115 [20:09<08:49, 15.12s/it][AEpoch 1:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 538/573 [2:14:51<08:46, 15.04s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1566])

Validation DataLoader 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 81/115 [20:15<08:30, 15.01s/it][AEpoch 1:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 539/573 [2:14:57<08:30, 15.02s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1571])

Validation DataLoader 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 82/115 [20:25<08:13, 14.95s/it][AEpoch 1:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 540/573 [2:15:07<08:15, 15.01s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6901 examples [4:43:50,  2.22s/ examples]embeddings shape: torch.Size([32, 768, 1568])

Validation DataLoader 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 83/115 [20:55<08:03, 15.12s/it][AEpoch 1:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 541/573 [2:15:37<08:01, 15.04s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1600])

Validation DataLoader 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 84/115 [21:28<07:55, 15.34s/it][AEpoch 1:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 542/573 [2:16:10<07:47, 15.07s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 7564 examples [4:44:22,  2.35s/ examples]Generating train split: 6150 examples [4:43:26,  2.81s/ examples]embeddings shape: torch.Size([32, 768, 1569])

Validation DataLoader 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 85/115 [21:50<07:42, 15.42s/it][AEpoch 1:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 543/573 [2:16:32<07:32, 15.09s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1626])

Validation DataLoader 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 86/115 [21:58<07:24, 15.33s/it][AEpoch 1:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 544/573 [2:16:40<07:17, 15.07s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1599])

Validation DataLoader 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 87/115 [22:08<07:07, 15.27s/it][AEpoch 1:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 545/573 [2:16:50<07:01, 15.06s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1553])

Validation DataLoader 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 88/115 [22:27<06:53, 15.31s/it][AEpoch 1:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 546/573 [2:17:09<06:46, 15.07s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6968 examples [4:45:30,  2.01s/ examples]embeddings shape: torch.Size([32, 768, 1636])

Validation DataLoader 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 89/115 [22:40<06:37, 15.29s/it][AEpoch 1:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 547/573 [2:17:22<06:31, 15.07s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1600])

Validation DataLoader 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 90/115 [22:47<06:19, 15.19s/it][AEpoch 1:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 548/573 [2:17:28<06:16, 15.05s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1475])

Validation DataLoader 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 91/115 [22:53<06:02, 15.09s/it][AEpoch 1:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 549/573 [2:17:34<06:00, 15.04s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1533])

Validation DataLoader 0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 92/115 [22:55<05:43, 14.95s/it][AEpoch 1:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 550/573 [2:17:37<05:45, 15.01s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1593])

Validation DataLoader 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 93/115 [23:08<05:28, 14.93s/it][AEpoch 1:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 551/573 [2:17:49<05:30, 15.01s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 48])

Validation DataLoader 0:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 94/115 [23:09<05:10, 14.78s/it][AEpoch 1:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 552/573 [2:17:51<05:14, 14.98s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1074])

Validation DataLoader 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 95/115 [23:22<04:55, 14.77s/it][AEpoch 1:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 553/573 [2:18:04<04:59, 14.98s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 6200 examples [4:45:36,  2.75s/ examples]Generating train split: 7625 examples [4:46:38,  2.31s/ examples]embeddings shape: torch.Size([32, 768, 1660])

Validation DataLoader 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 96/115 [24:00<04:45, 15.01s/it][AEpoch 1:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 554/573 [2:18:42<04:45, 15.02s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 7035 examples [4:47:18,  1.89s/ examples]embeddings shape: torch.Size([32, 768, 1624])

Validation DataLoader 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 97/115 [24:26<04:32, 15.12s/it][AEpoch 1:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 555/573 [2:19:08<04:30, 15.04s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1568])

Validation DataLoader 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 98/115 [24:32<04:15, 15.03s/it][AEpoch 1:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 556/573 [2:19:14<04:15, 15.03s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1581])

Validation DataLoader 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 99/115 [24:40<03:59, 14.95s/it][AEpoch 1:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 557/573 [2:19:22<04:00, 15.01s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1602])

Validation DataLoader 0:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 100/115 [24:47<03:43, 14.87s/it][AEpoch 1:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 558/573 [2:19:29<03:44, 15.00s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1598])

Validation DataLoader 0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 101/115 [24:52<03:26, 14.78s/it][AEpoch 1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 559/573 [2:19:34<03:29, 14.98s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1603])

Validation DataLoader 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 102/115 [25:00<03:11, 14.71s/it][AEpoch 1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 560/573 [2:19:42<03:14, 14.97s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Generating train split: 7102 examples [4:48:17,  1.59s/ examples]embeddings shape: torch.Size([32, 768, 1564])

Validation DataLoader 0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 103/115 [25:24<02:57, 14.80s/it][AEpoch 1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 561/573 [2:20:06<02:59, 14.98s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1615])

Validation DataLoader 0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 104/115 [25:37<02:42, 14.78s/it][AEpoch 1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 562/573 [2:20:18<02:44, 14.98s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 1659])

Validation DataLoader 0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 105/115 [26:11<02:29, 14.97s/it][AEpoch 1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 563/573 [2:20:53<02:30, 15.02s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 34])

Validation DataLoader 0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 106/115 [26:12<02:13, 14.84s/it][AEpoch 1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 564/573 [2:20:54<02:14, 14.99s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 30])

Validation DataLoader 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 107/115 [26:13<01:57, 14.71s/it][AEpoch 1:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 565/573 [2:20:55<01:59, 14.97s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 25])

Validation DataLoader 0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 108/115 [26:14<01:42, 14.58s/it][AEpoch 1:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 566/573 [2:20:55<01:44, 14.94s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 109/115 [26:14<01:26, 14.44s/it][AEpoch 1:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 567/573 [2:20:56<01:29, 14.91s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 110/115 [26:14<01:11, 14.32s/it][AEpoch 1:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 568/573 [2:20:56<01:14, 14.89s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 111/115 [26:14<00:56, 14.19s/it][AEpoch 1:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 569/573 [2:20:56<00:59, 14.86s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 112/115 [26:15<00:42, 14.07s/it][AEpoch 1:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 570/573 [2:20:57<00:44, 14.84s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 113/115 [26:15<00:27, 13.95s/it][AEpoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 571/573 [2:20:57<00:29, 14.81s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([32, 768, 6])

Validation DataLoader 0:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 114/115 [26:16<00:13, 13.83s/it][AEpoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 572/573 [2:20:57<00:14, 14.79s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]embeddings shape: torch.Size([22, 768, 22])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [26:16<00:00, 13.71s/it][AEpoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 573/573 [2:20:58<00:00, 14.76s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.859, val categorical_accuracy_epoch=0.414, val categorical_accuracy_strict_epoch=0.172, val binary_accuracy_epoch=0.838, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 573/573 [2:20:58<00:00, 14.76s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.407, train categorical_accuracy_strict_epoch=0.182, train binary_accuracy_epoch=0.842]
                                                                          [AEpoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 573/573 [2:20:58<00:00, 14.76s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 1, global step 916: 'val_loss' was not in top 1
Epoch duration: 8458.26 seconds
Epoch 1:   0%|          | 0/573 [00:00<?, ?it/s, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]            Epoch 2:   0%|          | 0/573 [00:00<?, ?it/s, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7686 examples [4:49:00,  2.32s/ examples]Generating train split: 6250 examples [4:48:00,  2.78s/ examples]embeddings shape: torch.Size([32, 768, 1603])
Epoch 2:   0%|          | 1/573 [00:07<1:14:37,  7.83s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   0%|          | 1/573 [00:07<1:14:38,  7.83s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 949])
Epoch 2:   0%|          | 2/573 [00:11<54:23,  5.72s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]  Epoch 2:   0%|          | 2/573 [00:11<54:23,  5.72s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1589])
Epoch 2:   1%|          | 3/573 [00:15<50:31,  5.32s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   1%|          | 3/573 [00:15<50:31,  5.32s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1598])
Epoch 2:   1%|          | 4/573 [00:21<51:12,  5.40s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   1%|          | 4/573 [00:21<51:12,  5.40s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1263])
Epoch 2:   1%|          | 5/573 [00:26<49:56,  5.28s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   1%|          | 5/573 [00:26<49:56,  5.28s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1599])
Epoch 2:   1%|          | 6/573 [00:31<48:58,  5.18s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   1%|          | 6/573 [00:31<48:58,  5.18s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1609])
Epoch 2:   1%|          | 7/573 [00:42<57:43,  6.12s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   1%|          | 7/573 [00:42<57:43,  6.12s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7169 examples [4:50:09,  1.61s/ examples]embeddings shape: torch.Size([32, 768, 1653])
Epoch 2:   1%|‚ñè         | 8/573 [00:57<1:07:43,  7.19s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   1%|‚ñè         | 8/573 [00:57<1:07:43,  7.19s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1573])
Epoch 2:   2%|‚ñè         | 9/573 [01:02<1:05:19,  6.95s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   2%|‚ñè         | 9/573 [01:02<1:05:19,  6.95s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1623])
Epoch 2:   2%|‚ñè         | 10/573 [01:08<1:04:41,  6.90s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   2%|‚ñè         | 10/573 [01:08<1:04:41,  6.90s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1591])
Epoch 2:   2%|‚ñè         | 11/573 [01:15<1:03:56,  6.83s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   2%|‚ñè         | 11/573 [01:15<1:03:56,  6.83s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1597])
Epoch 2:   2%|‚ñè         | 12/573 [01:31<1:11:27,  7.64s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   2%|‚ñè         | 12/573 [01:31<1:11:28,  7.64s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1011])
Epoch 2:   2%|‚ñè         | 13/573 [01:45<1:15:40,  8.11s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   2%|‚ñè         | 13/573 [01:45<1:15:40,  8.11s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1640])
Epoch 2:   2%|‚ñè         | 14/573 [02:14<1:29:43,  9.63s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   2%|‚ñè         | 14/573 [02:14<1:29:43,  9.63s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7747 examples [4:51:14,  2.28s/ examples]Generating train split: 6300 examples [4:50:14,  2.75s/ examples]embeddings shape: torch.Size([32, 768, 1617])
Epoch 2:   3%|‚ñé         | 15/573 [02:22<1:28:13,  9.49s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   3%|‚ñé         | 15/573 [02:22<1:28:13,  9.49s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] Generating train split: 7236 examples [4:51:53,  1.59s/ examples]embeddings shape: torch.Size([32, 768, 1593])
Epoch 2:   3%|‚ñé         | 16/573 [02:42<1:34:02, 10.13s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   3%|‚ñé         | 16/573 [02:42<1:34:02, 10.13s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1610])
Epoch 2:   3%|‚ñé         | 17/573 [02:48<1:31:58,  9.93s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   3%|‚ñé         | 17/573 [02:48<1:31:58,  9.93s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1625])
Epoch 2:   3%|‚ñé         | 18/573 [02:54<1:29:40,  9.70s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   3%|‚ñé         | 18/573 [02:54<1:29:40,  9.70s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1628])
Epoch 2:   3%|‚ñé         | 19/573 [03:00<1:27:51,  9.52s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   3%|‚ñé         | 19/573 [03:00<1:27:51,  9.52s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1560])
Epoch 2:   3%|‚ñé         | 20/573 [03:06<1:26:05,  9.34s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   3%|‚ñé         | 20/573 [03:06<1:26:05,  9.34s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1551])
Epoch 2:   4%|‚ñé         | 21/573 [03:13<1:24:49,  9.22s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   4%|‚ñé         | 21/573 [03:13<1:24:49,  9.22s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1605])
Epoch 2:   4%|‚ñç         | 22/573 [03:17<1:22:38,  9.00s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   4%|‚ñç         | 22/573 [03:17<1:22:38,  9.00s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1529])
Epoch 2:   4%|‚ñç         | 23/573 [03:29<1:23:28,  9.11s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   4%|‚ñç         | 23/573 [03:29<1:23:28,  9.11s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7303 examples [4:52:52,  1.38s/ examples]embeddings shape: torch.Size([32, 768, 1631])
Epoch 2:   4%|‚ñç         | 24/573 [03:43<1:25:01,  9.29s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   4%|‚ñç         | 24/573 [03:43<1:25:01,  9.29s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1620])
Epoch 2:   4%|‚ñç         | 25/573 [03:56<1:26:13,  9.44s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   4%|‚ñç         | 25/573 [03:56<1:26:13,  9.44s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1678])
Epoch 2:   5%|‚ñç         | 26/573 [04:22<1:32:01, 10.09s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   5%|‚ñç         | 26/573 [04:22<1:32:01, 10.09s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] Generating train split: 7808 examples [4:53:43,  2.33s/ examples]embeddings shape: torch.Size([32, 768, 1645])
Epoch 2:   5%|‚ñç         | 27/573 [04:45<1:36:19, 10.59s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   5%|‚ñç         | 27/573 [04:45<1:36:19, 10.59s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 6350 examples [4:52:43,  2.82s/ examples]embeddings shape: torch.Size([32, 768, 1648])
Epoch 2:   5%|‚ñç         | 28/573 [04:52<1:34:54, 10.45s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   5%|‚ñç         | 28/573 [04:52<1:34:54, 10.45s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1653])
Epoch 2:   5%|‚ñå         | 29/573 [04:58<1:33:21, 10.30s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   5%|‚ñå         | 29/573 [04:58<1:33:21, 10.30s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1603])
Epoch 2:   5%|‚ñå         | 30/573 [05:05<1:32:03, 10.17s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   5%|‚ñå         | 30/573 [05:05<1:32:03, 10.17s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1589])
Epoch 2:   5%|‚ñå         | 31/573 [05:13<1:31:25, 10.12s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   5%|‚ñå         | 31/573 [05:13<1:31:25, 10.12s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7370 examples [4:54:40,  1.45s/ examples]embeddings shape: torch.Size([32, 768, 1596])
Epoch 2:   6%|‚ñå         | 32/573 [05:29<1:32:54, 10.30s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   6%|‚ñå         | 32/573 [05:29<1:32:54, 10.30s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1176])
Epoch 2:   6%|‚ñå         | 33/573 [05:34<1:31:08, 10.13s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   6%|‚ñå         | 33/573 [05:34<1:31:08, 10.13s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1634])
Epoch 2:   6%|‚ñå         | 34/573 [05:39<1:29:38,  9.98s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   6%|‚ñå         | 34/573 [05:39<1:29:38,  9.98s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1594])
Epoch 2:   6%|‚ñå         | 35/573 [05:44<1:28:20,  9.85s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   6%|‚ñå         | 35/573 [05:44<1:28:20,  9.85s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1565])
Epoch 2:   6%|‚ñã         | 36/573 [05:49<1:26:51,  9.71s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   6%|‚ñã         | 36/573 [05:49<1:26:51,  9.71s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1447])
Epoch 2:   6%|‚ñã         | 37/573 [05:53<1:25:23,  9.56s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   6%|‚ñã         | 37/573 [05:53<1:25:23,  9.56s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1575])
Epoch 2:   7%|‚ñã         | 38/573 [06:03<1:25:23,  9.58s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   7%|‚ñã         | 38/573 [06:03<1:25:24,  9.58s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1670])
Epoch 2:   7%|‚ñã         | 39/573 [06:26<1:28:08,  9.90s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   7%|‚ñã         | 39/573 [06:26<1:28:08,  9.90s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 6400 examples [4:54:46,  2.71s/ examples]embeddings shape: torch.Size([32, 768, 1540])
Epoch 2:   7%|‚ñã         | 40/573 [06:49<1:30:52, 10.23s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   7%|‚ñã         | 40/573 [06:49<1:30:52, 10.23s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7869 examples [4:55:52,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1600])
Epoch 2:   7%|‚ñã         | 41/573 [07:12<1:33:28, 10.54s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   7%|‚ñã         | 41/573 [07:12<1:33:28, 10.54s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7437 examples [4:56:32,  1.52s/ examples]embeddings shape: torch.Size([32, 768, 1561])
Epoch 2:   7%|‚ñã         | 42/573 [07:24<1:33:45, 10.59s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   7%|‚ñã         | 42/573 [07:24<1:33:45, 10.59s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1586])
Epoch 2:   8%|‚ñä         | 43/573 [07:31<1:32:49, 10.51s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   8%|‚ñä         | 43/573 [07:31<1:32:49, 10.51s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1642])
Epoch 2:   8%|‚ñä         | 44/573 [07:36<1:31:28, 10.37s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   8%|‚ñä         | 44/573 [07:36<1:31:28, 10.37s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1602])
Epoch 2:   8%|‚ñä         | 45/573 [07:41<1:30:14, 10.25s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   8%|‚ñä         | 45/573 [07:41<1:30:14, 10.25s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1613])
Epoch 2:   8%|‚ñä         | 46/573 [07:46<1:29:09, 10.15s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   8%|‚ñä         | 46/573 [07:46<1:29:09, 10.15s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1602])
Epoch 2:   8%|‚ñä         | 47/573 [07:54<1:28:26, 10.09s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   8%|‚ñä         | 47/573 [07:54<1:28:26, 10.09s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1625])
Epoch 2:   8%|‚ñä         | 48/573 [07:58<1:27:17,  9.98s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   8%|‚ñä         | 48/573 [07:58<1:27:17,  9.98s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7504 examples [4:57:33,  1.34s/ examples]embeddings shape: torch.Size([32, 768, 1698])
Epoch 2:   9%|‚ñä         | 49/573 [08:21<1:29:27, 10.24s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   9%|‚ñä         | 49/573 [08:21<1:29:27, 10.24s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1582])
Epoch 2:   9%|‚ñä         | 50/573 [08:31<1:29:15, 10.24s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   9%|‚ñä         | 50/573 [08:31<1:29:15, 10.24s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1535])
Epoch 2:   9%|‚ñâ         | 51/573 [08:53<1:31:01, 10.46s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   9%|‚ñâ         | 51/573 [08:53<1:31:01, 10.46s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1301])
Epoch 2:   9%|‚ñâ         | 52/573 [09:22<1:33:53, 10.81s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   9%|‚ñâ         | 52/573 [09:22<1:33:53, 10.81s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 6450 examples [4:57:22,  2.84s/ examples]Generating train split: 7930 examples [4:58:30,  2.36s/ examples]embeddings shape: torch.Size([32, 768, 1570])
Epoch 2:   9%|‚ñâ         | 53/573 [09:35<1:34:08, 10.86s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   9%|‚ñâ         | 53/573 [09:35<1:34:08, 10.86s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1626])
Epoch 2:   9%|‚ñâ         | 54/573 [09:41<1:33:08, 10.77s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:   9%|‚ñâ         | 54/573 [09:41<1:33:08, 10.77s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1667])
Epoch 2:  10%|‚ñâ         | 55/573 [09:48<1:32:21, 10.70s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  10%|‚ñâ         | 55/573 [09:48<1:32:21, 10.70s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1542])
Epoch 2:  10%|‚ñâ         | 56/573 [10:07<1:33:26, 10.85s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  10%|‚ñâ         | 56/573 [10:07<1:33:26, 10.85s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7571 examples [4:59:23,  1.42s/ examples]embeddings shape: torch.Size([32, 768, 1589])
Epoch 2:  10%|‚ñâ         | 57/573 [10:15<1:32:48, 10.79s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  10%|‚ñâ         | 57/573 [10:15<1:32:48, 10.79s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1548])
Epoch 2:  10%|‚ñà         | 58/573 [10:21<1:32:01, 10.72s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  10%|‚ñà         | 58/573 [10:21<1:32:01, 10.72s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1328])
Epoch 2:  10%|‚ñà         | 59/573 [10:27<1:31:03, 10.63s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  10%|‚ñà         | 59/573 [10:27<1:31:03, 10.63s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1585])
Epoch 2:  10%|‚ñà         | 60/573 [10:32<1:30:10, 10.55s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  10%|‚ñà         | 60/573 [10:32<1:30:10, 10.55s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1565])
Epoch 2:  11%|‚ñà         | 61/573 [10:37<1:29:14, 10.46s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  11%|‚ñà         | 61/573 [10:37<1:29:14, 10.46s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1210])
Epoch 2:  11%|‚ñà         | 62/573 [10:42<1:28:16, 10.36s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  11%|‚ñà         | 62/573 [10:42<1:28:16, 10.36s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1571])
Epoch 2:  11%|‚ñà         | 63/573 [10:57<1:28:38, 10.43s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  11%|‚ñà         | 63/573 [10:57<1:28:38, 10.43s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1594])
Epoch 2:  11%|‚ñà         | 64/573 [11:17<1:29:45, 10.58s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  11%|‚ñà         | 64/573 [11:17<1:29:45, 10.58s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1402])
Epoch 2:  11%|‚ñà‚ñè        | 65/573 [11:38<1:31:02, 10.75s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  11%|‚ñà‚ñè        | 65/573 [11:38<1:31:02, 10.75s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7638 examples [5:00:58,  1.43s/ examples]Generating train split: 7990 examples [5:00:47,  2.34s/ examples]Generating train split: 6500 examples [4:59:48,  2.86s/ examples]embeddings shape: torch.Size([32, 768, 1575])
Epoch 2:  12%|‚ñà‚ñè        | 66/573 [11:52<1:31:13, 10.79s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  12%|‚ñà‚ñè        | 66/573 [11:52<1:31:13, 10.80s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.802, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1575])
Epoch 2:  12%|‚ñà‚ñè        | 67/573 [11:58<1:30:29, 10.73s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.802, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  12%|‚ñà‚ñè        | 67/573 [11:58<1:30:29, 10.73s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1573])
Epoch 2:  12%|‚ñà‚ñè        | 68/573 [12:06<1:29:54, 10.68s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  12%|‚ñà‚ñè        | 68/573 [12:06<1:29:54, 10.68s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1572])
Epoch 2:  12%|‚ñà‚ñè        | 69/573 [12:12<1:29:11, 10.62s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  12%|‚ñà‚ñè        | 69/573 [12:12<1:29:11, 10.62s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1567])
Epoch 2:  12%|‚ñà‚ñè        | 70/573 [12:18<1:28:29, 10.56s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  12%|‚ñà‚ñè        | 70/573 [12:18<1:28:29, 10.56s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1683])
Epoch 2:  12%|‚ñà‚ñè        | 71/573 [12:26<1:27:55, 10.51s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  12%|‚ñà‚ñè        | 71/573 [12:26<1:27:55, 10.51s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.917, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1588])
Epoch 2:  13%|‚ñà‚ñé        | 72/573 [12:40<1:28:08, 10.56s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.917, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  13%|‚ñà‚ñé        | 72/573 [12:40<1:28:08, 10.56s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7705 examples [5:02:02,  1.28s/ examples]embeddings shape: torch.Size([32, 768, 1616])
Epoch 2:  13%|‚ñà‚ñé        | 73/573 [12:54<1:28:23, 10.61s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  13%|‚ñà‚ñé        | 73/573 [12:54<1:28:23, 10.61s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1608])
Epoch 2:  13%|‚ñà‚ñé        | 74/573 [13:01<1:27:51, 10.56s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  13%|‚ñà‚ñé        | 74/573 [13:01<1:27:51, 10.56s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1645])
Epoch 2:  13%|‚ñà‚ñé        | 75/573 [13:16<1:28:07, 10.62s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  13%|‚ñà‚ñé        | 75/573 [13:16<1:28:07, 10.62s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1614])
Epoch 2:  13%|‚ñà‚ñé        | 76/573 [13:45<1:29:59, 10.86s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  13%|‚ñà‚ñé        | 76/573 [13:45<1:29:59, 10.86s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8050 examples [5:02:50,  2.25s/ examples]Generating train split: 6550 examples [5:01:59,  2.79s/ examples]embeddings shape: torch.Size([32, 768, 1557])
Epoch 2:  13%|‚ñà‚ñé        | 77/573 [14:05<1:30:45, 10.98s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  13%|‚ñà‚ñé        | 77/573 [14:05<1:30:45, 10.98s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1114])
Epoch 2:  14%|‚ñà‚ñé        | 78/573 [14:11<1:30:01, 10.91s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  14%|‚ñà‚ñé        | 78/573 [14:11<1:30:01, 10.91s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1579])
Epoch 2:  14%|‚ñà‚ñç        | 79/573 [14:17<1:29:19, 10.85s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  14%|‚ñà‚ñç        | 79/573 [14:17<1:29:19, 10.85s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1514])
Epoch 2:  14%|‚ñà‚ñç        | 80/573 [14:25<1:28:54, 10.82s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  14%|‚ñà‚ñç        | 80/573 [14:25<1:28:54, 10.82s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7772 examples [5:03:55,  1.40s/ examples]embeddings shape: torch.Size([32, 768, 1573])
Epoch 2:  14%|‚ñà‚ñç        | 81/573 [14:44<1:29:29, 10.91s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  14%|‚ñà‚ñç        | 81/573 [14:44<1:29:29, 10.91s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1481])
Epoch 2:  14%|‚ñà‚ñç        | 82/573 [14:50<1:28:52, 10.86s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  14%|‚ñà‚ñç        | 82/573 [14:50<1:28:52, 10.86s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1588])
Epoch 2:  14%|‚ñà‚ñç        | 83/573 [14:57<1:28:19, 10.82s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  14%|‚ñà‚ñç        | 83/573 [14:57<1:28:19, 10.82s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.809, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1613])
Epoch 2:  15%|‚ñà‚ñç        | 84/573 [15:04<1:27:47, 10.77s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.809, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  15%|‚ñà‚ñç        | 84/573 [15:04<1:27:47, 10.77s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1590])
Epoch 2:  15%|‚ñà‚ñç        | 85/573 [15:10<1:27:05, 10.71s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  15%|‚ñà‚ñç        | 85/573 [15:10<1:27:05, 10.71s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1608])
Epoch 2:  15%|‚ñà‚ñå        | 86/573 [15:18<1:26:39, 10.68s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  15%|‚ñà‚ñå        | 86/573 [15:18<1:26:39, 10.68s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7839 examples [5:04:48,  1.22s/ examples]embeddings shape: torch.Size([32, 768, 1533])
Epoch 2:  15%|‚ñà‚ñå        | 87/573 [15:36<1:27:09, 10.76s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  15%|‚ñà‚ñå        | 87/573 [15:36<1:27:09, 10.76s/it, loss=0.31, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1621])
Epoch 2:  15%|‚ñà‚ñå        | 88/573 [16:05<1:28:41, 10.97s/it, loss=0.31, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  15%|‚ñà‚ñå        | 88/573 [16:05<1:28:41, 10.97s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8110 examples [5:05:18,  2.32s/ examples]Generating train split: 6600 examples [5:04:22,  2.81s/ examples]embeddings shape: torch.Size([32, 768, 1264])
Epoch 2:  16%|‚ñà‚ñå        | 89/573 [16:26<1:29:24, 11.08s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  16%|‚ñà‚ñå        | 89/573 [16:26<1:29:24, 11.08s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1431])
Epoch 2:  16%|‚ñà‚ñå        | 90/573 [16:30<1:28:36, 11.01s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  16%|‚ñà‚ñå        | 90/573 [16:30<1:28:36, 11.01s/it, loss=0.306, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1596])
Epoch 2:  16%|‚ñà‚ñå        | 91/573 [16:38<1:28:08, 10.97s/it, loss=0.306, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  16%|‚ñà‚ñå        | 91/573 [16:38<1:28:08, 10.97s/it, loss=0.308, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1473])
Epoch 2:  16%|‚ñà‚ñå        | 92/573 [16:45<1:27:37, 10.93s/it, loss=0.308, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  16%|‚ñà‚ñå        | 92/573 [16:45<1:27:37, 10.93s/it, loss=0.31, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1233])
Epoch 2:  16%|‚ñà‚ñå        | 93/573 [16:54<1:27:17, 10.91s/it, loss=0.31, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  16%|‚ñà‚ñå        | 93/573 [16:54<1:27:17, 10.91s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7906 examples [5:06:19,  1.26s/ examples]embeddings shape: torch.Size([32, 768, 1623])
Epoch 2:  16%|‚ñà‚ñã        | 94/573 [17:13<1:27:44, 10.99s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  16%|‚ñà‚ñã        | 94/573 [17:13<1:27:44, 10.99s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1599])
Epoch 2:  17%|‚ñà‚ñã        | 95/573 [17:18<1:27:04, 10.93s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  17%|‚ñà‚ñã        | 95/573 [17:18<1:27:04, 10.93s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1671])
Epoch 2:  17%|‚ñà‚ñã        | 96/573 [17:25<1:26:32, 10.89s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  17%|‚ñà‚ñã        | 96/573 [17:25<1:26:32, 10.89s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1614])
Epoch 2:  17%|‚ñà‚ñã        | 97/573 [17:31<1:26:01, 10.84s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  17%|‚ñà‚ñã        | 97/573 [17:31<1:26:01, 10.84s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1151])
Epoch 2:  17%|‚ñà‚ñã        | 98/573 [17:39<1:25:33, 10.81s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  17%|‚ñà‚ñã        | 98/573 [17:39<1:25:33, 10.81s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1529])
Epoch 2:  17%|‚ñà‚ñã        | 99/573 [17:57<1:25:57, 10.88s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  17%|‚ñà‚ñã        | 99/573 [17:57<1:25:57, 10.88s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1585])
Epoch 2:  17%|‚ñà‚ñã        | 100/573 [18:23<1:26:58, 11.03s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  17%|‚ñà‚ñã        | 100/573 [18:23<1:26:58, 11.03s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7973 examples [5:07:39,  1.24s/ examples]Generating train split: 8170 examples [5:07:46,  2.36s/ examples]Generating train split: 6650 examples [5:06:55,  2.88s/ examples]embeddings shape: torch.Size([32, 768, 1628])
Epoch 2:  18%|‚ñà‚ñä        | 101/573 [18:57<1:28:37, 11.27s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  18%|‚ñà‚ñä        | 101/573 [18:57<1:28:37, 11.27s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1599])
Epoch 2:  18%|‚ñà‚ñä        | 102/573 [19:05<1:28:10, 11.23s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  18%|‚ñà‚ñä        | 102/573 [19:05<1:28:10, 11.23s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.816, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1582])
Epoch 2:  18%|‚ñà‚ñä        | 103/573 [19:12<1:27:36, 11.18s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.816, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  18%|‚ñà‚ñä        | 103/573 [19:12<1:27:36, 11.18s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1615])
Epoch 2:  18%|‚ñà‚ñä        | 104/573 [19:18<1:27:05, 11.14s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  18%|‚ñà‚ñä        | 104/573 [19:18<1:27:05, 11.14s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8040 examples [5:08:50,  1.19s/ examples]embeddings shape: torch.Size([32, 768, 1568])
Epoch 2:  18%|‚ñà‚ñä        | 105/573 [19:38<1:27:33, 11.23s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  18%|‚ñà‚ñä        | 105/573 [19:38<1:27:33, 11.23s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1582])
Epoch 2:  18%|‚ñà‚ñä        | 106/573 [19:47<1:27:11, 11.20s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  18%|‚ñà‚ñä        | 106/573 [19:47<1:27:11, 11.20s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1583])
Epoch 2:  19%|‚ñà‚ñä        | 107/573 [19:54<1:26:43, 11.17s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  19%|‚ñà‚ñä        | 107/573 [19:54<1:26:43, 11.17s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1504])
Epoch 2:  19%|‚ñà‚ñâ        | 108/573 [20:01<1:26:11, 11.12s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  19%|‚ñà‚ñâ        | 108/573 [20:01<1:26:11, 11.12s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1589])
Epoch 2:  19%|‚ñà‚ñâ        | 109/573 [20:10<1:25:54, 11.11s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  19%|‚ñà‚ñâ        | 109/573 [20:10<1:25:54, 11.11s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] Generating train split: 8107 examples [5:09:50,  1.10s/ examples]embeddings shape: torch.Size([32, 768, 1566])
Epoch 2:  19%|‚ñà‚ñâ        | 110/573 [20:39<1:26:58, 11.27s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  19%|‚ñà‚ñâ        | 110/573 [20:39<1:26:58, 11.27s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.906, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8230 examples [5:10:05,  2.35s/ examples]embeddings shape: torch.Size([32, 768, 1548])
Epoch 2:  19%|‚ñà‚ñâ        | 111/573 [21:11<1:28:12, 11.46s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.906, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  19%|‚ñà‚ñâ        | 111/573 [21:11<1:28:12, 11.46s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 6700 examples [5:09:14,  2.85s/ examples]embeddings shape: torch.Size([32, 768, 1195])
Epoch 2:  20%|‚ñà‚ñâ        | 112/573 [21:21<1:27:56, 11.45s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  20%|‚ñà‚ñâ        | 112/573 [21:21<1:27:56, 11.45s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1659])
Epoch 2:  20%|‚ñà‚ñâ        | 113/573 [21:28<1:27:25, 11.40s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  20%|‚ñà‚ñâ        | 113/573 [21:28<1:27:25, 11.40s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1585])
Epoch 2:  20%|‚ñà‚ñâ        | 114/573 [21:34<1:26:53, 11.36s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  20%|‚ñà‚ñâ        | 114/573 [21:34<1:26:53, 11.36s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.816, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8174 examples [5:11:02,  1.09s/ examples]embeddings shape: torch.Size([32, 768, 1624])
Epoch 2:  20%|‚ñà‚ñà        | 115/573 [21:51<1:27:01, 11.40s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.816, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  20%|‚ñà‚ñà        | 115/573 [21:51<1:27:01, 11.40s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1630])
Epoch 2:  20%|‚ñà‚ñà        | 116/573 [21:59<1:26:38, 11.38s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  20%|‚ñà‚ñà        | 116/573 [21:59<1:26:38, 11.38s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1604])
Epoch 2:  20%|‚ñà‚ñà        | 117/573 [22:06<1:26:08, 11.33s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  20%|‚ñà‚ñà        | 117/573 [22:06<1:26:08, 11.33s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1563])
Epoch 2:  21%|‚ñà‚ñà        | 118/573 [22:15<1:25:47, 11.31s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  21%|‚ñà‚ñà        | 118/573 [22:15<1:25:47, 11.31s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1637])
Epoch 2:  21%|‚ñà‚ñà        | 119/573 [22:21<1:25:19, 11.28s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  21%|‚ñà‚ñà        | 119/573 [22:21<1:25:19, 11.28s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1574])
Epoch 2:  21%|‚ñà‚ñà        | 120/573 [22:38<1:25:29, 11.32s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  21%|‚ñà‚ñà        | 120/573 [22:38<1:25:29, 11.32s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8241 examples [5:12:09,  1.07s/ examples]embeddings shape: torch.Size([32, 768, 1617])
Epoch 2:  21%|‚ñà‚ñà        | 121/573 [23:07<1:26:22, 11.47s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  21%|‚ñà‚ñà        | 121/573 [23:07<1:26:22, 11.47s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8290 examples [5:12:25,  2.34s/ examples]Generating train split: 6750 examples [5:11:28,  2.80s/ examples]embeddings shape: torch.Size([32, 768, 1645])
Epoch 2:  21%|‚ñà‚ñà‚ñè       | 122/573 [23:31<1:26:59, 11.57s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  21%|‚ñà‚ñà‚ñè       | 122/573 [23:31<1:26:59, 11.57s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1618])
Epoch 2:  21%|‚ñà‚ñà‚ñè       | 123/573 [23:41<1:26:40, 11.56s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  21%|‚ñà‚ñà‚ñè       | 123/573 [23:41<1:26:40, 11.56s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1664])
Epoch 2:  22%|‚ñà‚ñà‚ñè       | 124/573 [23:49<1:26:15, 11.53s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  22%|‚ñà‚ñà‚ñè       | 124/573 [23:49<1:26:15, 11.53s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.816, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1266])
Epoch 2:  22%|‚ñà‚ñà‚ñè       | 125/573 [23:59<1:25:58, 11.51s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.816, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  22%|‚ñà‚ñà‚ñè       | 125/573 [23:59<1:25:58, 11.51s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] Generating train split: 8308 examples [5:13:21,  1.07s/ examples]embeddings shape: torch.Size([32, 768, 1502])
Epoch 2:  22%|‚ñà‚ñà‚ñè       | 126/573 [24:13<1:25:58, 11.54s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  22%|‚ñà‚ñà‚ñè       | 126/573 [24:13<1:25:58, 11.54s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1585])
Epoch 2:  22%|‚ñà‚ñà‚ñè       | 127/573 [24:20<1:25:28, 11.50s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  22%|‚ñà‚ñà‚ñè       | 127/573 [24:20<1:25:28, 11.50s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1522])
Epoch 2:  22%|‚ñà‚ñà‚ñè       | 128/573 [24:27<1:25:00, 11.46s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  22%|‚ñà‚ñà‚ñè       | 128/573 [24:27<1:25:00, 11.46s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1602])
Epoch 2:  23%|‚ñà‚ñà‚ñé       | 129/573 [24:34<1:24:35, 11.43s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  23%|‚ñà‚ñà‚ñé       | 129/573 [24:34<1:24:35, 11.43s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1545])
Epoch 2:  23%|‚ñà‚ñà‚ñé       | 130/573 [24:42<1:24:13, 11.41s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  23%|‚ñà‚ñà‚ñé       | 130/573 [24:42<1:24:13, 11.41s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1247])
Epoch 2:  23%|‚ñà‚ñà‚ñé       | 131/573 [24:58<1:24:16, 11.44s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  23%|‚ñà‚ñà‚ñé       | 131/573 [24:58<1:24:16, 11.44s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1596])
Epoch 2:  23%|‚ñà‚ñà‚ñé       | 132/573 [25:30<1:25:13, 11.60s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  23%|‚ñà‚ñà‚ñé       | 132/573 [25:30<1:25:13, 11.60s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8375 examples [5:14:54,  1.16s/ examples]Generating train split: 8350 examples [5:14:47,  2.35s/ examples]Generating train split: 6800 examples [5:13:50,  2.81s/ examples]embeddings shape: torch.Size([32, 768, 1571])
Epoch 2:  23%|‚ñà‚ñà‚ñé       | 133/573 [25:54<1:25:41, 11.69s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  23%|‚ñà‚ñà‚ñé       | 133/573 [25:54<1:25:41, 11.69s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1578])
Epoch 2:  23%|‚ñà‚ñà‚ñé       | 134/573 [26:02<1:25:17, 11.66s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  23%|‚ñà‚ñà‚ñé       | 134/573 [26:02<1:25:17, 11.66s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1593])
Epoch 2:  24%|‚ñà‚ñà‚ñé       | 135/573 [26:11<1:24:57, 11.64s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  24%|‚ñà‚ñà‚ñé       | 135/573 [26:11<1:24:57, 11.64s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1341])
Epoch 2:  24%|‚ñà‚ñà‚ñé       | 136/573 [26:17<1:24:28, 11.60s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  24%|‚ñà‚ñà‚ñé       | 136/573 [26:17<1:24:28, 11.60s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1593])
Epoch 2:  24%|‚ñà‚ñà‚ñç       | 137/573 [26:24<1:24:03, 11.57s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  24%|‚ñà‚ñà‚ñç       | 137/573 [26:24<1:24:03, 11.57s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1577])
Epoch 2:  24%|‚ñà‚ñà‚ñç       | 138/573 [26:33<1:23:44, 11.55s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  24%|‚ñà‚ñà‚ñç       | 138/573 [26:33<1:23:44, 11.55s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1537])
Epoch 2:  24%|‚ñà‚ñà‚ñç       | 139/573 [26:41<1:23:21, 11.52s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  24%|‚ñà‚ñà‚ñç       | 139/573 [26:41<1:23:21, 11.52s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1614])
Epoch 2:  24%|‚ñà‚ñà‚ñç       | 140/573 [26:49<1:22:58, 11.50s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  24%|‚ñà‚ñà‚ñç       | 140/573 [26:49<1:22:58, 11.50s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1572])
Epoch 2:  25%|‚ñà‚ñà‚ñç       | 141/573 [26:56<1:22:31, 11.46s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  25%|‚ñà‚ñà‚ñç       | 141/573 [26:56<1:22:31, 11.46s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1563])
Epoch 2:  25%|‚ñà‚ñà‚ñç       | 142/573 [27:14<1:22:42, 11.51s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  25%|‚ñà‚ñà‚ñç       | 142/573 [27:14<1:22:42, 11.51s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8410 examples [5:16:40,  2.21s/ examples]embeddings shape: torch.Size([32, 768, 1613])
Epoch 2:  25%|‚ñà‚ñà‚ñç       | 143/573 [27:43<1:23:21, 11.63s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  25%|‚ñà‚ñà‚ñç       | 143/573 [27:43<1:23:21, 11.63s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 6850 examples [5:15:50,  2.69s/ examples]embeddings shape: torch.Size([32, 768, 1581])
Epoch 2:  25%|‚ñà‚ñà‚ñå       | 144/573 [27:58<1:23:19, 11.65s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  25%|‚ñà‚ñà‚ñå       | 144/573 [27:58<1:23:19, 11.65s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1596])
Epoch 2:  25%|‚ñà‚ñà‚ñå       | 145/573 [28:16<1:23:26, 11.70s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  25%|‚ñà‚ñà‚ñå       | 145/573 [28:16<1:23:26, 11.70s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1560])
Epoch 2:  25%|‚ñà‚ñà‚ñå       | 146/573 [28:46<1:24:08, 11.82s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  25%|‚ñà‚ñà‚ñå       | 146/573 [28:46<1:24:08, 11.82s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8442 examples [5:17:59,  1.65s/ examples]embeddings shape: torch.Size([32, 768, 1596])
Epoch 2:  26%|‚ñà‚ñà‚ñå       | 147/573 [28:55<1:23:48, 11.81s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  26%|‚ñà‚ñà‚ñå       | 147/573 [28:55<1:23:48, 11.81s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1545])
Epoch 2:  26%|‚ñà‚ñà‚ñå       | 148/573 [29:01<1:23:21, 11.77s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  26%|‚ñà‚ñà‚ñå       | 148/573 [29:01<1:23:21, 11.77s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1566])
Epoch 2:  26%|‚ñà‚ñà‚ñå       | 149/573 [29:09<1:22:57, 11.74s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  26%|‚ñà‚ñà‚ñå       | 149/573 [29:09<1:22:57, 11.74s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1484])
Epoch 2:  26%|‚ñà‚ñà‚ñå       | 150/573 [29:17<1:22:36, 11.72s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  26%|‚ñà‚ñà‚ñå       | 150/573 [29:17<1:22:36, 11.72s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1573])
Epoch 2:  26%|‚ñà‚ñà‚ñã       | 151/573 [29:23<1:22:08, 11.68s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  26%|‚ñà‚ñà‚ñã       | 151/573 [29:23<1:22:08, 11.68s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1560])
Epoch 2:  27%|‚ñà‚ñà‚ñã       | 152/573 [29:43<1:22:18, 11.73s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  27%|‚ñà‚ñà‚ñã       | 152/573 [29:43<1:22:18, 11.73s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.809, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8470 examples [5:19:04,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1474])
Epoch 2:  27%|‚ñà‚ñà‚ñã       | 153/573 [30:15<1:23:04, 11.87s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.809, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  27%|‚ñà‚ñà‚ñã       | 153/573 [30:15<1:23:04, 11.87s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 6900 examples [5:18:24,  2.81s/ examples]embeddings shape: torch.Size([32, 768, 1595])
Epoch 2:  27%|‚ñà‚ñà‚ñã       | 154/573 [30:33<1:23:07, 11.90s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  27%|‚ñà‚ñà‚ñã       | 154/573 [30:33<1:23:07, 11.90s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1418])
Epoch 2:  27%|‚ñà‚ñà‚ñã       | 155/573 [30:41<1:22:45, 11.88s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  27%|‚ñà‚ñà‚ñã       | 155/573 [30:41<1:22:45, 11.88s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1578])
Epoch 2:  27%|‚ñà‚ñà‚ñã       | 156/573 [30:49<1:22:23, 11.85s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  27%|‚ñà‚ñà‚ñã       | 156/573 [30:49<1:22:23, 11.85s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1570])
Epoch 2:  27%|‚ñà‚ñà‚ñã       | 157/573 [31:22<1:23:08, 11.99s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  27%|‚ñà‚ñà‚ñã       | 157/573 [31:22<1:23:08, 11.99s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8509 examples [5:20:46,  1.90s/ examples]embeddings shape: torch.Size([32, 768, 1594])
Epoch 2:  28%|‚ñà‚ñà‚ñä       | 158/573 [31:40<1:23:10, 12.03s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  28%|‚ñà‚ñà‚ñä       | 158/573 [31:40<1:23:10, 12.03s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1645])
Epoch 2:  28%|‚ñà‚ñà‚ñä       | 159/573 [31:49<1:22:52, 12.01s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  28%|‚ñà‚ñà‚ñä       | 159/573 [31:49<1:22:52, 12.01s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1622])
Epoch 2:  28%|‚ñà‚ñà‚ñä       | 160/573 [31:59<1:22:34, 12.00s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  28%|‚ñà‚ñà‚ñä       | 160/573 [31:59<1:22:34, 12.00s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1612])
Epoch 2:  28%|‚ñà‚ñà‚ñä       | 161/573 [32:36<1:23:26, 12.15s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  28%|‚ñà‚ñà‚ñä       | 161/573 [32:36<1:23:26, 12.15s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8530 examples [5:21:36,  2.35s/ examples]embeddings shape: torch.Size([32, 768, 1410])
Epoch 2:  28%|‚ñà‚ñà‚ñä       | 162/573 [33:09<1:24:06, 12.28s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  28%|‚ñà‚ñà‚ñä       | 162/573 [33:09<1:24:06, 12.28s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] Generating train split: 6950 examples [5:21:08,  2.95s/ examples]embeddings shape: torch.Size([32, 768, 1589])
Epoch 2:  28%|‚ñà‚ñà‚ñä       | 163/573 [33:19<1:23:48, 12.26s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  28%|‚ñà‚ñà‚ñä       | 163/573 [33:19<1:23:48, 12.26s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1577])
Epoch 2:  29%|‚ñà‚ñà‚ñä       | 164/573 [33:26<1:23:24, 12.24s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  29%|‚ñà‚ñà‚ñä       | 164/573 [33:26<1:23:24, 12.24s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1598])
Epoch 2:  29%|‚ñà‚ñà‚ñâ       | 165/573 [33:34<1:23:02, 12.21s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  29%|‚ñà‚ñà‚ñâ       | 165/573 [33:34<1:23:02, 12.21s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1466])
Epoch 2:  29%|‚ñà‚ñà‚ñâ       | 166/573 [33:43<1:22:40, 12.19s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  29%|‚ñà‚ñà‚ñâ       | 166/573 [33:43<1:22:40, 12.19s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.816, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1580])
Epoch 2:  29%|‚ñà‚ñà‚ñâ       | 167/573 [33:50<1:22:17, 12.16s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.816, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  29%|‚ñà‚ñà‚ñâ       | 167/573 [33:50<1:22:17, 12.16s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1570])
Epoch 2:  29%|‚ñà‚ñà‚ñâ       | 168/573 [34:12<1:22:27, 12.22s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  29%|‚ñà‚ñà‚ñâ       | 168/573 [34:12<1:22:27, 12.22s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8576 examples [5:23:43,  2.12s/ examples]embeddings shape: torch.Size([32, 768, 1624])
Epoch 2:  29%|‚ñà‚ñà‚ñâ       | 169/573 [34:37<1:22:46, 12.29s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  29%|‚ñà‚ñà‚ñâ       | 169/573 [34:37<1:22:46, 12.29s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1233])
Epoch 2:  30%|‚ñà‚ñà‚ñâ       | 170/573 [35:03<1:23:07, 12.38s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  30%|‚ñà‚ñà‚ñâ       | 170/573 [35:03<1:23:07, 12.38s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8590 examples [5:24:04,  2.38s/ examples]embeddings shape: torch.Size([32, 768, 1470])
Epoch 2:  30%|‚ñà‚ñà‚ñâ       | 171/573 [35:29<1:23:26, 12.46s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  30%|‚ñà‚ñà‚ñâ       | 171/573 [35:29<1:23:26, 12.46s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7000 examples [5:23:36,  2.95s/ examples]embeddings shape: torch.Size([32, 768, 1561])
Epoch 2:  30%|‚ñà‚ñà‚ñà       | 172/573 [35:43<1:23:18, 12.46s/it, loss=0.345, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  30%|‚ñà‚ñà‚ñà       | 172/573 [35:43<1:23:18, 12.46s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1594])
Epoch 2:  30%|‚ñà‚ñà‚ñà       | 173/573 [35:50<1:22:53, 12.43s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  30%|‚ñà‚ñà‚ñà       | 173/573 [35:50<1:22:53, 12.43s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1562])
Epoch 2:  30%|‚ñà‚ñà‚ñà       | 174/573 [35:57<1:22:28, 12.40s/it, loss=0.338, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  30%|‚ñà‚ñà‚ñà       | 174/573 [35:57<1:22:28, 12.40s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1608])
Epoch 2:  31%|‚ñà‚ñà‚ñà       | 175/573 [36:05<1:22:05, 12.38s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  31%|‚ñà‚ñà‚ñà       | 175/573 [36:05<1:22:05, 12.38s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1585])
Epoch 2:  31%|‚ñà‚ñà‚ñà       | 176/573 [36:14<1:21:44, 12.35s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  31%|‚ñà‚ñà‚ñà       | 176/573 [36:14<1:21:44, 12.35s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1632])
Epoch 2:  31%|‚ñà‚ñà‚ñà       | 177/573 [36:22<1:21:22, 12.33s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  31%|‚ñà‚ñà‚ñà       | 177/573 [36:22<1:21:22, 12.33s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1608])
Epoch 2:  31%|‚ñà‚ñà‚ñà       | 178/573 [36:31<1:21:02, 12.31s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  31%|‚ñà‚ñà‚ñà       | 178/573 [36:31<1:21:02, 12.31s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1600])
Epoch 2:  31%|‚ñà‚ñà‚ñà       | 179/573 [37:11<1:21:51, 12.47s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  31%|‚ñà‚ñà‚ñà       | 179/573 [37:11<1:21:51, 12.47s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8650 examples [5:26:24,  2.37s/ examples]Generating train split: 8643 examples [5:26:57,  2.35s/ examples]embeddings shape: torch.Size([32, 768, 1591])
Epoch 2:  31%|‚ñà‚ñà‚ñà‚ñè      | 180/573 [37:46<1:22:28, 12.59s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  31%|‚ñà‚ñà‚ñà‚ñè      | 180/573 [37:46<1:22:28, 12.59s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7050 examples [5:26:03,  2.95s/ examples]embeddings shape: torch.Size([32, 768, 1613])
Epoch 2:  32%|‚ñà‚ñà‚ñà‚ñè      | 181/573 [38:11<1:22:42, 12.66s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  32%|‚ñà‚ñà‚ñà‚ñè      | 181/573 [38:11<1:22:42, 12.66s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1596])
Epoch 2:  32%|‚ñà‚ñà‚ñà‚ñè      | 182/573 [38:18<1:22:18, 12.63s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  32%|‚ñà‚ñà‚ñà‚ñè      | 182/573 [38:18<1:22:18, 12.63s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1630])
Epoch 2:  32%|‚ñà‚ñà‚ñà‚ñè      | 183/573 [38:25<1:21:54, 12.60s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  32%|‚ñà‚ñà‚ñà‚ñè      | 183/573 [38:25<1:21:54, 12.60s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1560])
Epoch 2:  32%|‚ñà‚ñà‚ñà‚ñè      | 184/573 [38:34<1:21:34, 12.58s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  32%|‚ñà‚ñà‚ñà‚ñè      | 184/573 [38:34<1:21:34, 12.58s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1575])
Epoch 2:  32%|‚ñà‚ñà‚ñà‚ñè      | 185/573 [38:42<1:21:11, 12.56s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  32%|‚ñà‚ñà‚ñà‚ñè      | 185/573 [38:42<1:21:11, 12.56s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1643])
Epoch 2:  32%|‚ñà‚ñà‚ñà‚ñè      | 186/573 [38:50<1:20:49, 12.53s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  32%|‚ñà‚ñà‚ñà‚ñè      | 186/573 [38:50<1:20:49, 12.53s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1531])
Epoch 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 187/573 [38:59<1:20:28, 12.51s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 187/573 [38:59<1:20:28, 12.51s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1618])
Epoch 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 188/573 [39:32<1:20:58, 12.62s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 188/573 [39:32<1:20:58, 12.62s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] Generating train split: 8710 examples [5:28:30,  2.29s/ examples]embeddings shape: torch.Size([32, 768, 1616])
Epoch 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 189/573 [39:52<1:21:01, 12.66s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 189/573 [39:52<1:21:01, 12.66s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7100 examples [5:28:04,  2.79s/ examples]embeddings shape: torch.Size([32, 768, 1573])
Epoch 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 190/573 [40:13<1:21:04, 12.70s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 190/573 [40:13<1:21:04, 12.70s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1557])
Epoch 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 191/573 [40:20<1:20:40, 12.67s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 191/573 [40:20<1:20:40, 12.67s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1612])
Epoch 2:  34%|‚ñà‚ñà‚ñà‚ñé      | 192/573 [40:29<1:20:20, 12.65s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  34%|‚ñà‚ñà‚ñà‚ñé      | 192/573 [40:29<1:20:20, 12.65s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1618])
Epoch 2:  34%|‚ñà‚ñà‚ñà‚ñé      | 193/573 [40:35<1:19:56, 12.62s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  34%|‚ñà‚ñà‚ñà‚ñé      | 193/573 [40:35<1:19:56, 12.62s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1619])
Epoch 2:  34%|‚ñà‚ñà‚ñà‚ñç      | 194/573 [41:07<1:20:20, 12.72s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  34%|‚ñà‚ñà‚ñà‚ñç      | 194/573 [41:07<1:20:20, 12.72s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8710 examples [5:30:39,  2.64s/ examples]embeddings shape: torch.Size([32, 768, 1694])
Epoch 2:  34%|‚ñà‚ñà‚ñà‚ñç      | 195/573 [41:29<1:20:25, 12.77s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  34%|‚ñà‚ñà‚ñà‚ñç      | 195/573 [41:29<1:20:25, 12.77s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1578])
Epoch 2:  34%|‚ñà‚ñà‚ñà‚ñç      | 196/573 [41:42<1:20:13, 12.77s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  34%|‚ñà‚ñà‚ñà‚ñç      | 196/573 [41:42<1:20:13, 12.77s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8770 examples [5:31:02,  2.36s/ examples]embeddings shape: torch.Size([32, 768, 1590])
Epoch 2:  34%|‚ñà‚ñà‚ñà‚ñç      | 197/573 [42:08<1:20:25, 12.83s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  34%|‚ñà‚ñà‚ñà‚ñç      | 197/573 [42:08<1:20:25, 12.83s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1257])
Epoch 2:  35%|‚ñà‚ñà‚ñà‚ñç      | 198/573 [42:30<1:20:30, 12.88s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  35%|‚ñà‚ñà‚ñà‚ñç      | 198/573 [42:30<1:20:30, 12.88s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7150 examples [5:30:37,  2.87s/ examples]embeddings shape: torch.Size([32, 768, 1587])
Epoch 2:  35%|‚ñà‚ñà‚ñà‚ñç      | 199/573 [42:46<1:20:23, 12.90s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  35%|‚ñà‚ñà‚ñà‚ñç      | 199/573 [42:46<1:20:23, 12.90s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1578])
Epoch 2:  35%|‚ñà‚ñà‚ñà‚ñç      | 200/573 [42:53<1:20:00, 12.87s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  35%|‚ñà‚ñà‚ñà‚ñç      | 200/573 [42:53<1:20:00, 12.87s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.795, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1433])
Epoch 2:  35%|‚ñà‚ñà‚ñà‚ñå      | 201/573 [43:02<1:19:39, 12.85s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.795, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  35%|‚ñà‚ñà‚ñà‚ñå      | 201/573 [43:02<1:19:39, 12.85s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1649])
Epoch 2:  35%|‚ñà‚ñà‚ñà‚ñå      | 202/573 [43:11<1:19:18, 12.83s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  35%|‚ñà‚ñà‚ñà‚ñå      | 202/573 [43:11<1:19:18, 12.83s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1658])
Epoch 2:  35%|‚ñà‚ñà‚ñà‚ñå      | 203/573 [43:19<1:18:57, 12.81s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  35%|‚ñà‚ñà‚ñà‚ñå      | 203/573 [43:19<1:18:57, 12.81s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1624])
Epoch 2:  36%|‚ñà‚ñà‚ñà‚ñå      | 204/573 [43:27<1:18:36, 12.78s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  36%|‚ñà‚ñà‚ñà‚ñå      | 204/573 [43:27<1:18:36, 12.78s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1644])
Epoch 2:  36%|‚ñà‚ñà‚ñà‚ñå      | 205/573 [43:52<1:18:45, 12.84s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  36%|‚ñà‚ñà‚ñà‚ñå      | 205/573 [43:52<1:18:45, 12.84s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.809, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8830 examples [5:33:03,  2.26s/ examples]embeddings shape: torch.Size([32, 768, 1621])
Epoch 2:  36%|‚ñà‚ñà‚ñà‚ñå      | 206/573 [44:23<1:19:05, 12.93s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.809, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  36%|‚ñà‚ñà‚ñà‚ñå      | 206/573 [44:23<1:19:05, 12.93s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1644])
Epoch 2:  36%|‚ñà‚ñà‚ñà‚ñå      | 207/573 [44:53<1:19:21, 13.01s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  36%|‚ñà‚ñà‚ñà‚ñå      | 207/573 [44:53<1:19:21, 13.01s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8776 examples [5:34:22,  2.86s/ examples]Generating train split: 7200 examples [5:33:16,  2.96s/ examples]embeddings shape: torch.Size([32, 768, 1394])
Epoch 2:  36%|‚ñà‚ñà‚ñà‚ñã      | 208/573 [45:20<1:19:34, 13.08s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  36%|‚ñà‚ñà‚ñà‚ñã      | 208/573 [45:20<1:19:34, 13.08s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1590])
Epoch 2:  36%|‚ñà‚ñà‚ñà‚ñã      | 209/573 [45:28<1:19:12, 13.06s/it, loss=0.336, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  36%|‚ñà‚ñà‚ñà‚ñã      | 209/573 [45:28<1:19:12, 13.06s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.917, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1553])
Epoch 2:  37%|‚ñà‚ñà‚ñà‚ñã      | 210/573 [45:35<1:18:48, 13.03s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.917, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  37%|‚ñà‚ñà‚ñà‚ñã      | 210/573 [45:35<1:18:48, 13.03s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1635])
Epoch 2:  37%|‚ñà‚ñà‚ñà‚ñã      | 211/573 [45:46<1:18:32, 13.02s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  37%|‚ñà‚ñà‚ñà‚ñã      | 211/573 [45:46<1:18:32, 13.02s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1579])
Epoch 2:  37%|‚ñà‚ñà‚ñà‚ñã      | 212/573 [45:55<1:18:11, 13.00s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  37%|‚ñà‚ñà‚ñà‚ñã      | 212/573 [45:55<1:18:11, 13.00s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1623])
Epoch 2:  37%|‚ñà‚ñà‚ñà‚ñã      | 213/573 [46:13<1:18:07, 13.02s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  37%|‚ñà‚ñà‚ñà‚ñã      | 213/573 [46:13<1:18:07, 13.02s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8890 examples [5:35:32,  2.32s/ examples]embeddings shape: torch.Size([32, 768, 1525])
Epoch 2:  37%|‚ñà‚ñà‚ñà‚ñã      | 214/573 [46:38<1:18:14, 13.08s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  37%|‚ñà‚ñà‚ñà‚ñã      | 214/573 [46:38<1:18:14, 13.08s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1598])
Epoch 2:  38%|‚ñà‚ñà‚ñà‚ñä      | 215/573 [46:47<1:17:54, 13.06s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  38%|‚ñà‚ñà‚ñà‚ñä      | 215/573 [46:47<1:17:54, 13.06s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7250 examples [5:35:18,  2.81s/ examples]embeddings shape: torch.Size([32, 768, 1556])
Epoch 2:  38%|‚ñà‚ñà‚ñà‚ñä      | 216/573 [47:22<1:18:17, 13.16s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  38%|‚ñà‚ñà‚ñà‚ñä      | 216/573 [47:22<1:18:17, 13.16s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1569])
Epoch 2:  38%|‚ñà‚ñà‚ñà‚ñä      | 217/573 [47:30<1:17:56, 13.14s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  38%|‚ñà‚ñà‚ñà‚ñä      | 217/573 [47:30<1:17:56, 13.14s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1527])
Epoch 2:  38%|‚ñà‚ñà‚ñà‚ñä      | 218/573 [47:40<1:17:38, 13.12s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  38%|‚ñà‚ñà‚ñà‚ñä      | 218/573 [47:40<1:17:38, 13.12s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1602])
Epoch 2:  38%|‚ñà‚ñà‚ñà‚ñä      | 219/573 [47:54<1:17:26, 13.13s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  38%|‚ñà‚ñà‚ñà‚ñä      | 219/573 [47:54<1:17:26, 13.13s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1606])
Epoch 2:  38%|‚ñà‚ñà‚ñà‚ñä      | 220/573 [48:28<1:17:47, 13.22s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  38%|‚ñà‚ñà‚ñà‚ñä      | 220/573 [48:28<1:17:47, 13.22s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8842 examples [5:37:49,  2.94s/ examples]embeddings shape: torch.Size([32, 768, 1668])
Epoch 2:  39%|‚ñà‚ñà‚ñà‚ñä      | 221/573 [48:59<1:18:01, 13.30s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  39%|‚ñà‚ñà‚ñà‚ñä      | 221/573 [48:59<1:18:01, 13.30s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8950 examples [5:38:11,  2.42s/ examples]embeddings shape: torch.Size([32, 768, 1588])
Epoch 2:  39%|‚ñà‚ñà‚ñà‚ñä      | 222/573 [49:21<1:18:02, 13.34s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  39%|‚ñà‚ñà‚ñà‚ñä      | 222/573 [49:21<1:18:02, 13.34s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1581])
Epoch 2:  39%|‚ñà‚ñà‚ñà‚ñâ      | 223/573 [49:32<1:17:44, 13.33s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  39%|‚ñà‚ñà‚ñà‚ñâ      | 223/573 [49:32<1:17:44, 13.33s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7300 examples [5:38:02,  2.95s/ examples]embeddings shape: torch.Size([32, 768, 1607])
Epoch 2:  39%|‚ñà‚ñà‚ñà‚ñâ      | 224/573 [50:07<1:18:05, 13.43s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  39%|‚ñà‚ñà‚ñà‚ñâ      | 224/573 [50:07<1:18:05, 13.43s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1582])
Epoch 2:  39%|‚ñà‚ñà‚ñà‚ñâ      | 225/573 [50:15<1:17:44, 13.40s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  39%|‚ñà‚ñà‚ñà‚ñâ      | 225/573 [50:15<1:17:44, 13.40s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1609])
Epoch 2:  39%|‚ñà‚ñà‚ñà‚ñâ      | 226/573 [50:25<1:17:25, 13.39s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  39%|‚ñà‚ñà‚ñà‚ñâ      | 226/573 [50:25<1:17:25, 13.39s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1601])
Epoch 2:  40%|‚ñà‚ñà‚ñà‚ñâ      | 227/573 [50:33<1:17:03, 13.36s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  40%|‚ñà‚ñà‚ñà‚ñâ      | 227/573 [50:33<1:17:03, 13.36s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1568])
Epoch 2:  40%|‚ñà‚ñà‚ñà‚ñâ      | 228/573 [50:43<1:16:44, 13.35s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  40%|‚ñà‚ñà‚ñà‚ñâ      | 228/573 [50:43<1:16:44, 13.35s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9010 examples [5:40:20,  2.34s/ examples]embeddings shape: torch.Size([32, 768, 1605])
Epoch 2:  40%|‚ñà‚ñà‚ñà‚ñâ      | 229/573 [51:22<1:17:10, 13.46s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  40%|‚ñà‚ñà‚ñà‚ñâ      | 229/573 [51:22<1:17:10, 13.46s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8908 examples [5:41:03,  2.94s/ examples]embeddings shape: torch.Size([32, 768, 1570])
Epoch 2:  40%|‚ñà‚ñà‚ñà‚ñà      | 230/573 [51:55<1:17:25, 13.54s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  40%|‚ñà‚ñà‚ñà‚ñà      | 230/573 [51:55<1:17:25, 13.54s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1591])
Epoch 2:  40%|‚ñà‚ñà‚ñà‚ñà      | 231/573 [52:18<1:17:26, 13.59s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  40%|‚ñà‚ñà‚ñà‚ñà      | 231/573 [52:18<1:17:26, 13.59s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7350 examples [5:40:28,  2.94s/ examples]embeddings shape: torch.Size([32, 768, 1299])
Epoch 2:  40%|‚ñà‚ñà‚ñà‚ñà      | 232/573 [52:38<1:17:22, 13.61s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  40%|‚ñà‚ñà‚ñà‚ñà      | 232/573 [52:38<1:17:22, 13.61s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1329])
Epoch 2:  41%|‚ñà‚ñà‚ñà‚ñà      | 233/573 [52:46<1:17:00, 13.59s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  41%|‚ñà‚ñà‚ñà‚ñà      | 233/573 [52:46<1:17:00, 13.59s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1563])
Epoch 2:  41%|‚ñà‚ñà‚ñà‚ñà      | 234/573 [52:56<1:16:41, 13.57s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  41%|‚ñà‚ñà‚ñà‚ñà      | 234/573 [52:56<1:16:41, 13.57s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1605])
Epoch 2:  41%|‚ñà‚ñà‚ñà‚ñà      | 235/573 [53:05<1:16:22, 13.56s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  41%|‚ñà‚ñà‚ñà‚ñà      | 235/573 [53:05<1:16:22, 13.56s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1554])
Epoch 2:  41%|‚ñà‚ñà‚ñà‚ñà      | 236/573 [53:22<1:16:12, 13.57s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  41%|‚ñà‚ñà‚ñà‚ñà      | 236/573 [53:22<1:16:12, 13.57s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9070 examples [5:42:42,  2.35s/ examples]embeddings shape: torch.Size([32, 768, 1687])
Epoch 2:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 237/573 [53:49<1:16:18, 13.63s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 237/573 [53:49<1:16:18, 13.63s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1663])
Epoch 2:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 238/573 [53:58<1:15:58, 13.61s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 238/573 [53:58<1:15:58, 13.61s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1595])
Epoch 2:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 239/573 [54:30<1:16:10, 13.68s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 239/573 [54:30<1:16:10, 13.68s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7400 examples [5:42:40,  2.85s/ examples]embeddings shape: torch.Size([32, 768, 1591])
Epoch 2:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 240/573 [55:11<1:16:34, 13.80s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 240/573 [55:11<1:16:34, 13.80s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8974 examples [5:44:29,  2.99s/ examples]embeddings shape: torch.Size([32, 768, 1630])
Epoch 2:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 241/573 [55:24<1:16:19, 13.79s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 241/573 [55:24<1:16:19, 13.79s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1674])
Epoch 2:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 242/573 [55:32<1:15:57, 13.77s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 242/573 [55:32<1:15:57, 13.77s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1670])
Epoch 2:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 243/573 [55:41<1:15:37, 13.75s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 243/573 [55:41<1:15:37, 13.75s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1577])
Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 244/573 [56:06<1:15:39, 13.80s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 244/573 [56:06<1:15:39, 13.80s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9130 examples [5:45:14,  2.41s/ examples]embeddings shape: torch.Size([32, 768, 1432])
Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 245/573 [56:22<1:15:27, 13.80s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 245/573 [56:22<1:15:27, 13.80s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1600])
Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 246/573 [56:32<1:15:09, 13.79s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 246/573 [56:32<1:15:09, 13.79s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1589])
Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 247/573 [56:40<1:14:48, 13.77s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 247/573 [56:40<1:14:48, 13.77s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1268])
Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 248/573 [57:12<1:14:57, 13.84s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 248/573 [57:12<1:14:57, 13.84s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7450 examples [5:45:12,  2.91s/ examples]embeddings shape: torch.Size([32, 768, 1631])
Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 249/573 [57:25<1:14:43, 13.84s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 249/573 [57:25<1:14:43, 13.84s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9040 examples [5:47:08,  2.82s/ examples]embeddings shape: torch.Size([32, 768, 1555])
Epoch 2:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 250/573 [57:58<1:14:54, 13.91s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 250/573 [57:58<1:14:54, 13.91s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1580])
Epoch 2:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 251/573 [58:09<1:14:36, 13.90s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 251/573 [58:09<1:14:36, 13.90s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1576])
Epoch 2:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 252/573 [58:26<1:14:26, 13.92s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 252/573 [58:26<1:14:26, 13.92s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9190 examples [5:47:43,  2.43s/ examples]embeddings shape: torch.Size([32, 768, 1572])
Epoch 2:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 253/573 [58:51<1:14:26, 13.96s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 253/573 [58:51<1:14:26, 13.96s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1582])
Epoch 2:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 254/573 [59:00<1:14:06, 13.94s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 254/573 [59:00<1:14:06, 13.94s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1501])
Epoch 2:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 255/573 [59:06<1:13:43, 13.91s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 255/573 [59:06<1:13:43, 13.91s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1625])
Epoch 2:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 256/573 [59:32<1:13:43, 13.96s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 256/573 [59:32<1:13:43, 13.96s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7500 examples [5:47:40,  2.92s/ examples]embeddings shape: torch.Size([32, 768, 1587])
Epoch 2:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 257/573 [59:51<1:13:36, 13.97s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 257/573 [59:51<1:13:36, 13.97s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.816, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1434])
Epoch 2:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 258/573 [59:59<1:13:15, 13.95s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.816, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 258/573 [59:59<1:13:15, 13.95s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1694])
Epoch 2:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 259/573 [1:00:08<1:12:55, 13.93s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 259/573 [1:00:08<1:12:55, 13.93s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1616])
Epoch 2:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 260/573 [1:00:40<1:13:02, 14.00s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 260/573 [1:00:40<1:13:02, 14.00s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1601])
Epoch 2:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 261/573 [1:01:15<1:13:13, 14.08s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 261/573 [1:01:15<1:13:13, 14.08s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9250 examples [5:50:17,  2.47s/ examples]Generating train split: 9106 examples [5:50:39,  2.93s/ examples]embeddings shape: torch.Size([32, 768, 1609])
Epoch 2:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 262/573 [1:01:33<1:13:04, 14.10s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 262/573 [1:01:33<1:13:04, 14.10s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1616])
Epoch 2:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 263/573 [1:01:42<1:12:44, 14.08s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 263/573 [1:01:42<1:12:44, 14.08s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7550 examples [5:50:13,  2.97s/ examples]embeddings shape: torch.Size([32, 768, 1672])
Epoch 2:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 264/573 [1:02:17<1:12:54, 14.16s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 264/573 [1:02:17<1:12:54, 14.16s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1590])
Epoch 2:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 265/573 [1:02:26<1:12:33, 14.14s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 265/573 [1:02:26<1:12:33, 14.14s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1590])
Epoch 2:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 266/573 [1:02:33<1:12:12, 14.11s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 266/573 [1:02:33<1:12:12, 14.11s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1671])
Epoch 2:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 267/573 [1:02:42<1:11:51, 14.09s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 267/573 [1:02:42<1:11:51, 14.09s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1249])
Epoch 2:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 268/573 [1:02:53<1:11:34, 14.08s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 268/573 [1:02:53<1:11:34, 14.08s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9310 examples [5:52:16,  2.33s/ examples]embeddings shape: torch.Size([32, 768, 1650])
Epoch 2:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 269/573 [1:03:24<1:11:39, 14.14s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 269/573 [1:03:24<1:11:39, 14.14s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1597])
Epoch 2:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 270/573 [1:03:30<1:11:15, 14.11s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 270/573 [1:03:30<1:11:15, 14.11s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1630])
Epoch 2:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 271/573 [1:03:37<1:10:53, 14.09s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 271/573 [1:03:37<1:10:53, 14.09s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1598])
Epoch 2:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 272/573 [1:03:53<1:10:42, 14.09s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 272/573 [1:03:53<1:10:42, 14.09s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1560])
Epoch 2:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 273/573 [1:04:23<1:10:45, 14.15s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 273/573 [1:04:23<1:10:45, 14.15s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7600 examples [5:52:36,  2.93s/ examples]Generating train split: 9172 examples [5:54:05,  2.99s/ examples]embeddings shape: torch.Size([32, 768, 1620])
Epoch 2:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 274/573 [1:04:55<1:10:50, 14.22s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 274/573 [1:04:55<1:10:50, 14.22s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1635])
Epoch 2:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 275/573 [1:05:03<1:10:29, 14.19s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 275/573 [1:05:03<1:10:29, 14.19s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1337])
Epoch 2:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 276/573 [1:05:10<1:10:08, 14.17s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 276/573 [1:05:10<1:10:08, 14.17s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1533])
Epoch 2:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 277/573 [1:05:36<1:10:07, 14.21s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 277/573 [1:05:36<1:10:07, 14.21s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9370 examples [5:54:42,  2.36s/ examples]embeddings shape: torch.Size([32, 768, 1554])
Epoch 2:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 278/573 [1:05:54<1:09:55, 14.22s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 278/573 [1:05:54<1:09:55, 14.22s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1491])
Epoch 2:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 279/573 [1:06:03<1:09:37, 14.21s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 279/573 [1:06:03<1:09:37, 14.21s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1645])
Epoch 2:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 280/573 [1:06:13<1:09:18, 14.19s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 280/573 [1:06:13<1:09:18, 14.19s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1595])
Epoch 2:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 281/573 [1:06:36<1:09:12, 14.22s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 281/573 [1:06:36<1:09:12, 14.22s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7650 examples [5:54:46,  2.83s/ examples]embeddings shape: torch.Size([32, 768, 1587])
Epoch 2:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 282/573 [1:06:54<1:09:02, 14.24s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 282/573 [1:06:54<1:09:02, 14.24s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1579])
Epoch 2:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 283/573 [1:07:02<1:08:41, 14.21s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 283/573 [1:07:02<1:08:41, 14.21s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1572])
Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 284/573 [1:07:28<1:08:39, 14.26s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 284/573 [1:07:28<1:08:39, 14.26s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9238 examples [5:57:18,  2.97s/ examples]embeddings shape: torch.Size([32, 768, 1606])
Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 285/573 [1:08:07<1:08:50, 14.34s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 285/573 [1:08:07<1:08:50, 14.34s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9430 examples [5:57:12,  2.40s/ examples]embeddings shape: torch.Size([32, 768, 1561])
Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 286/573 [1:08:19<1:08:34, 14.33s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 286/573 [1:08:19<1:08:34, 14.33s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1567])
Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 287/573 [1:08:27<1:08:13, 14.31s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 287/573 [1:08:27<1:08:13, 14.31s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1590])
Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 288/573 [1:08:36<1:07:54, 14.29s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 288/573 [1:08:36<1:07:54, 14.29s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1617])
Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 289/573 [1:08:47<1:07:36, 14.28s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 289/573 [1:08:47<1:07:36, 14.28s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7700 examples [5:57:15,  2.88s/ examples]embeddings shape: torch.Size([32, 768, 1195])
Epoch 2:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 290/573 [1:09:18<1:07:37, 14.34s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 290/573 [1:09:18<1:07:37, 14.34s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1609])
Epoch 2:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 291/573 [1:09:27<1:07:18, 14.32s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 291/573 [1:09:27<1:07:18, 14.32s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1649])
Epoch 2:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 292/573 [1:09:35<1:06:58, 14.30s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 292/573 [1:09:35<1:06:58, 14.30s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1607])
Epoch 2:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 293/573 [1:09:45<1:06:40, 14.29s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 293/573 [1:09:45<1:06:40, 14.29s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9490 examples [5:59:12,  2.28s/ examples]embeddings shape: torch.Size([32, 768, 1581])
Epoch 2:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 294/573 [1:10:17<1:06:42, 14.35s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 294/573 [1:10:17<1:06:42, 14.35s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1570])
Epoch 2:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 295/573 [1:10:26<1:06:22, 14.33s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 295/573 [1:10:26<1:06:22, 14.33s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1599])
Epoch 2:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 296/573 [1:10:34<1:06:02, 14.31s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 296/573 [1:10:34<1:06:02, 14.31s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1685])
Epoch 2:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 297/573 [1:11:06<1:06:04, 14.36s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 297/573 [1:11:06<1:06:04, 14.36s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] Generating train split: 9304 examples [6:00:37,  2.99s/ examples]embeddings shape: torch.Size([32, 768, 1568])
Epoch 2:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 298/573 [1:11:41<1:06:09, 14.44s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 298/573 [1:11:41<1:06:09, 14.44s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7750 examples [5:59:47,  2.92s/ examples]embeddings shape: torch.Size([32, 768, 1563])
Epoch 2:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 299/573 [1:11:55<1:05:55, 14.43s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 299/573 [1:11:55<1:05:55, 14.43s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1607])
Epoch 2:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 300/573 [1:12:03<1:05:34, 14.41s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 300/573 [1:12:03<1:05:34, 14.41s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1581])
Epoch 2:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 301/573 [1:12:12<1:05:15, 14.39s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 301/573 [1:12:12<1:05:15, 14.39s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9550 examples [6:01:43,  2.35s/ examples]embeddings shape: torch.Size([32, 768, 1653])
Epoch 2:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 302/573 [1:12:45<1:05:17, 14.46s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 302/573 [1:12:45<1:05:17, 14.46s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1604])
Epoch 2:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 303/573 [1:12:55<1:04:58, 14.44s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 303/573 [1:12:55<1:04:58, 14.44s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1634])
Epoch 2:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 304/573 [1:13:04<1:04:39, 14.42s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 304/573 [1:13:04<1:04:39, 14.42s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1520])
Epoch 2:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 305/573 [1:13:12<1:04:19, 14.40s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 305/573 [1:13:12<1:04:19, 14.40s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1551])
Epoch 2:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 306/573 [1:13:21<1:04:00, 14.38s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 306/573 [1:13:21<1:04:00, 14.38s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7800 examples [6:01:47,  2.77s/ examples]embeddings shape: torch.Size([32, 768, 1631])
Epoch 2:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 307/573 [1:13:50<1:03:58, 14.43s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 307/573 [1:13:50<1:03:58, 14.43s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1579])
Epoch 2:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 308/573 [1:13:58<1:03:38, 14.41s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 308/573 [1:13:58<1:03:38, 14.41s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1628])
Epoch 2:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 309/573 [1:14:07<1:03:19, 14.39s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 309/573 [1:14:07<1:03:19, 14.39s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1617])
Epoch 2:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 310/573 [1:14:48<1:03:28, 14.48s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 310/573 [1:14:48<1:03:28, 14.48s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9610 examples [6:04:16,  2.41s/ examples]Generating train split: 9370 examples [6:04:33,  3.16s/ examples]embeddings shape: torch.Size([32, 768, 1598])
Epoch 2:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 311/573 [1:15:21<1:03:29, 14.54s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 311/573 [1:15:21<1:03:29, 14.54s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1619])
Epoch 2:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 312/573 [1:15:31<1:03:10, 14.52s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 312/573 [1:15:31<1:03:10, 14.52s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1612])
Epoch 2:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 313/573 [1:15:38<1:02:50, 14.50s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 313/573 [1:15:38<1:02:50, 14.50s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1439])
Epoch 2:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 314/573 [1:15:48<1:02:31, 14.48s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 314/573 [1:15:48<1:02:31, 14.48s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1563])
Epoch 2:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 315/573 [1:16:17<1:02:29, 14.53s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 315/573 [1:16:17<1:02:29, 14.53s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] Generating train split: 7850 examples [6:04:16,  2.83s/ examples]embeddings shape: torch.Size([32, 768, 1571])
Epoch 2:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 316/573 [1:16:25<1:02:09, 14.51s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 316/573 [1:16:25<1:02:09, 14.51s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1413])
Epoch 2:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 317/573 [1:16:33<1:01:49, 14.49s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 317/573 [1:16:33<1:01:49, 14.49s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1576])
Epoch 2:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 318/573 [1:16:41<1:01:29, 14.47s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 318/573 [1:16:41<1:01:29, 14.47s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1553])
Epoch 2:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 319/573 [1:17:03<1:01:21, 14.49s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 319/573 [1:17:03<1:01:21, 14.49s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9670 examples [6:06:13,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1582])
Epoch 2:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 320/573 [1:17:22<1:01:10, 14.51s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 320/573 [1:17:22<1:01:10, 14.51s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1592])
Epoch 2:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 321/573 [1:17:30<1:00:50, 14.49s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 321/573 [1:17:30<1:00:50, 14.49s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1590])
Epoch 2:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 322/573 [1:17:39<1:00:32, 14.47s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 322/573 [1:17:39<1:00:32, 14.47s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1625])
Epoch 2:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 323/573 [1:18:20<1:00:37, 14.55s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 323/573 [1:18:20<1:00:37, 14.55s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] Generating train split: 9436 examples [6:07:51,  3.11s/ examples]Generating train split: 7900 examples [6:06:59,  2.96s/ examples]embeddings shape: torch.Size([32, 768, 1565])
Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 324/573 [1:19:02<1:00:44, 14.64s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 324/573 [1:19:02<1:00:44, 14.64s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1560])
Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 325/573 [1:19:08<1:00:23, 14.61s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 325/573 [1:19:08<1:00:23, 14.61s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1572])
Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 326/573 [1:19:17<1:00:04, 14.59s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 326/573 [1:19:17<1:00:04, 14.59s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1631])
Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 327/573 [1:19:33<59:51, 14.60s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]  Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 327/573 [1:19:33<59:51, 14.60s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.656, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9730 examples [6:08:53,  2.39s/ examples]embeddings shape: torch.Size([32, 768, 1579])
Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 328/573 [1:19:57<59:43, 14.63s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.656, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 328/573 [1:19:57<59:43, 14.63s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1593])
Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 329/573 [1:20:06<59:24, 14.61s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 329/573 [1:20:06<59:24, 14.61s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1601])
Epoch 2:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 330/573 [1:20:13<59:04, 14.59s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 330/573 [1:20:13<59:04, 14.59s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1651])
Epoch 2:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 331/573 [1:20:21<58:45, 14.57s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 331/573 [1:20:21<58:45, 14.57s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1576])
Epoch 2:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 332/573 [1:20:44<58:36, 14.59s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 332/573 [1:20:44<58:36, 14.59s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 7950 examples [6:08:56,  2.77s/ examples]embeddings shape: torch.Size([32, 768, 1570])
Epoch 2:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 333/573 [1:21:04<58:26, 14.61s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 333/573 [1:21:04<58:26, 14.61s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1585])
Epoch 2:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 334/573 [1:21:13<58:07, 14.59s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 334/573 [1:21:13<58:07, 14.59s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1550])
Epoch 2:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 335/573 [1:21:36<57:58, 14.62s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 335/573 [1:21:36<57:58, 14.62s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9790 examples [6:10:56,  2.29s/ examples]embeddings shape: torch.Size([32, 768, 1618])
Epoch 2:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 336/573 [1:22:17<58:02, 14.70s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 336/573 [1:22:17<58:02, 14.70s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9502 examples [6:11:46,  3.25s/ examples]embeddings shape: torch.Size([32, 768, 1449])
Epoch 2:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 337/573 [1:22:36<57:51, 14.71s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 337/573 [1:22:36<57:51, 14.71s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1595])
Epoch 2:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 338/573 [1:22:43<57:30, 14.68s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 338/573 [1:22:43<57:30, 14.68s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1608])
Epoch 2:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 339/573 [1:22:51<57:11, 14.66s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 339/573 [1:22:51<57:11, 14.66s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1594])
Epoch 2:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 340/573 [1:22:58<56:51, 14.64s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 340/573 [1:22:58<56:51, 14.64s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8000 examples [6:11:25,  2.83s/ examples]embeddings shape: torch.Size([32, 768, 1638])
Epoch 2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 341/573 [1:23:30<56:48, 14.69s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 341/573 [1:23:30<56:48, 14.69s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1621])
Epoch 2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 342/573 [1:23:38<56:29, 14.67s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 342/573 [1:23:38<56:29, 14.67s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1250])
Epoch 2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 343/573 [1:23:46<56:10, 14.65s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 343/573 [1:23:46<56:10, 14.65s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1597])
Epoch 2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 344/573 [1:24:12<56:03, 14.69s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 344/573 [1:24:12<56:03, 14.69s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9850 examples [6:13:18,  2.31s/ examples]embeddings shape: torch.Size([32, 768, 1553])
Epoch 2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 345/573 [1:24:26<55:48, 14.69s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 345/573 [1:24:26<55:48, 14.69s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1622])
Epoch 2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 346/573 [1:24:34<55:29, 14.67s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 346/573 [1:24:34<55:29, 14.67s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1657])
Epoch 2:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 347/573 [1:24:49<55:14, 14.67s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 347/573 [1:24:49<55:14, 14.67s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9568 examples [6:14:28,  3.01s/ examples]embeddings shape: torch.Size([32, 768, 1541])
Epoch 2:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 348/573 [1:25:18<55:09, 14.71s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 348/573 [1:25:18<55:09, 14.71s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1562])
Epoch 2:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 349/573 [1:25:43<55:01, 14.74s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 349/573 [1:25:43<55:01, 14.74s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8050 examples [6:13:52,  2.87s/ examples]embeddings shape: torch.Size([32, 768, 1565])
Epoch 2:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 350/573 [1:26:00<54:48, 14.75s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 350/573 [1:26:00<54:48, 14.75s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1583])
Epoch 2:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 351/573 [1:26:07<54:28, 14.72s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 351/573 [1:26:07<54:28, 14.72s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1568])
Epoch 2:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 352/573 [1:26:25<54:15, 14.73s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 352/573 [1:26:25<54:15, 14.73s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] Generating train split: 9910 examples [6:15:40,  2.33s/ examples]embeddings shape: torch.Size([32, 768, 1565])
Epoch 2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 353/573 [1:26:46<54:05, 14.75s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 353/573 [1:26:46<54:05, 14.75s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1535])
Epoch 2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 354/573 [1:26:54<53:45, 14.73s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 354/573 [1:26:54<53:45, 14.73s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1605])
Epoch 2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 355/573 [1:27:03<53:27, 14.71s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 355/573 [1:27:03<53:27, 14.71s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1591])
Epoch 2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 356/573 [1:27:10<53:08, 14.69s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 356/573 [1:27:10<53:08, 14.69s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1591])
Epoch 2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 357/573 [1:27:23<52:52, 14.69s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 357/573 [1:27:23<52:52, 14.69s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8100 examples [6:15:47,  2.70s/ examples]embeddings shape: torch.Size([32, 768, 1313])
Epoch 2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 358/573 [1:27:51<52:45, 14.72s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 358/573 [1:27:51<52:45, 14.72s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1552])
Epoch 2:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 359/573 [1:28:26<52:43, 14.78s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 359/573 [1:28:26<52:43, 14.78s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9634 examples [6:17:46,  3.01s/ examples]embeddings shape: torch.Size([32, 768, 1553])
Epoch 2:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 360/573 [1:28:44<52:30, 14.79s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 360/573 [1:28:44<52:30, 14.79s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9970 examples [6:18:08,  2.37s/ examples]embeddings shape: torch.Size([32, 768, 1558])
Epoch 2:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 361/573 [1:29:12<52:23, 14.83s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 361/573 [1:29:12<52:23, 14.83s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1593])
Epoch 2:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 362/573 [1:29:20<52:04, 14.81s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 362/573 [1:29:20<52:04, 14.81s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1571])
Epoch 2:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 363/573 [1:29:30<51:46, 14.79s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 363/573 [1:29:30<51:46, 14.79s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1555])
Epoch 2:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 364/573 [1:29:37<51:27, 14.77s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 364/573 [1:29:37<51:27, 14.77s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1600])
Epoch 2:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 365/573 [1:29:45<51:08, 14.75s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 365/573 [1:29:45<51:08, 14.75s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1683])
Epoch 2:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 366/573 [1:30:16<51:03, 14.80s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 366/573 [1:30:16<51:03, 14.80s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8150 examples [6:18:17,  2.79s/ examples]embeddings shape: torch.Size([32, 768, 1614])
Epoch 2:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 367/573 [1:30:27<50:46, 14.79s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 367/573 [1:30:27<50:46, 14.79s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1570])
Epoch 2:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 368/573 [1:30:35<50:28, 14.77s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 368/573 [1:30:35<50:28, 14.77s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10030 examples [6:20:03,  2.23s/ examples]embeddings shape: torch.Size([32, 768, 1660])
Epoch 2:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 369/573 [1:31:08<50:23, 14.82s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 369/573 [1:31:08<50:23, 14.82s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1536])
Epoch 2:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 370/573 [1:31:17<50:05, 14.80s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 370/573 [1:31:17<50:05, 14.80s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1586])
Epoch 2:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 371/573 [1:31:27<49:48, 14.79s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 371/573 [1:31:27<49:48, 14.79s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1621])
Epoch 2:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 372/573 [1:32:07<49:46, 14.86s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 372/573 [1:32:07<49:46, 14.86s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9700 examples [6:21:22,  3.08s/ examples]embeddings shape: torch.Size([32, 768, 1568])
Epoch 2:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 373/573 [1:32:17<49:29, 14.85s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 373/573 [1:32:17<49:29, 14.85s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1636])
Epoch 2:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 374/573 [1:32:41<49:19, 14.87s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 374/573 [1:32:41<49:19, 14.87s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8200 examples [6:20:53,  2.89s/ examples]embeddings shape: torch.Size([32, 768, 1584])
Epoch 2:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 375/573 [1:33:01<49:07, 14.89s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 375/573 [1:33:01<49:07, 14.89s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1585])
Epoch 2:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 376/573 [1:33:12<48:50, 14.87s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 376/573 [1:33:12<48:50, 14.87s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10090 examples [6:22:38,  2.34s/ examples]embeddings shape: torch.Size([32, 768, 1577])
Epoch 2:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 377/573 [1:33:42<48:42, 14.91s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 377/573 [1:33:42<48:42, 14.91s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1594])
Epoch 2:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 378/573 [1:33:50<48:24, 14.90s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 378/573 [1:33:50<48:24, 14.90s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1588])
Epoch 2:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 379/573 [1:33:59<48:06, 14.88s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 379/573 [1:33:59<48:06, 14.88s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1599])
Epoch 2:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 380/573 [1:34:06<47:47, 14.86s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.0312, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 380/573 [1:34:06<47:47, 14.86s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.656, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1323])
Epoch 2:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 381/573 [1:34:19<47:31, 14.85s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.656, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 381/573 [1:34:19<47:31, 14.85s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] Generating train split: 9766 examples [6:24:01,  2.88s/ examples]embeddings shape: torch.Size([32, 768, 1580])
Epoch 2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 382/573 [1:34:52<47:26, 14.90s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 382/573 [1:34:52<47:26, 14.90s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8250 examples [6:23:18,  2.89s/ examples]embeddings shape: torch.Size([32, 768, 1609])
Epoch 2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 383/573 [1:35:22<47:18, 14.94s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 383/573 [1:35:22<47:18, 14.94s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1608])
Epoch 2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 384/573 [1:35:29<47:00, 14.92s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 384/573 [1:35:29<47:00, 14.92s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1291])
Epoch 2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 385/573 [1:36:01<46:53, 14.96s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 385/573 [1:36:01<46:53, 14.96s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10150 examples [6:25:02,  2.36s/ examples]embeddings shape: torch.Size([32, 768, 1576])
Epoch 2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 386/573 [1:36:13<46:36, 14.96s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 386/573 [1:36:13<46:36, 14.96s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1644])
Epoch 2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 387/573 [1:36:19<46:17, 14.93s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 387/573 [1:36:19<46:17, 14.93s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1578])
Epoch 2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 388/573 [1:36:26<45:58, 14.91s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 388/573 [1:36:26<45:58, 14.91s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.781, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1650])
Epoch 2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 389/573 [1:36:35<45:41, 14.90s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.781, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 389/573 [1:36:35<45:41, 14.90s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1588])
Epoch 2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 390/573 [1:36:49<45:25, 14.90s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 390/573 [1:36:49<45:25, 14.90s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9832 examples [6:26:31,  2.70s/ examples]embeddings shape: torch.Size([32, 768, 1687])
Epoch 2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 391/573 [1:37:19<45:18, 14.93s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 391/573 [1:37:19<45:18, 14.93s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8300 examples [6:25:39,  2.87s/ examples]embeddings shape: torch.Size([32, 768, 1610])
Epoch 2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 392/573 [1:37:45<45:08, 14.96s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 392/573 [1:37:45<45:08, 14.96s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1483])
Epoch 2:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 393/573 [1:38:01<44:53, 14.96s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 393/573 [1:38:01<44:53, 14.96s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10210 examples [6:27:18,  2.33s/ examples]embeddings shape: torch.Size([32, 768, 1569])
Epoch 2:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 394/573 [1:38:26<44:43, 14.99s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 394/573 [1:38:26<44:43, 14.99s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1619])
Epoch 2:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 395/573 [1:38:33<44:24, 14.97s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 395/573 [1:38:33<44:24, 14.97s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.656, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1568])
Epoch 2:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 396/573 [1:38:40<44:06, 14.95s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.656, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 396/573 [1:38:40<44:06, 14.95s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1571])
Epoch 2:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 397/573 [1:38:47<43:48, 14.93s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 397/573 [1:38:47<43:48, 14.93s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1162])
Epoch 2:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 398/573 [1:38:54<43:29, 14.91s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 398/573 [1:38:54<43:29, 14.91s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1671])
Epoch 2:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 399/573 [1:39:06<43:13, 14.90s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 399/573 [1:39:06<43:13, 14.90s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9898 examples [6:28:47,  2.51s/ examples]embeddings shape: torch.Size([32, 768, 1610])
Epoch 2:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 400/573 [1:39:40<43:06, 14.95s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 400/573 [1:39:40<43:06, 14.95s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8350 examples [6:28:03,  2.88s/ examples]embeddings shape: torch.Size([32, 768, 1518])
Epoch 2:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 401/573 [1:40:09<42:57, 14.99s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 401/573 [1:40:09<42:57, 14.99s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1583])
Epoch 2:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 402/573 [1:40:39<42:49, 15.02s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 402/573 [1:40:39<42:49, 15.02s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10270 examples [6:29:42,  2.35s/ examples]embeddings shape: torch.Size([32, 768, 1565])
Epoch 2:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 403/573 [1:40:51<42:32, 15.02s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 403/573 [1:40:51<42:32, 15.02s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1586])
Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 404/573 [1:40:59<42:14, 15.00s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 404/573 [1:40:59<42:14, 15.00s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1644])
Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 405/573 [1:41:08<41:57, 14.98s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 405/573 [1:41:08<41:57, 14.98s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1573])
Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 406/573 [1:41:18<41:40, 14.97s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 406/573 [1:41:18<41:40, 14.97s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1628])
Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 407/573 [1:41:29<41:23, 14.96s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 407/573 [1:41:29<41:23, 14.96s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9964 examples [6:31:08,  2.40s/ examples]embeddings shape: torch.Size([32, 768, 1364])
Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 408/573 [1:41:57<41:14, 14.99s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 408/573 [1:41:57<41:14, 14.99s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8400 examples [6:30:26,  2.87s/ examples]embeddings shape: torch.Size([32, 768, 1577])
Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 409/573 [1:42:30<41:06, 15.04s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 409/573 [1:42:30<41:06, 15.04s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10330 examples [6:32:01,  2.34s/ examples]embeddings shape: torch.Size([32, 768, 1602])
Epoch 2:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 410/573 [1:43:04<40:58, 15.08s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 410/573 [1:43:04<40:58, 15.08s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1611])
Epoch 2:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 411/573 [1:43:14<40:41, 15.07s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 411/573 [1:43:14<40:41, 15.07s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1654])
Epoch 2:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 412/573 [1:43:23<40:24, 15.06s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 412/573 [1:43:23<40:24, 15.06s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1647])
Epoch 2:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 413/573 [1:43:33<40:07, 15.04s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 413/573 [1:43:33<40:07, 15.04s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1659])
Epoch 2:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 414/573 [1:43:53<39:54, 15.06s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 414/573 [1:43:53<39:54, 15.06s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10030 examples [6:33:17,  2.26s/ examples]embeddings shape: torch.Size([32, 768, 1559])
Epoch 2:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 415/573 [1:44:12<39:40, 15.07s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 415/573 [1:44:12<39:40, 15.07s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1606])
Epoch 2:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 416/573 [1:44:26<39:24, 15.06s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 416/573 [1:44:26<39:24, 15.06s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8450 examples [6:32:46,  2.85s/ examples]embeddings shape: torch.Size([32, 768, 1578])
Epoch 2:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 417/573 [1:44:51<39:13, 15.09s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 417/573 [1:44:51<39:13, 15.09s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10390 examples [6:34:21,  2.34s/ examples]embeddings shape: torch.Size([32, 768, 1583])
Epoch 2:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 418/573 [1:45:24<39:05, 15.13s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 418/573 [1:45:24<39:05, 15.13s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1552])
Epoch 2:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 419/573 [1:45:32<38:47, 15.11s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 419/573 [1:45:32<38:47, 15.11s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1613])
Epoch 2:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 420/573 [1:45:44<38:31, 15.10s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 420/573 [1:45:44<38:31, 15.10s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10096 examples [6:35:10,  2.10s/ examples]embeddings shape: torch.Size([32, 768, 1595])
Epoch 2:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 421/573 [1:46:03<38:17, 15.12s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 421/573 [1:46:03<38:17, 15.12s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1209])
Epoch 2:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 422/573 [1:46:12<38:00, 15.10s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 422/573 [1:46:12<38:00, 15.10s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1555])
Epoch 2:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 423/573 [1:46:19<37:42, 15.08s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 423/573 [1:46:19<37:42, 15.08s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1591])
Epoch 2:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 424/573 [1:46:31<37:25, 15.07s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 424/573 [1:46:31<37:25, 15.07s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8500 examples [6:34:57,  2.78s/ examples]embeddings shape: torch.Size([32, 768, 1568])
Epoch 2:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 425/573 [1:47:00<37:15, 15.11s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 425/573 [1:47:00<37:15, 15.11s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1622])
Epoch 2:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 426/573 [1:47:33<37:06, 15.15s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 426/573 [1:47:33<37:06, 15.15s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10450 examples [6:36:33,  2.30s/ examples]Generating train split: 10162 examples [6:37:14,  2.03s/ examples]embeddings shape: torch.Size([32, 768, 1653])
Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 427/573 [1:48:04<36:57, 15.19s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 427/573 [1:48:04<36:57, 15.19s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1511])
Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 428/573 [1:48:12<36:39, 15.17s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 428/573 [1:48:12<36:39, 15.17s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1674])
Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 429/573 [1:48:22<36:22, 15.16s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 429/573 [1:48:22<36:22, 15.16s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1560])
Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 430/573 [1:48:31<36:05, 15.14s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 430/573 [1:48:31<36:05, 15.14s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1424])
Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 431/573 [1:48:39<35:48, 15.13s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 431/573 [1:48:39<35:48, 15.13s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1647])
Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 432/573 [1:48:48<35:30, 15.11s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 432/573 [1:48:48<35:30, 15.11s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8550 examples [6:37:20,  2.80s/ examples]embeddings shape: torch.Size([32, 768, 1575])
Epoch 2:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 433/573 [1:49:24<35:22, 15.16s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 433/573 [1:49:24<35:22, 15.16s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1563])
Epoch 2:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 434/573 [1:50:03<35:15, 15.22s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 434/573 [1:50:03<35:15, 15.22s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] Generating train split: 10228 examples [6:39:30,  2.04s/ examples]Generating train split: 10510 examples [6:39:25,  2.47s/ examples]embeddings shape: torch.Size([32, 768, 1588])
Epoch 2:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 435/573 [1:50:30<35:03, 15.24s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 435/573 [1:50:30<35:03, 15.24s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1556])
Epoch 2:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 436/573 [1:50:39<34:46, 15.23s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 436/573 [1:50:39<34:46, 15.23s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1619])
Epoch 2:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 437/573 [1:50:46<34:28, 15.21s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 437/573 [1:50:46<34:28, 15.21s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1568])
Epoch 2:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 438/573 [1:50:54<34:11, 15.19s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 438/573 [1:50:54<34:11, 15.19s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1660])
Epoch 2:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 439/573 [1:51:03<33:53, 15.18s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 439/573 [1:51:03<33:53, 15.18s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1551])
Epoch 2:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 440/573 [1:51:11<33:36, 15.16s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 440/573 [1:51:11<33:36, 15.16s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1458])
Epoch 2:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 441/573 [1:51:24<33:20, 15.16s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 441/573 [1:51:24<33:20, 15.16s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8600 examples [6:39:45,  2.83s/ examples]embeddings shape: torch.Size([32, 768, 1588])
Epoch 2:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 442/573 [1:51:56<33:10, 15.20s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 442/573 [1:51:56<33:10, 15.20s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10294 examples [6:41:32,  1.98s/ examples]embeddings shape: torch.Size([32, 768, 1620])
Epoch 2:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 443/573 [1:52:33<33:01, 15.24s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 443/573 [1:52:33<33:01, 15.24s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10570 examples [6:41:47,  2.44s/ examples]embeddings shape: torch.Size([32, 768, 1598])
Epoch 2:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 444/573 [1:52:52<32:47, 15.25s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 444/573 [1:52:52<32:47, 15.25s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1616])
Epoch 2:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 445/573 [1:52:58<32:29, 15.23s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 445/573 [1:52:58<32:29, 15.23s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1624])
Epoch 2:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 446/573 [1:53:08<32:12, 15.22s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 446/573 [1:53:08<32:12, 15.22s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1595])
Epoch 2:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 447/573 [1:53:17<31:56, 15.21s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 447/573 [1:53:17<31:56, 15.21s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860] embeddings shape: torch.Size([32, 768, 1556])
Epoch 2:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 448/573 [1:53:27<31:39, 15.20s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 448/573 [1:53:27<31:39, 15.20s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10360 examples [6:42:56,  1.77s/ examples]embeddings shape: torch.Size([32, 768, 1592])
Epoch 2:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 449/573 [1:53:51<31:26, 15.21s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 449/573 [1:53:51<31:26, 15.21s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1573])
Epoch 2:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 450/573 [1:54:14<31:13, 15.23s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 450/573 [1:54:14<31:13, 15.23s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8650 examples [6:42:21,  2.92s/ examples]embeddings shape: torch.Size([32, 768, 1586])
Epoch 2:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 451/573 [1:54:29<30:58, 15.23s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 451/573 [1:54:29<30:58, 15.23s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1580])
Epoch 2:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 452/573 [1:54:57<30:46, 15.26s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 452/573 [1:54:57<30:46, 15.26s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10630 examples [6:43:59,  2.36s/ examples]embeddings shape: torch.Size([32, 768, 1655])
Epoch 2:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 453/573 [1:55:09<30:30, 15.25s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 453/573 [1:55:09<30:30, 15.25s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10426 examples [6:44:44,  1.73s/ examples]embeddings shape: torch.Size([32, 768, 1568])
Epoch 2:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 454/573 [1:55:33<30:17, 15.27s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 454/573 [1:55:33<30:17, 15.27s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1564])
Epoch 2:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 455/573 [1:55:42<30:00, 15.26s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 455/573 [1:55:42<30:00, 15.26s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1279])
Epoch 2:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 456/573 [1:55:49<29:43, 15.24s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 456/573 [1:55:49<29:43, 15.24s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1598])
Epoch 2:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 457/573 [1:55:57<29:25, 15.22s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 457/573 [1:55:57<29:25, 15.22s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([26, 768, 1572])
Epoch 2:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 458/573 [1:56:04<29:08, 15.21s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 458/573 [1:56:04<29:08, 15.21s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]
Validation: 0it [00:00, ?it/s][A
Validation:   0%|          | 0/115 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/115 [00:00<?, ?it/s][Aembeddings shape: torch.Size([32, 768, 1569])

Validation DataLoader 0:   1%|          | 1/115 [00:00<00:00, 173.06it/s][AEpoch 2:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 459/573 [1:56:21<28:53, 15.21s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10492 examples [6:45:48,  1.50s/ examples]Generating train split: 8700 examples [6:44:51,  2.94s/ examples]embeddings shape: torch.Size([32, 768, 1635])

Validation DataLoader 0:   2%|‚ñè         | 2/115 [00:41<38:51, 20.63s/it] [AEpoch 2:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 460/573 [1:57:02<28:45, 15.27s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10690 examples [6:46:22,  2.37s/ examples]embeddings shape: torch.Size([32, 768, 1614])

Validation DataLoader 0:   3%|‚ñé         | 3/115 [01:08<42:21, 22.69s/it][AEpoch 2:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 461/573 [1:57:29<28:32, 15.29s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1551])

Validation DataLoader 0:   3%|‚ñé         | 4/115 [01:16<35:15, 19.06s/it][AEpoch 2:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 462/573 [1:57:37<28:15, 15.28s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1601])

Validation DataLoader 0:   4%|‚ñç         | 5/115 [01:24<30:59, 16.91s/it][AEpoch 2:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 463/573 [1:57:46<27:58, 15.26s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10558 examples [6:47:17,  1.46s/ examples]embeddings shape: torch.Size([32, 768, 1631])

Validation DataLoader 0:   5%|‚ñå         | 6/115 [01:45<31:51, 17.54s/it][AEpoch 2:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 464/573 [1:58:06<27:44, 15.27s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1677])

Validation DataLoader 0:   6%|‚ñå         | 7/115 [01:51<28:40, 15.93s/it][AEpoch 2:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 465/573 [1:58:13<27:27, 15.25s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1594])

Validation DataLoader 0:   7%|‚ñã         | 8/115 [01:57<26:13, 14.71s/it][AEpoch 2:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 466/573 [1:58:19<27:10, 15.23s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1383])

Validation DataLoader 0:   8%|‚ñä         | 9/115 [02:05<24:42, 13.98s/it][AEpoch 2:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 467/573 [1:58:27<26:53, 15.22s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1634])

Validation DataLoader 0:   9%|‚ñä         | 10/115 [02:13<23:16, 13.30s/it][AEpoch 2:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 468/573 [1:58:34<26:36, 15.20s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1573])

Validation DataLoader 0:  10%|‚ñâ         | 11/115 [02:43<25:41, 14.82s/it][AEpoch 2:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 469/573 [1:59:04<26:24, 15.23s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8750 examples [6:47:19,  2.95s/ examples]embeddings shape: torch.Size([32, 768, 1607])

Validation DataLoader 0:  10%|‚ñà         | 12/115 [03:20<28:44, 16.74s/it][AEpoch 2:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 470/573 [1:59:42<26:14, 15.28s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10750 examples [6:48:43,  2.37s/ examples]embeddings shape: torch.Size([32, 768, 1446])

Validation DataLoader 0:  11%|‚ñà‚ñè        | 13/115 [03:30<27:29, 16.17s/it][AEpoch 2:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 471/573 [1:59:51<25:57, 15.27s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1323])

Validation DataLoader 0:  12%|‚ñà‚ñè        | 14/115 [03:36<26:02, 15.47s/it][AEpoch 2:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 472/573 [1:59:58<25:40, 15.25s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1219])

Validation DataLoader 0:  13%|‚ñà‚ñé        | 15/115 [03:41<24:38, 14.78s/it][AEpoch 2:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 473/573 [2:00:03<25:22, 15.23s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1092])

Validation DataLoader 0:  14%|‚ñà‚ñç        | 16/115 [03:47<23:26, 14.21s/it][AEpoch 2:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 474/573 [2:00:08<25:05, 15.21s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1197])

Validation DataLoader 0:  15%|‚ñà‚ñç        | 17/115 [03:59<22:57, 14.06s/it][AEpoch 2:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 475/573 [2:00:20<24:49, 15.20s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10624 examples [6:50:00,  1.76s/ examples]embeddings shape: torch.Size([32, 768, 1569])

Validation DataLoader 0:  16%|‚ñà‚ñå        | 18/115 [04:26<23:55, 14.79s/it][AEpoch 2:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 476/573 [2:00:47<24:36, 15.23s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1234])

Validation DataLoader 0:  17%|‚ñà‚ñã        | 19/115 [04:32<22:58, 14.36s/it][AEpoch 2:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 477/573 [2:00:54<24:20, 15.21s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1597])

Validation DataLoader 0:  17%|‚ñà‚ñã        | 20/115 [04:44<22:33, 14.24s/it][AEpoch 2:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 478/573 [2:01:06<24:04, 15.20s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1641])

Validation DataLoader 0:  18%|‚ñà‚ñä        | 21/115 [05:29<24:34, 15.68s/it][AEpoch 2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 479/573 [2:01:50<23:54, 15.26s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8800 examples [6:49:49,  2.97s/ examples]Generating train split: 10810 examples [6:51:08,  2.38s/ examples]embeddings shape: torch.Size([32, 768, 1672])

Validation DataLoader 0:  19%|‚ñà‚ñâ        | 22/115 [05:56<25:06, 16.20s/it][AEpoch 2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 480/573 [2:02:18<23:41, 15.29s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1613])

Validation DataLoader 0:  20%|‚ñà‚ñà        | 23/115 [06:08<24:33, 16.01s/it][AEpoch 2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 481/573 [2:02:29<23:25, 15.28s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1631])

Validation DataLoader 0:  21%|‚ñà‚ñà        | 24/115 [06:20<24:01, 15.84s/it][AEpoch 2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 482/573 [2:02:41<23:09, 15.27s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1631])

Validation DataLoader 0:  22%|‚ñà‚ñà‚ñè       | 25/115 [06:34<23:41, 15.80s/it][AEpoch 2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 483/573 [2:02:56<22:54, 15.27s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1614])

Validation DataLoader 0:  23%|‚ñà‚ñà‚ñé       | 26/115 [07:20<25:08, 16.95s/it][AEpoch 2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 484/573 [2:03:42<22:44, 15.34s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10690 examples [6:52:56,  2.03s/ examples]embeddings shape: torch.Size([32, 768, 1592])

Validation DataLoader 0:  23%|‚ñà‚ñà‚ñé       | 27/115 [07:57<25:57, 17.70s/it][AEpoch 2:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 485/573 [2:04:19<22:33, 15.38s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8850 examples [6:52:39,  3.10s/ examples]Generating train split: 10870 examples [6:53:45,  2.45s/ examples]embeddings shape: torch.Size([32, 768, 1681])

Validation DataLoader 0:  24%|‚ñà‚ñà‚ñç       | 28/115 [08:32<26:30, 18.29s/it][AEpoch 2:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 486/573 [2:04:53<22:21, 15.42s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1618])

Validation DataLoader 0:  25%|‚ñà‚ñà‚ñå       | 29/115 [08:41<25:45, 17.97s/it][AEpoch 2:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 487/573 [2:05:02<22:04, 15.41s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1618])

Validation DataLoader 0:  26%|‚ñà‚ñà‚ñå       | 30/115 [08:49<25:00, 17.65s/it][AEpoch 2:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 488/573 [2:05:11<21:48, 15.39s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1540])

Validation DataLoader 0:  27%|‚ñà‚ñà‚ñã       | 31/115 [08:57<24:17, 17.35s/it][AEpoch 2:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 489/573 [2:05:19<21:31, 15.38s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1574])

Validation DataLoader 0:  28%|‚ñà‚ñà‚ñä       | 32/115 [09:06<23:36, 17.06s/it][AEpoch 2:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 490/573 [2:05:27<21:15, 15.36s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1577])

Validation DataLoader 0:  29%|‚ñà‚ñà‚ñä       | 33/115 [09:15<23:00, 16.83s/it][AEpoch 2:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 491/573 [2:05:37<20:58, 15.35s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1574])

Validation DataLoader 0:  30%|‚ñà‚ñà‚ñâ       | 34/115 [09:21<22:18, 16.52s/it][AEpoch 2:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 492/573 [2:05:43<20:41, 15.33s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1677])

Validation DataLoader 0:  30%|‚ñà‚ñà‚ñà       | 35/115 [09:46<22:20, 16.76s/it][AEpoch 2:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 493/573 [2:06:08<20:28, 15.35s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1615])

Validation DataLoader 0:  31%|‚ñà‚ñà‚ñà‚ñè      | 36/115 [10:17<22:34, 17.15s/it][AEpoch 2:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 494/573 [2:06:38<20:15, 15.38s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10756 examples [6:56:12,  2.31s/ examples]Generating train split: 10930 examples [6:56:07,  2.42s/ examples]Generating train split: 8900 examples [6:55:11,  3.08s/ examples]embeddings shape: torch.Size([32, 768, 1582])

Validation DataLoader 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 37/115 [10:55<23:01, 17.71s/it][AEpoch 2:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 495/573 [2:07:16<20:03, 15.43s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1602])

Validation DataLoader 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 38/115 [11:04<22:27, 17.50s/it][AEpoch 2:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 496/573 [2:07:26<19:47, 15.42s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1545])

Validation DataLoader 0:  34%|‚ñà‚ñà‚ñà‚ñç      | 39/115 [11:14<21:53, 17.29s/it][AEpoch 2:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 497/573 [2:07:35<19:30, 15.40s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1615])

Validation DataLoader 0:  35%|‚ñà‚ñà‚ñà‚ñç      | 40/115 [11:22<21:19, 17.06s/it][AEpoch 2:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 498/573 [2:07:43<19:14, 15.39s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1602])

Validation DataLoader 0:  36%|‚ñà‚ñà‚ñà‚ñå      | 41/115 [11:29<20:44, 16.82s/it][AEpoch 2:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 499/573 [2:07:51<18:57, 15.37s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1656])

Validation DataLoader 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 42/115 [11:39<20:15, 16.64s/it][AEpoch 2:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 500/573 [2:08:00<18:41, 15.36s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1654])

Validation DataLoader 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 43/115 [11:49<19:47, 16.50s/it][AEpoch 2:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 501/573 [2:08:11<18:25, 15.35s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1570])

Validation DataLoader 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 44/115 [12:14<19:44, 16.69s/it][AEpoch 2:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 502/573 [2:08:35<18:11, 15.37s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10990 examples [6:58:05,  2.29s/ examples]embeddings shape: torch.Size([32, 768, 1627])

Validation DataLoader 0:  39%|‚ñà‚ñà‚ñà‚ñâ      | 45/115 [12:46<19:52, 17.04s/it][AEpoch 2:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 503/573 [2:09:08<17:58, 15.40s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 8950 examples [6:57:12,  2.88s/ examples]embeddings shape: torch.Size([32, 768, 1646])

Validation DataLoader 0:  40%|‚ñà‚ñà‚ñà‚ñà      | 46/115 [13:00<19:30, 16.96s/it][AEpoch 2:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 504/573 [2:09:21<17:42, 15.40s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1492])

Validation DataLoader 0:  41%|‚ñà‚ñà‚ñà‚ñà      | 47/115 [13:22<19:21, 17.08s/it][AEpoch 2:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 505/573 [2:09:44<17:28, 15.41s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10822 examples [6:59:16,  2.46s/ examples]embeddings shape: torch.Size([32, 768, 1575])

Validation DataLoader 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 48/115 [13:47<19:14, 17.23s/it][AEpoch 2:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 506/573 [2:10:08<17:13, 15.43s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1594])

Validation DataLoader 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 49/115 [13:53<18:43, 17.02s/it][AEpoch 2:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 507/573 [2:10:15<16:57, 15.42s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1549])

Validation DataLoader 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 50/115 [14:00<18:12, 16.81s/it][AEpoch 2:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 508/573 [2:10:22<16:40, 15.40s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1532])

Validation DataLoader 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51/115 [14:06<17:42, 16.60s/it][AEpoch 2:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 509/573 [2:10:28<16:24, 15.38s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1573])

Validation DataLoader 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 52/115 [14:13<17:13, 16.41s/it][AEpoch 2:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 510/573 [2:10:34<16:07, 15.36s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1604])

Validation DataLoader 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 53/115 [14:19<16:46, 16.23s/it][AEpoch 2:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 511/573 [2:10:41<15:51, 15.35s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1557])

Validation DataLoader 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 54/115 [14:29<16:21, 16.09s/it][AEpoch 2:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 512/573 [2:10:50<15:35, 15.33s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1535])

Validation DataLoader 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 55/115 [14:54<16:15, 16.25s/it][AEpoch 2:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 513/573 [2:11:15<15:21, 15.35s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 11050 examples [7:00:29,  2.32s/ examples]embeddings shape: torch.Size([32, 768, 802])

Validation DataLoader 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 56/115 [15:10<15:58, 16.25s/it][AEpoch 2:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 514/573 [2:11:31<15:05, 15.35s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9000 examples [6:59:42,  2.91s/ examples]embeddings shape: torch.Size([32, 768, 1446])

Validation DataLoader 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 57/115 [15:26<15:42, 16.25s/it][AEpoch 2:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 515/573 [2:11:48<14:50, 15.36s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1251])

Validation DataLoader 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 58/115 [15:32<15:16, 16.08s/it][AEpoch 2:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 516/573 [2:11:54<14:34, 15.34s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1607])

Validation DataLoader 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 59/115 [15:43<14:55, 15.99s/it][AEpoch 2:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 517/573 [2:12:04<14:18, 15.33s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1191])

Validation DataLoader 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 60/115 [16:07<14:46, 16.12s/it][AEpoch 2:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 518/573 [2:12:28<14:03, 15.35s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10888 examples [7:01:43,  2.39s/ examples]embeddings shape: torch.Size([32, 768, 1343])

Validation DataLoader 0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 61/115 [16:14<14:22, 15.97s/it][AEpoch 2:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 519/573 [2:12:35<13:47, 15.33s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 985])

Validation DataLoader 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 62/115 [16:18<13:56, 15.78s/it][AEpoch 2:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 520/573 [2:12:40<13:31, 15.31s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1337])

Validation DataLoader 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 63/115 [16:22<13:31, 15.60s/it][AEpoch 2:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 521/573 [2:12:44<13:14, 15.29s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1107])

Validation DataLoader 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64/115 [16:26<13:06, 15.42s/it][AEpoch 2:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 522/573 [2:12:48<12:58, 15.26s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1142])

Validation DataLoader 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 65/115 [16:30<12:42, 15.24s/it][AEpoch 2:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 523/573 [2:12:52<12:42, 15.24s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1570])

Validation DataLoader 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 66/115 [16:38<12:21, 15.12s/it][AEpoch 2:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 524/573 [2:12:59<12:26, 15.23s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1587])

Validation DataLoader 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 67/115 [16:52<12:05, 15.11s/it][AEpoch 2:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 525/573 [2:13:14<12:10, 15.23s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 11110 examples [7:02:42,  2.29s/ examples]embeddings shape: torch.Size([32, 768, 1609])

Validation DataLoader 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 68/115 [17:29<12:05, 15.43s/it][AEpoch 2:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 526/573 [2:13:50<11:57, 15.27s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9050 examples [7:02:01,  2.87s/ examples]embeddings shape: torch.Size([32, 768, 1595])

Validation DataLoader 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 69/115 [17:47<11:51, 15.47s/it][AEpoch 2:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 527/573 [2:14:09<11:42, 15.27s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1593])

Validation DataLoader 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 70/115 [17:59<11:33, 15.42s/it][AEpoch 2:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 528/573 [2:14:21<11:27, 15.27s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1575])

Validation DataLoader 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 71/115 [18:32<11:29, 15.67s/it][AEpoch 2:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 529/573 [2:14:53<11:13, 15.30s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 10954 examples [7:04:07,  2.33s/ examples]embeddings shape: torch.Size([32, 768, 1684])

Validation DataLoader 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 72/115 [18:44<11:11, 15.62s/it][AEpoch 2:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 530/573 [2:15:05<10:57, 15.29s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1662])

Validation DataLoader 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 73/115 [18:55<10:53, 15.55s/it][AEpoch 2:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 531/573 [2:15:17<10:42, 15.29s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1617])

Validation DataLoader 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 74/115 [19:06<10:35, 15.50s/it][AEpoch 2:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 532/573 [2:15:28<10:26, 15.28s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 11170 examples [7:04:54,  2.26s/ examples]embeddings shape: torch.Size([32, 768, 1639])

Validation DataLoader 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 75/115 [19:49<10:34, 15.86s/it][AEpoch 2:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 533/573 [2:16:11<10:13, 15.33s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 9100 examples [7:04:25,  2.88s/ examples]embeddings shape: torch.Size([32, 768, 1612])

Validation DataLoader 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 76/115 [20:13<10:22, 15.97s/it][AEpoch 2:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 534/573 [2:16:35<09:58, 15.35s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1581])

Validation DataLoader 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 77/115 [20:20<10:02, 15.86s/it][AEpoch 2:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 535/573 [2:16:42<09:42, 15.33s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1159])

Validation DataLoader 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 78/115 [20:27<09:42, 15.73s/it][AEpoch 2:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 536/573 [2:16:48<09:26, 15.32s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1591])

Validation DataLoader 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 79/115 [20:33<09:22, 15.62s/it][AEpoch 2:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 537/573 [2:16:55<09:10, 15.30s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1455])

Validation DataLoader 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 80/115 [20:39<09:02, 15.49s/it][AEpoch 2:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 538/573 [2:17:00<08:54, 15.28s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1566])

Validation DataLoader 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 81/115 [20:49<08:44, 15.43s/it][AEpoch 2:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 539/573 [2:17:11<08:39, 15.27s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1571])

Validation DataLoader 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 82/115 [21:16<08:33, 15.57s/it][AEpoch 2:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 540/573 [2:17:38<08:24, 15.29s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 11020 examples [7:06:59,  2.41s/ examples]embeddings shape: torch.Size([32, 768, 1568])

Validation DataLoader 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 83/115 [21:30<08:17, 15.55s/it][AEpoch 2:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 541/573 [2:17:52<08:09, 15.29s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1600])

Validation DataLoader 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 84/115 [22:02<08:07, 15.74s/it][AEpoch 2:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 542/573 [2:18:23<07:54, 15.32s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 11230 examples [7:07:31,  2.37s/ examples]Generating train split: 9150 examples [7:06:53,  2.90s/ examples]embeddings shape: torch.Size([32, 768, 1569])

Validation DataLoader 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 85/115 [22:35<07:58, 15.95s/it][AEpoch 2:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 543/573 [2:18:56<07:40, 15.35s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1626])

Validation DataLoader 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 86/115 [22:44<07:40, 15.86s/it][AEpoch 2:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 544/573 [2:19:05<07:24, 15.34s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1599])

Validation DataLoader 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 87/115 [22:55<07:22, 15.81s/it][AEpoch 2:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 545/573 [2:19:16<07:09, 15.33s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1553])

Validation DataLoader 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 88/115 [23:04<07:04, 15.73s/it][AEpoch 2:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 546/573 [2:19:26<06:53, 15.32s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1636])

Validation DataLoader 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 89/115 [23:11<06:46, 15.64s/it][AEpoch 2:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 547/573 [2:19:33<06:38, 15.31s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1600])

Validation DataLoader 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 90/115 [23:18<06:28, 15.53s/it][AEpoch 2:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 548/573 [2:19:39<06:22, 15.29s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1475])

Validation DataLoader 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 91/115 [23:24<06:10, 15.44s/it][AEpoch 2:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 549/573 [2:19:46<06:06, 15.28s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1533])

Validation DataLoader 0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 92/115 [23:28<05:52, 15.31s/it][AEpoch 2:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 550/573 [2:19:50<05:50, 15.25s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1593])

Validation DataLoader 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 93/115 [23:46<05:37, 15.33s/it][AEpoch 2:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 551/573 [2:20:07<05:35, 15.26s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 48])

Validation DataLoader 0:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 94/115 [23:47<05:18, 15.18s/it][AEpoch 2:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 552/573 [2:20:08<05:19, 15.23s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1074])

Validation DataLoader 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 95/115 [24:01<05:03, 15.17s/it][AEpoch 2:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 553/573 [2:20:22<05:04, 15.23s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 11290 examples [7:09:30,  2.25s/ examples]Generating train split: 9200 examples [7:08:43,  2.69s/ examples]embeddings shape: torch.Size([32, 768, 1660])

Validation DataLoader 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 96/115 [24:31<04:51, 15.33s/it][AEpoch 2:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 554/573 [2:20:53<04:49, 15.26s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1624])

Validation DataLoader 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 97/115 [25:12<04:40, 15.59s/it][AEpoch 2:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 555/573 [2:21:33<04:35, 15.30s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 11086 examples [7:10:52,  2.74s/ examples]embeddings shape: torch.Size([32, 768, 1568])

Validation DataLoader 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 98/115 [25:22<04:24, 15.54s/it][AEpoch 2:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 556/573 [2:21:44<04:20, 15.30s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1581])

Validation DataLoader 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 99/115 [25:30<04:07, 15.46s/it][AEpoch 2:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 557/573 [2:21:52<04:04, 15.28s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1602])

Validation DataLoader 0:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 100/115 [25:39<03:50, 15.40s/it][AEpoch 2:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 558/573 [2:22:01<03:49, 15.27s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1598])

Validation DataLoader 0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 101/115 [25:45<03:34, 15.30s/it][AEpoch 2:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 559/573 [2:22:06<03:33, 15.25s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1603])

Validation DataLoader 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 102/115 [25:54<03:18, 15.24s/it][AEpoch 2:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 560/573 [2:22:16<03:18, 15.24s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1564])

Validation DataLoader 0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 103/115 [26:19<03:04, 15.34s/it][AEpoch 2:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 561/573 [2:22:41<03:03, 15.26s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 1615])

Validation DataLoader 0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 104/115 [26:35<02:48, 15.34s/it][AEpoch 2:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 562/573 [2:22:56<02:47, 15.26s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Generating train split: 11350 examples [7:12:01,  2.33s/ examples]Generating train split: 9250 examples [7:11:16,  2.80s/ examples]embeddings shape: torch.Size([32, 768, 1659])

Validation DataLoader 0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 105/115 [27:01<02:34, 15.45s/it][AEpoch 2:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 563/573 [2:23:23<02:32, 15.28s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 34])

Validation DataLoader 0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 106/115 [27:01<02:17, 15.30s/it][AEpoch 2:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 564/573 [2:23:23<02:17, 15.25s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 30])

Validation DataLoader 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 107/115 [27:02<02:01, 15.16s/it][AEpoch 2:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 565/573 [2:23:23<02:01, 15.23s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 25])

Validation DataLoader 0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 108/115 [27:02<01:45, 15.02s/it][AEpoch 2:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 566/573 [2:23:24<01:46, 15.20s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 109/115 [27:02<01:29, 14.89s/it][AEpoch 2:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 567/573 [2:23:24<01:31, 15.17s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 110/115 [27:02<01:13, 14.75s/it][AEpoch 2:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 568/573 [2:23:24<01:15, 15.15s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 111/115 [27:02<00:58, 14.62s/it][AEpoch 2:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 569/573 [2:23:24<01:00, 15.12s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 112/115 [27:02<00:43, 14.49s/it][AEpoch 2:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 570/573 [2:23:24<00:45, 15.10s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 113/115 [27:02<00:28, 14.36s/it][AEpoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 571/573 [2:23:24<00:30, 15.07s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([32, 768, 6])

Validation DataLoader 0:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 114/115 [27:02<00:14, 14.24s/it][AEpoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 572/573 [2:23:24<00:15, 15.04s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]embeddings shape: torch.Size([22, 768, 22])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [27:02<00:00, 14.11s/it][AEpoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 573/573 [2:23:24<00:00, 15.02s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.182, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 573/573 [2:23:24<00:00, 15.02s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.414, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.860]
                                                                          [AEpoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 573/573 [2:23:24<00:00, 15.02s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 2, global step 1374: 'val_loss' was not in top 1
Epoch duration: 8604.57 seconds
Epoch 2:   0%|          | 0/573 [00:00<?, ?it/s, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]            Epoch 3:   0%|          | 0/573 [00:00<?, ?it/s, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1674])
Epoch 3:   0%|          | 1/573 [00:07<1:16:01,  7.97s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.192, train binary_accuracy_step=0.829, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   0%|          | 1/573 [00:07<1:16:01,  7.97s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1595])
Epoch 3:   0%|          | 2/573 [00:12<1:00:23,  6.35s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   0%|          | 2/573 [00:12<1:00:24,  6.35s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1590])
Epoch 3:   1%|          | 3/573 [00:35<1:52:22, 11.83s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   1%|          | 3/573 [00:35<1:52:22, 11.83s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11152 examples [7:13:27,  2.63s/ examples]embeddings shape: torch.Size([32, 768, 1589])
Epoch 3:   1%|          | 4/573 [00:53<2:08:00, 13.50s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   1%|          | 4/573 [00:53<2:08:01, 13.50s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1564])
Epoch 3:   1%|          | 5/573 [00:58<1:51:04, 11.73s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   1%|          | 5/573 [00:58<1:51:04, 11.73s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.809, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1592])
Epoch 3:   1%|          | 6/573 [01:04<1:41:33, 10.75s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.809, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   1%|          | 6/573 [01:04<1:41:33, 10.75s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1565])
Epoch 3:   1%|          | 7/573 [01:10<1:35:10, 10.09s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   1%|          | 7/573 [01:10<1:35:10, 10.09s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1591])
Epoch 3:   1%|‚ñè         | 8/573 [01:14<1:28:07,  9.36s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   1%|‚ñè         | 8/573 [01:14<1:28:07,  9.36s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1556])
Epoch 3:   2%|‚ñè         | 9/573 [01:21<1:25:19,  9.08s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   2%|‚ñè         | 9/573 [01:21<1:25:19,  9.08s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1554])
Epoch 3:   2%|‚ñè         | 10/573 [01:40<1:34:41, 10.09s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   2%|‚ñè         | 10/573 [01:40<1:34:41, 10.09s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1325])
Epoch 3:   2%|‚ñè         | 11/573 [01:57<1:39:56, 10.67s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   2%|‚ñè         | 11/573 [01:57<1:39:56, 10.67s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11410 examples [7:14:29,  2.37s/ examples]Generating train split: 9300 examples [7:13:42,  2.84s/ examples]embeddings shape: torch.Size([32, 768, 1373])
Epoch 3:   2%|‚ñè         | 12/573 [02:21<1:50:21, 11.80s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   2%|‚ñè         | 12/573 [02:21<1:50:21, 11.80s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1630])
Epoch 3:   2%|‚ñè         | 13/573 [02:28<1:46:48, 11.44s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   2%|‚ñè         | 13/573 [02:28<1:46:48, 11.44s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1621])
Epoch 3:   2%|‚ñè         | 14/573 [02:36<1:43:49, 11.14s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   2%|‚ñè         | 14/573 [02:36<1:43:49, 11.14s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1570])
Epoch 3:   3%|‚ñé         | 15/573 [02:42<1:40:44, 10.83s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   3%|‚ñé         | 15/573 [02:42<1:40:44, 10.83s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.812, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1650])
Epoch 3:   3%|‚ñé         | 16/573 [03:01<1:45:04, 11.32s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.812, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   3%|‚ñé         | 16/573 [03:01<1:45:04, 11.32s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11218 examples [7:15:51,  2.50s/ examples]embeddings shape: torch.Size([32, 768, 1678])
Epoch 3:   3%|‚ñé         | 17/573 [03:17<1:47:53, 11.64s/it, loss=0.337, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   3%|‚ñé         | 17/573 [03:17<1:47:53, 11.64s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1590])
Epoch 3:   3%|‚ñé         | 18/573 [03:24<1:44:55, 11.34s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   3%|‚ñé         | 18/573 [03:24<1:44:55, 11.34s/it, loss=0.341, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1455])
Epoch 3:   3%|‚ñé         | 19/573 [03:29<1:41:37, 11.01s/it, loss=0.341, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   3%|‚ñé         | 19/573 [03:29<1:41:37, 11.01s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1226])
Epoch 3:   3%|‚ñé         | 20/573 [03:33<1:38:17, 10.66s/it, loss=0.344, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   3%|‚ñé         | 20/573 [03:33<1:38:17, 10.66s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1179])
Epoch 3:   4%|‚ñé         | 21/573 [03:39<1:36:02, 10.44s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   4%|‚ñé         | 21/573 [03:39<1:36:02, 10.44s/it, loss=0.341, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1569])
Epoch 3:   4%|‚ñç         | 22/573 [03:46<1:34:25, 10.28s/it, loss=0.341, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   4%|‚ñç         | 22/573 [03:46<1:34:25, 10.28s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1622])
Epoch 3:   4%|‚ñç         | 23/573 [04:10<1:39:44, 10.88s/it, loss=0.34, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   4%|‚ñç         | 23/573 [04:10<1:39:44, 10.88s/it, loss=0.341, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11470 examples [7:16:45,  2.34s/ examples]embeddings shape: torch.Size([32, 768, 1561])
Epoch 3:   4%|‚ñç         | 24/573 [04:35<1:44:56, 11.47s/it, loss=0.341, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   4%|‚ñç         | 24/573 [04:35<1:44:56, 11.47s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 9350 examples [7:16:13,  2.89s/ examples]embeddings shape: torch.Size([32, 768, 1548])
Epoch 3:   4%|‚ñç         | 25/573 [04:54<1:47:27, 11.76s/it, loss=0.339, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   4%|‚ñç         | 25/573 [04:54<1:47:27, 11.76s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 920])
Epoch 3:   5%|‚ñç         | 26/573 [04:58<1:44:40, 11.48s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   5%|‚ñç         | 26/573 [04:58<1:44:40, 11.48s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1649])
Epoch 3:   5%|‚ñç         | 27/573 [05:06<1:43:14, 11.35s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   5%|‚ñç         | 27/573 [05:06<1:43:14, 11.35s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1574])
Epoch 3:   5%|‚ñç         | 28/573 [05:14<1:41:56, 11.22s/it, loss=0.335, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   5%|‚ñç         | 28/573 [05:14<1:41:56, 11.22s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1616])
Epoch 3:   5%|‚ñå         | 29/573 [05:47<1:48:36, 11.98s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   5%|‚ñå         | 29/573 [05:47<1:48:36, 11.98s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11284 examples [7:18:25,  2.44s/ examples]embeddings shape: torch.Size([32, 768, 1517])
Epoch 3:   5%|‚ñå         | 30/573 [05:53<1:46:40, 11.79s/it, loss=0.334, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   5%|‚ñå         | 30/573 [05:53<1:46:40, 11.79s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1578])
Epoch 3:   5%|‚ñå         | 31/573 [05:59<1:44:40, 11.59s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   5%|‚ñå         | 31/573 [05:59<1:44:40, 11.59s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1559])
Epoch 3:   6%|‚ñå         | 32/573 [06:05<1:43:04, 11.43s/it, loss=0.332, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   6%|‚ñå         | 32/573 [06:05<1:43:04, 11.43s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.500, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1616])
Epoch 3:   6%|‚ñå         | 33/573 [06:13<1:41:48, 11.31s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.500, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   6%|‚ñå         | 33/573 [06:13<1:41:48, 11.31s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1618])
Epoch 3:   6%|‚ñå         | 34/573 [06:36<1:44:44, 11.66s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   6%|‚ñå         | 34/573 [06:36<1:44:45, 11.66s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11530 examples [7:19:12,  2.37s/ examples]embeddings shape: torch.Size([32, 768, 1555])
Epoch 3:   6%|‚ñå         | 35/573 [07:10<1:50:19, 12.30s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   6%|‚ñå         | 35/573 [07:10<1:50:19, 12.30s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 9400 examples [7:18:42,  2.92s/ examples]embeddings shape: torch.Size([32, 768, 1519])
Epoch 3:   6%|‚ñã         | 36/573 [07:23<1:50:16, 12.32s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   6%|‚ñã         | 36/573 [07:23<1:50:16, 12.32s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1567])
Epoch 3:   6%|‚ñã         | 37/573 [07:29<1:48:32, 12.15s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   6%|‚ñã         | 37/573 [07:29<1:48:32, 12.15s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1632])
Epoch 3:   7%|‚ñã         | 38/573 [07:37<1:47:18, 12.04s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   7%|‚ñã         | 38/573 [07:37<1:47:18, 12.04s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1608])
Epoch 3:   7%|‚ñã         | 39/573 [07:44<1:46:03, 11.92s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   7%|‚ñã         | 39/573 [07:44<1:46:03, 11.92s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1599])
Epoch 3:   7%|‚ñã         | 40/573 [07:49<1:44:14, 11.74s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   7%|‚ñã         | 40/573 [07:49<1:44:14, 11.74s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1604])
Epoch 3:   7%|‚ñã         | 41/573 [07:56<1:43:06, 11.63s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   7%|‚ñã         | 41/573 [07:56<1:43:06, 11.63s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1373])
Epoch 3:   7%|‚ñã         | 42/573 [08:06<1:42:32, 11.59s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   7%|‚ñã         | 42/573 [08:06<1:42:32, 11.59s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1618])
Epoch 3:   8%|‚ñä         | 43/573 [08:33<1:45:26, 11.94s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   8%|‚ñä         | 43/573 [08:33<1:45:26, 11.94s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11350 examples [7:21:11,  2.47s/ examples]embeddings shape: torch.Size([32, 768, 1585])
Epoch 3:   8%|‚ñä         | 44/573 [08:40<1:44:16, 11.83s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   8%|‚ñä         | 44/573 [08:40<1:44:16, 11.83s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1619])
Epoch 3:   8%|‚ñä         | 45/573 [08:47<1:43:07, 11.72s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   8%|‚ñä         | 45/573 [08:47<1:43:07, 11.72s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1645])
Epoch 3:   8%|‚ñä         | 46/573 [09:16<1:46:17, 12.10s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   8%|‚ñä         | 46/573 [09:16<1:46:17, 12.10s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11590 examples [7:21:44,  2.42s/ examples]embeddings shape: torch.Size([32, 768, 1315])
Epoch 3:   8%|‚ñä         | 47/573 [09:39<1:48:03, 12.33s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   8%|‚ñä         | 47/573 [09:39<1:48:03, 12.33s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 9450 examples [7:21:10,  2.93s/ examples]embeddings shape: torch.Size([32, 768, 1584])
Epoch 3:   8%|‚ñä         | 48/573 [09:55<1:48:37, 12.41s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   8%|‚ñä         | 48/573 [09:55<1:48:37, 12.41s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1630])
Epoch 3:   9%|‚ñä         | 49/573 [10:03<1:47:31, 12.31s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   9%|‚ñä         | 49/573 [10:03<1:47:31, 12.31s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1609])
Epoch 3:   9%|‚ñä         | 50/573 [10:10<1:46:21, 12.20s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   9%|‚ñä         | 50/573 [10:10<1:46:21, 12.20s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1602])
Epoch 3:   9%|‚ñâ         | 51/573 [10:18<1:45:34, 12.13s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   9%|‚ñâ         | 51/573 [10:18<1:45:34, 12.13s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1563])
Epoch 3:   9%|‚ñâ         | 52/573 [10:26<1:44:36, 12.05s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   9%|‚ñâ         | 52/573 [10:26<1:44:36, 12.05s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1576])
Epoch 3:   9%|‚ñâ         | 53/573 [10:32<1:43:24, 11.93s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   9%|‚ñâ         | 53/573 [10:32<1:43:24, 11.93s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1527])
Epoch 3:   9%|‚ñâ         | 54/573 [10:56<1:45:11, 12.16s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:   9%|‚ñâ         | 54/573 [10:56<1:45:11, 12.16s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11416 examples [7:23:44,  2.42s/ examples]embeddings shape: torch.Size([32, 768, 1588])
Epoch 3:  10%|‚ñâ         | 55/573 [11:12<1:45:36, 12.23s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  10%|‚ñâ         | 55/573 [11:12<1:45:36, 12.23s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1649])
Epoch 3:  10%|‚ñâ         | 56/573 [11:31<1:46:19, 12.34s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  10%|‚ñâ         | 56/573 [11:31<1:46:19, 12.34s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.906, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11650 examples [7:24:07,  2.41s/ examples]embeddings shape: torch.Size([32, 768, 1625])
Epoch 3:  10%|‚ñâ         | 57/573 [11:53<1:47:38, 12.52s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.906, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  10%|‚ñâ         | 57/573 [11:53<1:47:38, 12.52s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 9500 examples [7:23:43,  2.97s/ examples]embeddings shape: torch.Size([32, 768, 1577])
Epoch 3:  10%|‚ñà         | 58/573 [12:22<1:49:49, 12.80s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  10%|‚ñà         | 58/573 [12:22<1:49:50, 12.80s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1621])
Epoch 3:  10%|‚ñà         | 59/573 [12:30<1:49:02, 12.73s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  10%|‚ñà         | 59/573 [12:30<1:49:02, 12.73s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1076])
Epoch 3:  10%|‚ñà         | 60/573 [12:35<1:47:38, 12.59s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  10%|‚ñà         | 60/573 [12:35<1:47:38, 12.59s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.656, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1624])
Epoch 3:  11%|‚ñà         | 61/573 [12:42<1:46:37, 12.50s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.656, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  11%|‚ñà         | 61/573 [12:42<1:46:37, 12.50s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1615])
Epoch 3:  11%|‚ñà         | 62/573 [12:48<1:45:32, 12.39s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  11%|‚ñà         | 62/573 [12:48<1:45:32, 12.39s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1572])
Epoch 3:  11%|‚ñà         | 63/573 [12:55<1:44:36, 12.31s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  11%|‚ñà         | 63/573 [12:55<1:44:36, 12.31s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1584])
Epoch 3:  11%|‚ñà         | 64/573 [13:01<1:43:33, 12.21s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  11%|‚ñà         | 64/573 [13:01<1:43:33, 12.21s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1609])
Epoch 3:  11%|‚ñà‚ñè        | 65/573 [13:16<1:43:41, 12.25s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  11%|‚ñà‚ñè        | 65/573 [13:16<1:43:41, 12.25s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11482 examples [7:26:12,  2.37s/ examples]embeddings shape: torch.Size([32, 768, 1595])
Epoch 3:  12%|‚ñà‚ñè        | 66/573 [13:37<1:44:40, 12.39s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  12%|‚ñà‚ñè        | 66/573 [13:37<1:44:40, 12.39s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1683])
Epoch 3:  12%|‚ñà‚ñè        | 67/573 [14:06<1:46:31, 12.63s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  12%|‚ñà‚ñè        | 67/573 [14:06<1:46:31, 12.63s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11710 examples [7:26:33,  2.42s/ examples]embeddings shape: torch.Size([32, 768, 1589])
Epoch 3:  12%|‚ñà‚ñè        | 68/573 [14:15<1:45:54, 12.58s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  12%|‚ñà‚ñè        | 68/573 [14:15<1:45:54, 12.58s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1659])
Epoch 3:  12%|‚ñà‚ñè        | 69/573 [14:38<1:46:58, 12.74s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  12%|‚ñà‚ñè        | 69/573 [14:38<1:46:58, 12.74s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 9550 examples [7:26:11,  2.97s/ examples]embeddings shape: torch.Size([32, 768, 1522])
Epoch 3:  12%|‚ñà‚ñè        | 70/573 [14:54<1:47:09, 12.78s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  12%|‚ñà‚ñè        | 70/573 [14:54<1:47:09, 12.78s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1653])
Epoch 3:  12%|‚ñà‚ñè        | 71/573 [15:02<1:46:19, 12.71s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  12%|‚ñà‚ñè        | 71/573 [15:02<1:46:19, 12.71s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1329])
Epoch 3:  13%|‚ñà‚ñé        | 72/573 [15:09<1:45:28, 12.63s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  13%|‚ñà‚ñé        | 72/573 [15:09<1:45:28, 12.63s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1600])
Epoch 3:  13%|‚ñà‚ñé        | 73/573 [15:16<1:44:35, 12.55s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  13%|‚ñà‚ñé        | 73/573 [15:16<1:44:35, 12.55s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1601])
Epoch 3:  13%|‚ñà‚ñé        | 74/573 [15:23<1:43:44, 12.47s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  13%|‚ñà‚ñé        | 74/573 [15:23<1:43:44, 12.47s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1612])
Epoch 3:  13%|‚ñà‚ñé        | 75/573 [15:31<1:43:05, 12.42s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  13%|‚ñà‚ñé        | 75/573 [15:31<1:43:05, 12.42s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1578])
Epoch 3:  13%|‚ñà‚ñé        | 76/573 [15:39<1:42:21, 12.36s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  13%|‚ñà‚ñé        | 76/573 [15:39<1:42:21, 12.36s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1579])
Epoch 3:  13%|‚ñà‚ñé        | 77/573 [15:51<1:42:07, 12.35s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  13%|‚ñà‚ñé        | 77/573 [15:51<1:42:07, 12.35s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11770 examples [7:28:37,  2.31s/ examples]embeddings shape: torch.Size([32, 768, 1198])
Epoch 3:  14%|‚ñà‚ñé        | 78/573 [16:15<1:43:10, 12.51s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  14%|‚ñà‚ñé        | 78/573 [16:15<1:43:10, 12.51s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1581])
Epoch 3:  14%|‚ñà‚ñç        | 79/573 [16:33<1:43:35, 12.58s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  14%|‚ñà‚ñç        | 79/573 [16:33<1:43:35, 12.58s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1530])
Epoch 3:  14%|‚ñà‚ñç        | 80/573 [16:57<1:44:32, 12.72s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  14%|‚ñà‚ñç        | 80/573 [16:57<1:44:32, 12.72s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11548 examples [7:29:47,  2.63s/ examples]Generating train split: 9600 examples [7:28:50,  3.03s/ examples]embeddings shape: torch.Size([32, 768, 1478])
Epoch 3:  14%|‚ñà‚ñç        | 81/573 [17:29<1:46:16, 12.96s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  14%|‚ñà‚ñç        | 81/573 [17:29<1:46:16, 12.96s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1212])
Epoch 3:  14%|‚ñà‚ñç        | 82/573 [17:36<1:45:24, 12.88s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  14%|‚ñà‚ñç        | 82/573 [17:36<1:45:24, 12.88s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1395])
Epoch 3:  14%|‚ñà‚ñç        | 83/573 [17:43<1:44:37, 12.81s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  14%|‚ñà‚ñç        | 83/573 [17:43<1:44:37, 12.81s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1247])
Epoch 3:  15%|‚ñà‚ñç        | 84/573 [17:49<1:43:45, 12.73s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  15%|‚ñà‚ñç        | 84/573 [17:49<1:43:45, 12.73s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1551])
Epoch 3:  15%|‚ñà‚ñç        | 85/573 [17:56<1:43:02, 12.67s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  15%|‚ñà‚ñç        | 85/573 [17:56<1:43:02, 12.67s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1556])
Epoch 3:  15%|‚ñà‚ñå        | 86/573 [18:03<1:42:13, 12.59s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  15%|‚ñà‚ñå        | 86/573 [18:03<1:42:13, 12.59s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11614 examples [7:30:56,  2.16s/ examples]embeddings shape: torch.Size([32, 768, 1552])
Epoch 3:  15%|‚ñà‚ñå        | 87/573 [18:21<1:42:30, 12.66s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  15%|‚ñà‚ñå        | 87/573 [18:21<1:42:30, 12.66s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1617])
Epoch 3:  15%|‚ñà‚ñå        | 88/573 [18:27<1:41:45, 12.59s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  15%|‚ñà‚ñå        | 88/573 [18:27<1:41:45, 12.59s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1551])
Epoch 3:  16%|‚ñà‚ñå        | 89/573 [18:50<1:42:29, 12.71s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  16%|‚ñà‚ñå        | 89/573 [18:50<1:42:29, 12.71s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11830 examples [7:31:23,  2.45s/ examples]embeddings shape: torch.Size([32, 768, 1630])
Epoch 3:  16%|‚ñà‚ñå        | 90/573 [19:07<1:42:36, 12.75s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  16%|‚ñà‚ñå        | 90/573 [19:07<1:42:36, 12.75s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1572])
Epoch 3:  16%|‚ñà‚ñå        | 91/573 [19:22<1:42:39, 12.78s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  16%|‚ñà‚ñå        | 91/573 [19:22<1:42:39, 12.78s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 9650 examples [7:31:02,  2.91s/ examples]embeddings shape: torch.Size([32, 768, 1634])
Epoch 3:  16%|‚ñà‚ñå        | 92/573 [19:44<1:43:14, 12.88s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  16%|‚ñà‚ñå        | 92/573 [19:44<1:43:14, 12.88s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11680 examples [7:32:41,  1.99s/ examples]embeddings shape: torch.Size([32, 768, 1198])
Epoch 3:  16%|‚ñà‚ñå        | 93/573 [20:06<1:43:45, 12.97s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  16%|‚ñà‚ñå        | 93/573 [20:06<1:43:45, 12.97s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1610])
Epoch 3:  16%|‚ñà‚ñã        | 94/573 [20:12<1:42:56, 12.90s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  16%|‚ñà‚ñã        | 94/573 [20:12<1:42:56, 12.90s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1585])
Epoch 3:  17%|‚ñà‚ñã        | 95/573 [20:17<1:42:08, 12.82s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  17%|‚ñà‚ñã        | 95/573 [20:17<1:42:08, 12.82s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1554])
Epoch 3:  17%|‚ñà‚ñã        | 96/573 [20:25<1:41:29, 12.77s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  17%|‚ñà‚ñã        | 96/573 [20:25<1:41:29, 12.77s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11746 examples [7:33:13,  1.54s/ examples]embeddings shape: torch.Size([32, 768, 1614])
Epoch 3:  17%|‚ñà‚ñã        | 97/573 [20:39<1:41:21, 12.78s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  17%|‚ñà‚ñã        | 97/573 [20:39<1:41:21, 12.78s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1642])
Epoch 3:  17%|‚ñà‚ñã        | 98/573 [20:46<1:40:39, 12.72s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  17%|‚ñà‚ñã        | 98/573 [20:46<1:40:39, 12.72s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1597])
Epoch 3:  17%|‚ñà‚ñã        | 99/573 [21:07<1:41:06, 12.80s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  17%|‚ñà‚ñã        | 99/573 [21:07<1:41:06, 12.80s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11890 examples [7:33:43,  2.41s/ examples]embeddings shape: torch.Size([32, 768, 1593])
Epoch 3:  17%|‚ñà‚ñã        | 100/573 [21:26<1:41:23, 12.86s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  17%|‚ñà‚ñã        | 100/573 [21:26<1:41:23, 12.86s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1583])
Epoch 3:  18%|‚ñà‚ñä        | 101/573 [21:32<1:40:38, 12.79s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  18%|‚ñà‚ñä        | 101/573 [21:32<1:40:38, 12.79s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1571])
Epoch 3:  18%|‚ñà‚ñä        | 102/573 [22:03<1:41:49, 12.97s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  18%|‚ñà‚ñä        | 102/573 [22:03<1:41:49, 12.97s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 9700 examples [7:33:25,  2.90s/ examples]embeddings shape: torch.Size([32, 768, 1582])
Epoch 3:  18%|‚ñà‚ñä        | 103/573 [22:10<1:41:11, 12.92s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  18%|‚ñà‚ñä        | 103/573 [22:10<1:41:11, 12.92s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1598])
Epoch 3:  18%|‚ñà‚ñä        | 104/573 [22:18<1:40:35, 12.87s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  18%|‚ñà‚ñä        | 104/573 [22:18<1:40:35, 12.87s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1582])
Epoch 3:  18%|‚ñà‚ñä        | 105/573 [22:25<1:39:56, 12.81s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  18%|‚ñà‚ñä        | 105/573 [22:25<1:39:56, 12.81s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1627])
Epoch 3:  18%|‚ñà‚ñä        | 106/573 [22:31<1:39:15, 12.75s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  18%|‚ñà‚ñä        | 106/573 [22:31<1:39:15, 12.75s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1305])
Epoch 3:  19%|‚ñà‚ñä        | 107/573 [22:37<1:38:34, 12.69s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  19%|‚ñà‚ñä        | 107/573 [22:37<1:38:34, 12.69s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1554])
Epoch 3:  19%|‚ñà‚ñâ        | 108/573 [22:45<1:37:57, 12.64s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  19%|‚ñà‚ñâ        | 108/573 [22:45<1:37:57, 12.64s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1614])
Epoch 3:  19%|‚ñà‚ñâ        | 109/573 [23:05<1:38:16, 12.71s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  19%|‚ñà‚ñâ        | 109/573 [23:05<1:38:16, 12.71s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11950 examples [7:35:40,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1606])
Epoch 3:  19%|‚ñà‚ñâ        | 110/573 [23:22<1:38:21, 12.75s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  19%|‚ñà‚ñâ        | 110/573 [23:22<1:38:21, 12.75s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1599])
Epoch 3:  19%|‚ñà‚ñâ        | 111/573 [23:37<1:38:18, 12.77s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  19%|‚ñà‚ñâ        | 111/573 [23:37<1:38:18, 12.77s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1603])
Epoch 3:  20%|‚ñà‚ñâ        | 112/573 [24:09<1:39:27, 12.95s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  20%|‚ñà‚ñâ        | 112/573 [24:09<1:39:27, 12.95s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 9750 examples [7:35:54,  2.92s/ examples]Generating train split: 11812 examples [7:37:13,  2.16s/ examples]embeddings shape: torch.Size([32, 768, 1622])
Epoch 3:  20%|‚ñà‚ñâ        | 113/573 [24:37<1:40:12, 13.07s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  20%|‚ñà‚ñâ        | 113/573 [24:37<1:40:12, 13.07s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11878 examples [7:37:17,  1.53s/ examples]Generating train split: 11944 examples [7:37:20,  1.09s/ examples]embeddings shape: torch.Size([32, 768, 1621])
Epoch 3:  20%|‚ñà‚ñâ        | 114/573 [24:45<1:39:41, 13.03s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  20%|‚ñà‚ñâ        | 114/573 [24:45<1:39:41, 13.03s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12010 examples [7:37:24,  1.28 examples/s]embeddings shape: torch.Size([32, 768, 1561])
Epoch 3:  20%|‚ñà‚ñà        | 115/573 [24:53<1:39:09, 12.99s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  20%|‚ñà‚ñà        | 115/573 [24:53<1:39:09, 12.99s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1568])
Epoch 3:  20%|‚ñà‚ñà        | 116/573 [25:00<1:38:30, 12.93s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  20%|‚ñà‚ñà        | 116/573 [25:00<1:38:30, 12.93s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12010 examples [7:37:42,  1.28 examples/s]embeddings shape: torch.Size([32, 768, 1598])
Epoch 3:  20%|‚ñà‚ñà        | 117/573 [25:08<1:37:59, 12.89s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  20%|‚ñà‚ñà        | 117/573 [25:08<1:37:59, 12.89s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1634])
Epoch 3:  21%|‚ñà‚ñà        | 118/573 [25:15<1:37:23, 12.84s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  21%|‚ñà‚ñà        | 118/573 [25:15<1:37:23, 12.84s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12076 examples [7:38:15,  1.29 examples/s]embeddings shape: torch.Size([32, 768, 1610])
Epoch 3:  21%|‚ñà‚ñà        | 119/573 [25:56<1:38:58, 13.08s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  21%|‚ñà‚ñà        | 119/573 [25:56<1:38:58, 13.08s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12010 examples [7:38:20,  2.40s/ examples]embeddings shape: torch.Size([32, 768, 1587])
Epoch 3:  21%|‚ñà‚ñà        | 120/573 [26:04<1:38:26, 13.04s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  21%|‚ñà‚ñà        | 120/573 [26:04<1:38:26, 13.04s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1577])
Epoch 3:  21%|‚ñà‚ñà        | 121/573 [26:10<1:37:46, 12.98s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  21%|‚ñà‚ñà        | 121/573 [26:10<1:37:46, 12.98s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1653])
Epoch 3:  21%|‚ñà‚ñà‚ñè       | 122/573 [26:21<1:37:27, 12.97s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  21%|‚ñà‚ñà‚ñè       | 122/573 [26:21<1:37:27, 12.97s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 9799 examples [7:38:05,  2.85s/ examples]embeddings shape: torch.Size([32, 768, 1568])
Epoch 3:  21%|‚ñà‚ñà‚ñè       | 123/573 [26:46<1:37:56, 13.06s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  21%|‚ñà‚ñà‚ñè       | 123/573 [26:46<1:37:56, 13.06s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1599])
Epoch 3:  22%|‚ñà‚ñà‚ñè       | 124/573 [26:53<1:37:23, 13.02s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  22%|‚ñà‚ñà‚ñè       | 124/573 [26:53<1:37:23, 13.02s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1378])
Epoch 3:  22%|‚ñà‚ñà‚ñè       | 125/573 [26:59<1:36:45, 12.96s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  22%|‚ñà‚ñà‚ñè       | 125/573 [26:59<1:36:45, 12.96s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1575])
Epoch 3:  22%|‚ñà‚ñà‚ñè       | 126/573 [27:06<1:36:08, 12.91s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  22%|‚ñà‚ñà‚ñè       | 126/573 [27:06<1:36:08, 12.91s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1610])
Epoch 3:  22%|‚ñà‚ñà‚ñè       | 127/573 [27:11<1:35:29, 12.85s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  22%|‚ñà‚ñà‚ñè       | 127/573 [27:11<1:35:29, 12.85s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1623])
Epoch 3:  22%|‚ñà‚ñà‚ñè       | 128/573 [27:18<1:34:57, 12.80s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  22%|‚ñà‚ñà‚ñè       | 128/573 [27:18<1:34:57, 12.80s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1645])
Epoch 3:  23%|‚ñà‚ñà‚ñé       | 129/573 [27:47<1:35:38, 12.92s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  23%|‚ñà‚ñà‚ñé       | 129/573 [27:47<1:35:38, 12.92s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12070 examples [7:40:14,  2.24s/ examples]embeddings shape: torch.Size([32, 768, 1670])
Epoch 3:  23%|‚ñà‚ñà‚ñé       | 130/573 [27:58<1:35:18, 12.91s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  23%|‚ñà‚ñà‚ñé       | 130/573 [27:58<1:35:18, 12.91s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1558])
Epoch 3:  23%|‚ñà‚ñà‚ñé       | 131/573 [28:05<1:34:47, 12.87s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  23%|‚ñà‚ñà‚ñé       | 131/573 [28:05<1:34:47, 12.87s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1598])
Epoch 3:  23%|‚ñà‚ñà‚ñé       | 132/573 [28:28<1:35:07, 12.94s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  23%|‚ñà‚ñà‚ñé       | 132/573 [28:28<1:35:07, 12.94s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 9848 examples [7:40:06,  2.74s/ examples]embeddings shape: torch.Size([32, 768, 1101])
Epoch 3:  23%|‚ñà‚ñà‚ñé       | 133/573 [28:50<1:35:23, 13.01s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  23%|‚ñà‚ñà‚ñé       | 133/573 [28:50<1:35:23, 13.01s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1321])
Epoch 3:  23%|‚ñà‚ñà‚ñé       | 134/573 [29:19<1:36:03, 13.13s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  23%|‚ñà‚ñà‚ñé       | 134/573 [29:19<1:36:03, 13.13s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12142 examples [7:41:59,  1.56s/ examples]embeddings shape: torch.Size([32, 768, 1604])
Epoch 3:  24%|‚ñà‚ñà‚ñé       | 135/573 [29:28<1:35:38, 13.10s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  24%|‚ñà‚ñà‚ñé       | 135/573 [29:28<1:35:38, 13.10s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1272])
Epoch 3:  24%|‚ñà‚ñà‚ñé       | 136/573 [29:36<1:35:07, 13.06s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  24%|‚ñà‚ñà‚ñé       | 136/573 [29:36<1:35:07, 13.06s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1667])
Epoch 3:  24%|‚ñà‚ñà‚ñç       | 137/573 [29:42<1:34:31, 13.01s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  24%|‚ñà‚ñà‚ñç       | 137/573 [29:42<1:34:31, 13.01s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1546])
Epoch 3:  24%|‚ñà‚ñà‚ñç       | 138/573 [29:49<1:34:01, 12.97s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  24%|‚ñà‚ñà‚ñç       | 138/573 [29:49<1:34:01, 12.97s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1577])
Epoch 3:  24%|‚ñà‚ñà‚ñç       | 139/573 [30:12<1:34:19, 13.04s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  24%|‚ñà‚ñà‚ñç       | 139/573 [30:12<1:34:19, 13.04s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12130 examples [7:42:46,  2.33s/ examples]embeddings shape: torch.Size([32, 768, 1561])
Epoch 3:  24%|‚ñà‚ñà‚ñç       | 140/573 [30:27<1:34:12, 13.05s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  24%|‚ñà‚ñà‚ñç       | 140/573 [30:27<1:34:12, 13.05s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1649])
Epoch 3:  25%|‚ñà‚ñà‚ñç       | 141/573 [30:34<1:33:39, 13.01s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  25%|‚ñà‚ñà‚ñç       | 141/573 [30:34<1:33:39, 13.01s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.812, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1585])
Epoch 3:  25%|‚ñà‚ñà‚ñç       | 142/573 [30:42<1:33:13, 12.98s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.812, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  25%|‚ñà‚ñà‚ñç       | 142/573 [30:42<1:33:13, 12.98s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 9897 examples [7:42:33,  2.81s/ examples]embeddings shape: torch.Size([32, 768, 1563])
Epoch 3:  25%|‚ñà‚ñà‚ñç       | 143/573 [31:14<1:33:56, 13.11s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  25%|‚ñà‚ñà‚ñç       | 143/573 [31:14<1:33:56, 13.11s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1593])
Epoch 3:  25%|‚ñà‚ñà‚ñå       | 144/573 [31:23<1:33:31, 13.08s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  25%|‚ñà‚ñà‚ñå       | 144/573 [31:23<1:33:31, 13.08s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1622])
Epoch 3:  25%|‚ñà‚ñà‚ñå       | 145/573 [31:30<1:32:59, 13.04s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  25%|‚ñà‚ñà‚ñå       | 145/573 [31:30<1:32:59, 13.04s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1555])
Epoch 3:  25%|‚ñà‚ñà‚ñå       | 146/573 [31:38<1:32:31, 13.00s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  25%|‚ñà‚ñà‚ñå       | 146/573 [31:38<1:32:31, 13.00s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1584])
Epoch 3:  26%|‚ñà‚ñà‚ñå       | 147/573 [31:44<1:31:58, 12.95s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  26%|‚ñà‚ñà‚ñå       | 147/573 [31:44<1:31:58, 12.95s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1628])
Epoch 3:  26%|‚ñà‚ñà‚ñå       | 148/573 [32:09<1:32:20, 13.04s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  26%|‚ñà‚ñà‚ñå       | 148/573 [32:09<1:32:20, 13.04s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12208 examples [7:45:16,  1.99s/ examples]embeddings shape: torch.Size([32, 768, 1563])
Epoch 3:  26%|‚ñà‚ñà‚ñå       | 149/573 [32:39<1:32:56, 13.15s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  26%|‚ñà‚ñà‚ñå       | 149/573 [32:39<1:32:56, 13.15s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12190 examples [7:45:20,  2.40s/ examples]embeddings shape: torch.Size([32, 768, 1580])
Epoch 3:  26%|‚ñà‚ñà‚ñå       | 150/573 [33:00<1:33:05, 13.20s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  26%|‚ñà‚ñà‚ñå       | 150/573 [33:00<1:33:05, 13.20s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1608])
Epoch 3:  26%|‚ñà‚ñà‚ñã       | 151/573 [33:07<1:32:34, 13.16s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  26%|‚ñà‚ñà‚ñã       | 151/573 [33:07<1:32:34, 13.16s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1566])
Epoch 3:  27%|‚ñà‚ñà‚ñã       | 152/573 [33:20<1:32:22, 13.16s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  27%|‚ñà‚ñà‚ñã       | 152/573 [33:20<1:32:22, 13.16s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 9946 examples [7:45:03,  2.89s/ examples]embeddings shape: torch.Size([32, 768, 1575])
Epoch 3:  27%|‚ñà‚ñà‚ñã       | 153/573 [33:43<1:32:34, 13.22s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  27%|‚ñà‚ñà‚ñã       | 153/573 [33:43<1:32:34, 13.22s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1089])
Epoch 3:  27%|‚ñà‚ñà‚ñã       | 154/573 [33:48<1:31:59, 13.17s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  27%|‚ñà‚ñà‚ñã       | 154/573 [33:48<1:31:59, 13.17s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1647])
Epoch 3:  27%|‚ñà‚ñà‚ñã       | 155/573 [33:56<1:31:32, 13.14s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  27%|‚ñà‚ñà‚ñã       | 155/573 [33:56<1:31:32, 13.14s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1607])
Epoch 3:  27%|‚ñà‚ñà‚ñã       | 156/573 [34:04<1:31:04, 13.10s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  27%|‚ñà‚ñà‚ñã       | 156/573 [34:04<1:31:04, 13.10s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1578])
Epoch 3:  27%|‚ñà‚ñà‚ñã       | 157/573 [34:10<1:30:33, 13.06s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  27%|‚ñà‚ñà‚ñã       | 157/573 [34:10<1:30:33, 13.06s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1630])
Epoch 3:  28%|‚ñà‚ñà‚ñä       | 158/573 [34:18<1:30:07, 13.03s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  28%|‚ñà‚ñà‚ñä       | 158/573 [34:18<1:30:07, 13.03s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1594])
Epoch 3:  28%|‚ñà‚ñà‚ñä       | 159/573 [34:41<1:30:19, 13.09s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  28%|‚ñà‚ñà‚ñä       | 159/573 [34:41<1:30:19, 13.09s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.816, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12250 examples [7:47:16,  2.26s/ examples]embeddings shape: torch.Size([32, 768, 1611])
Epoch 3:  28%|‚ñà‚ñà‚ñä       | 160/573 [34:58<1:30:16, 13.11s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.816, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  28%|‚ñà‚ñà‚ñä       | 160/573 [34:58<1:30:16, 13.11s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1650])
Epoch 3:  28%|‚ñà‚ñà‚ñä       | 161/573 [35:06<1:29:50, 13.08s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  28%|‚ñà‚ñà‚ñä       | 161/573 [35:06<1:29:50, 13.08s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 9995 examples [7:46:59,  2.73s/ examples]embeddings shape: torch.Size([32, 768, 1663])
Epoch 3:  28%|‚ñà‚ñà‚ñä       | 162/573 [35:39<1:30:26, 13.20s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  28%|‚ñà‚ñà‚ñä       | 162/573 [35:39<1:30:26, 13.20s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1618])
Epoch 3:  28%|‚ñà‚ñà‚ñä       | 163/573 [36:13<1:31:07, 13.34s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  28%|‚ñà‚ñà‚ñä       | 163/573 [36:13<1:31:07, 13.34s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12274 examples [7:49:00,  2.41s/ examples]embeddings shape: torch.Size([32, 768, 1586])
Epoch 3:  29%|‚ñà‚ñà‚ñä       | 164/573 [36:30<1:31:04, 13.36s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  29%|‚ñà‚ñà‚ñä       | 164/573 [36:30<1:31:04, 13.36s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1411])
Epoch 3:  29%|‚ñà‚ñà‚ñâ       | 165/573 [36:37<1:30:32, 13.32s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  29%|‚ñà‚ñà‚ñâ       | 165/573 [36:37<1:30:32, 13.32s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1584])
Epoch 3:  29%|‚ñà‚ñà‚ñâ       | 166/573 [36:44<1:30:04, 13.28s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  29%|‚ñà‚ñà‚ñâ       | 166/573 [36:44<1:30:04, 13.28s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1671])
Epoch 3:  29%|‚ñà‚ñà‚ñâ       | 167/573 [36:53<1:29:42, 13.26s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  29%|‚ñà‚ñà‚ñâ       | 167/573 [36:53<1:29:42, 13.26s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1404])
Epoch 3:  29%|‚ñà‚ñà‚ñâ       | 168/573 [37:24<1:30:10, 13.36s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  29%|‚ñà‚ñà‚ñâ       | 168/573 [37:24<1:30:10, 13.36s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12310 examples [7:49:51,  2.36s/ examples]embeddings shape: torch.Size([32, 768, 1613])
Epoch 3:  29%|‚ñà‚ñà‚ñâ       | 169/573 [37:36<1:29:53, 13.35s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  29%|‚ñà‚ñà‚ñâ       | 169/573 [37:36<1:29:53, 13.35s/it, loss=0.31, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1582])
Epoch 3:  30%|‚ñà‚ñà‚ñâ       | 170/573 [37:56<1:29:56, 13.39s/it, loss=0.31, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  30%|‚ñà‚ñà‚ñâ       | 170/573 [37:56<1:29:56, 13.39s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 10044 examples [7:49:36,  2.88s/ examples]embeddings shape: torch.Size([32, 768, 1618])
Epoch 3:  30%|‚ñà‚ñà‚ñâ       | 171/573 [38:16<1:29:59, 13.43s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  30%|‚ñà‚ñà‚ñâ       | 171/573 [38:16<1:29:59, 13.43s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1694])
Epoch 3:  30%|‚ñà‚ñà‚ñà       | 172/573 [38:25<1:29:34, 13.40s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  30%|‚ñà‚ñà‚ñà       | 172/573 [38:25<1:29:34, 13.40s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1608])
Epoch 3:  30%|‚ñà‚ñà‚ñà       | 173/573 [38:33<1:29:10, 13.38s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  30%|‚ñà‚ñà‚ñà       | 173/573 [38:33<1:29:10, 13.38s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1602])
Epoch 3:  30%|‚ñà‚ñà‚ñà       | 174/573 [38:41<1:28:44, 13.34s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  30%|‚ñà‚ñà‚ñà       | 174/573 [38:41<1:28:44, 13.34s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1595])
Epoch 3:  31%|‚ñà‚ñà‚ñà       | 175/573 [38:49<1:28:18, 13.31s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  31%|‚ñà‚ñà‚ñà       | 175/573 [38:49<1:28:18, 13.31s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1660])
Epoch 3:  31%|‚ñà‚ñà‚ñà       | 176/573 [39:09<1:28:19, 13.35s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  31%|‚ñà‚ñà‚ñà       | 176/573 [39:09<1:28:19, 13.35s/it, loss=0.31, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1605])
Epoch 3:  31%|‚ñà‚ñà‚ñà       | 177/573 [39:40<1:28:46, 13.45s/it, loss=0.31, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  31%|‚ñà‚ñà‚ñà       | 177/573 [39:40<1:28:46, 13.45s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12370 examples [7:52:20,  2.39s/ examples]Generating train split: 12340 examples [7:52:51,  2.74s/ examples]embeddings shape: torch.Size([32, 768, 1644])
Epoch 3:  31%|‚ñà‚ñà‚ñà       | 178/573 [40:14<1:29:19, 13.57s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  31%|‚ñà‚ñà‚ñà       | 178/573 [40:14<1:29:19, 13.57s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1531])
Epoch 3:  31%|‚ñà‚ñà‚ñà       | 179/573 [40:39<1:29:28, 13.63s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  31%|‚ñà‚ñà‚ñà       | 179/573 [40:39<1:29:28, 13.63s/it, loss=0.307, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 10093 examples [7:52:12,  2.97s/ examples]embeddings shape: torch.Size([32, 768, 1431])
Epoch 3:  31%|‚ñà‚ñà‚ñà‚ñè      | 180/573 [40:54<1:29:19, 13.64s/it, loss=0.307, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  31%|‚ñà‚ñà‚ñà‚ñè      | 180/573 [40:54<1:29:19, 13.64s/it, loss=0.308, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1605])
Epoch 3:  32%|‚ñà‚ñà‚ñà‚ñè      | 181/573 [41:03<1:28:55, 13.61s/it, loss=0.308, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  32%|‚ñà‚ñà‚ñà‚ñè      | 181/573 [41:03<1:28:55, 13.61s/it, loss=0.31, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1616])
Epoch 3:  32%|‚ñà‚ñà‚ñà‚ñè      | 182/573 [41:10<1:28:26, 13.57s/it, loss=0.31, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  32%|‚ñà‚ñà‚ñà‚ñè      | 182/573 [41:10<1:28:26, 13.57s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1577])
Epoch 3:  32%|‚ñà‚ñà‚ñà‚ñè      | 183/573 [41:17<1:28:00, 13.54s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  32%|‚ñà‚ñà‚ñà‚ñè      | 183/573 [41:17<1:28:00, 13.54s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1604])
Epoch 3:  32%|‚ñà‚ñà‚ñà‚ñè      | 184/573 [41:25<1:27:35, 13.51s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  32%|‚ñà‚ñà‚ñà‚ñè      | 184/573 [41:25<1:27:35, 13.51s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1636])
Epoch 3:  32%|‚ñà‚ñà‚ñà‚ñè      | 185/573 [41:40<1:27:24, 13.52s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  32%|‚ñà‚ñà‚ñà‚ñè      | 185/573 [41:40<1:27:24, 13.52s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12406 examples [7:54:36,  2.39s/ examples]embeddings shape: torch.Size([32, 768, 1469])
Epoch 3:  32%|‚ñà‚ñà‚ñà‚ñè      | 186/573 [42:09<1:27:43, 13.60s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  32%|‚ñà‚ñà‚ñà‚ñè      | 186/573 [42:09<1:27:43, 13.60s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12430 examples [7:54:55,  2.45s/ examples]embeddings shape: torch.Size([32, 768, 1651])
Epoch 3:  33%|‚ñà‚ñà‚ñà‚ñé      | 187/573 [42:34<1:27:53, 13.66s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  33%|‚ñà‚ñà‚ñà‚ñé      | 187/573 [42:34<1:27:53, 13.66s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.688, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1602])
Epoch 3:  33%|‚ñà‚ñà‚ñà‚ñé      | 188/573 [43:03<1:28:10, 13.74s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.688, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  33%|‚ñà‚ñà‚ñà‚ñé      | 188/573 [43:03<1:28:10, 13.74s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 10142 examples [7:54:31,  2.93s/ examples]embeddings shape: torch.Size([32, 768, 1591])
Epoch 3:  33%|‚ñà‚ñà‚ñà‚ñé      | 189/573 [43:15<1:27:53, 13.73s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  33%|‚ñà‚ñà‚ñà‚ñé      | 189/573 [43:15<1:27:53, 13.73s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1558])
Epoch 3:  33%|‚ñà‚ñà‚ñà‚ñé      | 190/573 [43:23<1:27:29, 13.71s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  33%|‚ñà‚ñà‚ñà‚ñé      | 190/573 [43:23<1:27:29, 13.71s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1615])
Epoch 3:  33%|‚ñà‚ñà‚ñà‚ñé      | 191/573 [43:31<1:27:02, 13.67s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  33%|‚ñà‚ñà‚ñà‚ñé      | 191/573 [43:31<1:27:02, 13.67s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1454])
Epoch 3:  34%|‚ñà‚ñà‚ñà‚ñé      | 192/573 [43:38<1:26:36, 13.64s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  34%|‚ñà‚ñà‚ñà‚ñé      | 192/573 [43:38<1:26:36, 13.64s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.906, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12472 examples [7:56:42,  2.25s/ examples]embeddings shape: torch.Size([32, 768, 1670])
Epoch 3:  34%|‚ñà‚ñà‚ñà‚ñé      | 193/573 [44:07<1:26:52, 13.72s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.906, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  34%|‚ñà‚ñà‚ñà‚ñé      | 193/573 [44:07<1:26:52, 13.72s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1571])
Epoch 3:  34%|‚ñà‚ñà‚ñà‚ñç      | 194/573 [44:15<1:26:27, 13.69s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  34%|‚ñà‚ñà‚ñà‚ñç      | 194/573 [44:15<1:26:27, 13.69s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.806, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1618])
Epoch 3:  34%|‚ñà‚ñà‚ñà‚ñç      | 195/573 [44:42<1:26:39, 13.76s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.806, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  34%|‚ñà‚ñà‚ñà‚ñç      | 195/573 [44:42<1:26:39, 13.76s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12490 examples [7:57:13,  2.41s/ examples]embeddings shape: torch.Size([32, 768, 1573])
Epoch 3:  34%|‚ñà‚ñà‚ñà‚ñç      | 196/573 [45:02<1:26:38, 13.79s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  34%|‚ñà‚ñà‚ñà‚ñç      | 196/573 [45:02<1:26:38, 13.79s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 10191 examples [7:56:50,  2.90s/ examples]embeddings shape: torch.Size([32, 768, 1648])
Epoch 3:  34%|‚ñà‚ñà‚ñà‚ñç      | 197/573 [45:32<1:26:54, 13.87s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  34%|‚ñà‚ñà‚ñà‚ñç      | 197/573 [45:32<1:26:54, 13.87s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1579])
Epoch 3:  35%|‚ñà‚ñà‚ñà‚ñç      | 198/573 [45:42<1:26:33, 13.85s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  35%|‚ñà‚ñà‚ñà‚ñç      | 198/573 [45:42<1:26:33, 13.85s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1572])
Epoch 3:  35%|‚ñà‚ñà‚ñà‚ñç      | 199/573 [45:49<1:26:07, 13.82s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  35%|‚ñà‚ñà‚ñà‚ñç      | 199/573 [45:49<1:26:07, 13.82s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12538 examples [7:58:57,  2.19s/ examples]embeddings shape: torch.Size([32, 768, 1672])
Epoch 3:  35%|‚ñà‚ñà‚ñà‚ñç      | 200/573 [46:20<1:26:26, 13.90s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  35%|‚ñà‚ñà‚ñà‚ñç      | 200/573 [46:20<1:26:26, 13.90s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1584])
Epoch 3:  35%|‚ñà‚ñà‚ñà‚ñå      | 201/573 [46:28<1:26:00, 13.87s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  35%|‚ñà‚ñà‚ñà‚ñå      | 201/573 [46:28<1:26:00, 13.87s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1485])
Epoch 3:  35%|‚ñà‚ñà‚ñà‚ñå      | 202/573 [46:36<1:25:35, 13.84s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  35%|‚ñà‚ñà‚ñà‚ñå      | 202/573 [46:36<1:25:35, 13.84s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1589])
Epoch 3:  35%|‚ñà‚ñà‚ñà‚ñå      | 203/573 [46:59<1:25:38, 13.89s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  35%|‚ñà‚ñà‚ñà‚ñå      | 203/573 [46:59<1:25:38, 13.89s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12550 examples [7:59:36,  2.40s/ examples]embeddings shape: torch.Size([32, 768, 1561])
Epoch 3:  36%|‚ñà‚ñà‚ñà‚ñå      | 204/573 [47:17<1:25:32, 13.91s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  36%|‚ñà‚ñà‚ñà‚ñå      | 204/573 [47:17<1:25:32, 13.91s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] Generating train split: 10240 examples [7:59:14,  2.91s/ examples]embeddings shape: torch.Size([32, 768, 1589])
Epoch 3:  36%|‚ñà‚ñà‚ñà‚ñå      | 205/573 [47:52<1:25:57, 14.01s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  36%|‚ñà‚ñà‚ñà‚ñå      | 205/573 [47:52<1:25:57, 14.01s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1593])
Epoch 3:  36%|‚ñà‚ñà‚ñà‚ñå      | 206/573 [47:59<1:25:29, 13.98s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  36%|‚ñà‚ñà‚ñà‚ñå      | 206/573 [47:59<1:25:29, 13.98s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1573])
Epoch 3:  36%|‚ñà‚ñà‚ñà‚ñå      | 207/573 [48:06<1:25:03, 13.94s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  36%|‚ñà‚ñà‚ñà‚ñå      | 207/573 [48:06<1:25:03, 13.94s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1602])
Epoch 3:  36%|‚ñà‚ñà‚ñà‚ñã      | 208/573 [48:14<1:24:38, 13.91s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  36%|‚ñà‚ñà‚ñà‚ñã      | 208/573 [48:14<1:24:38, 13.91s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1564])
Epoch 3:  36%|‚ñà‚ñà‚ñà‚ñã      | 209/573 [48:30<1:24:28, 13.92s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  36%|‚ñà‚ñà‚ñà‚ñã      | 209/573 [48:30<1:24:28, 13.92s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.906, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12604 examples [8:01:29,  2.22s/ examples]embeddings shape: torch.Size([32, 768, 1556])
Epoch 3:  37%|‚ñà‚ñà‚ñà‚ñã      | 210/573 [48:55<1:24:33, 13.98s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.906, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  37%|‚ñà‚ñà‚ñà‚ñã      | 210/573 [48:55<1:24:33, 13.98s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1449])
Epoch 3:  37%|‚ñà‚ñà‚ñà‚ñã      | 211/573 [49:01<1:24:06, 13.94s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  37%|‚ñà‚ñà‚ñà‚ñã      | 211/573 [49:01<1:24:06, 13.94s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1592])
Epoch 3:  37%|‚ñà‚ñà‚ñà‚ñã      | 212/573 [49:12<1:23:48, 13.93s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  37%|‚ñà‚ñà‚ñà‚ñã      | 212/573 [49:12<1:23:48, 13.93s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12610 examples [8:02:02,  2.41s/ examples]embeddings shape: torch.Size([32, 768, 1593])
Epoch 3:  37%|‚ñà‚ñà‚ñà‚ñã      | 213/573 [49:42<1:24:00, 14.00s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  37%|‚ñà‚ñà‚ñà‚ñã      | 213/573 [49:42<1:24:00, 14.00s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1619])
Epoch 3:  37%|‚ñà‚ñà‚ñà‚ñã      | 214/573 [49:58<1:23:50, 14.01s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  37%|‚ñà‚ñà‚ñà‚ñã      | 214/573 [49:58<1:23:50, 14.01s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 10289 examples [8:01:39,  2.92s/ examples]embeddings shape: torch.Size([32, 768, 1561])
Epoch 3:  38%|‚ñà‚ñà‚ñà‚ñä      | 215/573 [50:21<1:23:50, 14.05s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  38%|‚ñà‚ñà‚ñà‚ñä      | 215/573 [50:21<1:23:50, 14.05s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1433])
Epoch 3:  38%|‚ñà‚ñà‚ñà‚ñä      | 216/573 [50:27<1:23:24, 14.02s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  38%|‚ñà‚ñà‚ñà‚ñä      | 216/573 [50:27<1:23:24, 14.02s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1625])
Epoch 3:  38%|‚ñà‚ñà‚ñà‚ñä      | 217/573 [50:34<1:22:58, 13.99s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  38%|‚ñà‚ñà‚ñà‚ñä      | 217/573 [50:34<1:22:58, 13.99s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1585])
Epoch 3:  38%|‚ñà‚ñà‚ñà‚ñä      | 218/573 [50:42<1:22:35, 13.96s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  38%|‚ñà‚ñà‚ñà‚ñä      | 218/573 [50:42<1:22:35, 13.96s/it, loss=0.307, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1605])
Epoch 3:  38%|‚ñà‚ñà‚ñà‚ñä      | 219/573 [50:49<1:22:08, 13.92s/it, loss=0.307, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  38%|‚ñà‚ñà‚ñà‚ñä      | 219/573 [50:49<1:22:08, 13.92s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1587])
Epoch 3:  38%|‚ñà‚ñà‚ñà‚ñä      | 220/573 [50:56<1:21:44, 13.89s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  38%|‚ñà‚ñà‚ñà‚ñä      | 220/573 [50:56<1:21:44, 13.89s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1241])
Epoch 3:  39%|‚ñà‚ñà‚ñà‚ñä      | 221/573 [51:02<1:21:17, 13.86s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  39%|‚ñà‚ñà‚ñà‚ñä      | 221/573 [51:02<1:21:17, 13.86s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1566])
Epoch 3:  39%|‚ñà‚ñà‚ñà‚ñä      | 222/573 [51:30<1:21:26, 13.92s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  39%|‚ñà‚ñà‚ñà‚ñä      | 222/573 [51:30<1:21:26, 13.92s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1598])
Epoch 3:  39%|‚ñà‚ñà‚ñà‚ñâ      | 223/573 [52:04<1:21:43, 14.01s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  39%|‚ñà‚ñà‚ñà‚ñâ      | 223/573 [52:04<1:21:43, 14.01s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12670 examples [8:04:29,  2.42s/ examples]Generating train split: 12670 examples [8:04:57,  2.50s/ examples]embeddings shape: torch.Size([32, 768, 1276])
Epoch 3:  39%|‚ñà‚ñà‚ñà‚ñâ      | 224/573 [52:36<1:21:58, 14.09s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  39%|‚ñà‚ñà‚ñà‚ñâ      | 224/573 [52:36<1:21:58, 14.09s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 10338 examples [8:04:09,  2.97s/ examples]embeddings shape: torch.Size([32, 768, 1568])
Epoch 3:  39%|‚ñà‚ñà‚ñà‚ñâ      | 225/573 [52:52<1:21:46, 14.10s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  39%|‚ñà‚ñà‚ñà‚ñâ      | 225/573 [52:52<1:21:46, 14.10s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1619])
Epoch 3:  39%|‚ñà‚ñà‚ñà‚ñâ      | 226/573 [53:00<1:21:23, 14.07s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  39%|‚ñà‚ñà‚ñà‚ñâ      | 226/573 [53:00<1:21:23, 14.07s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1560])
Epoch 3:  40%|‚ñà‚ñà‚ñà‚ñâ      | 227/573 [53:08<1:21:00, 14.05s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  40%|‚ñà‚ñà‚ñà‚ñâ      | 227/573 [53:08<1:21:00, 14.05s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1466])
Epoch 3:  40%|‚ñà‚ñà‚ñà‚ñâ      | 228/573 [53:15<1:20:35, 14.02s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  40%|‚ñà‚ñà‚ñà‚ñâ      | 228/573 [53:15<1:20:35, 14.02s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1598])
Epoch 3:  40%|‚ñà‚ñà‚ñà‚ñâ      | 229/573 [53:22<1:20:10, 13.98s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  40%|‚ñà‚ñà‚ñà‚ñâ      | 229/573 [53:22<1:20:10, 13.98s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1602])
Epoch 3:  40%|‚ñà‚ñà‚ñà‚ñà      | 230/573 [53:39<1:20:01, 14.00s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  40%|‚ñà‚ñà‚ñà‚ñà      | 230/573 [53:39<1:20:01, 14.00s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12736 examples [8:06:26,  2.15s/ examples]embeddings shape: torch.Size([32, 768, 1599])
Epoch 3:  40%|‚ñà‚ñà‚ñà‚ñà      | 231/573 [53:54<1:19:49, 14.00s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  40%|‚ñà‚ñà‚ñà‚ñà      | 231/573 [53:54<1:19:49, 14.00s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1612])
Epoch 3:  40%|‚ñà‚ñà‚ñà‚ñà      | 232/573 [54:19<1:19:50, 14.05s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  40%|‚ñà‚ñà‚ñà‚ñà      | 232/573 [54:19<1:19:50, 14.05s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12730 examples [8:06:48,  2.39s/ examples]embeddings shape: torch.Size([32, 768, 1628])
Epoch 3:  41%|‚ñà‚ñà‚ñà‚ñà      | 233/573 [54:41<1:19:48, 14.08s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  41%|‚ñà‚ñà‚ñà‚ñà      | 233/573 [54:41<1:19:48, 14.08s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 10387 examples [8:06:21,  2.89s/ examples]embeddings shape: torch.Size([32, 768, 1631])
Epoch 3:  41%|‚ñà‚ñà‚ñà‚ñà      | 234/573 [55:04<1:19:47, 14.12s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  41%|‚ñà‚ñà‚ñà‚ñà      | 234/573 [55:04<1:19:47, 14.12s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1647])
Epoch 3:  41%|‚ñà‚ñà‚ñà‚ñà      | 235/573 [55:11<1:19:23, 14.09s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  41%|‚ñà‚ñà‚ñà‚ñà      | 235/573 [55:11<1:19:23, 14.09s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1625])
Epoch 3:  41%|‚ñà‚ñà‚ñà‚ñà      | 236/573 [55:19<1:19:00, 14.07s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  41%|‚ñà‚ñà‚ñà‚ñà      | 236/573 [55:19<1:19:00, 14.07s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12802 examples [8:08:16,  2.01s/ examples]embeddings shape: torch.Size([32, 768, 1608])
Epoch 3:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 237/573 [55:42<1:18:58, 14.10s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 237/573 [55:42<1:18:58, 14.10s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1488])
Epoch 3:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 238/573 [55:48<1:18:33, 14.07s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 238/573 [55:48<1:18:33, 14.07s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1575])
Epoch 3:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 239/573 [55:55<1:18:09, 14.04s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 239/573 [55:55<1:18:09, 14.04s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1600])
Epoch 3:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 240/573 [56:06<1:17:50, 14.03s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 240/573 [56:06<1:17:50, 14.03s/it, loss=0.31, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] Generating train split: 12790 examples [8:08:55,  2.31s/ examples]embeddings shape: torch.Size([32, 768, 1598])
Epoch 3:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 241/573 [56:33<1:17:55, 14.08s/it, loss=0.31, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 241/573 [56:33<1:17:55, 14.08s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1408])
Epoch 3:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 242/573 [56:52<1:17:47, 14.10s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 242/573 [56:52<1:17:47, 14.10s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 10436 examples [8:08:30,  2.81s/ examples]embeddings shape: torch.Size([32, 768, 1588])
Epoch 3:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 243/573 [57:13<1:17:42, 14.13s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 243/573 [57:13<1:17:42, 14.13s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1556])
Epoch 3:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 244/573 [57:20<1:17:19, 14.10s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 244/573 [57:20<1:17:19, 14.10s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1632])
Epoch 3:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 245/573 [57:27<1:16:54, 14.07s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 245/573 [57:27<1:16:54, 14.07s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1609])
Epoch 3:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 246/573 [57:38<1:16:36, 14.06s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 246/573 [57:38<1:16:36, 14.06s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12868 examples [8:10:45,  2.08s/ examples]embeddings shape: torch.Size([32, 768, 1110])
Epoch 3:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 247/573 [58:08<1:16:44, 14.12s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 247/573 [58:08<1:16:44, 14.12s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1598])
Epoch 3:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 248/573 [58:16<1:16:21, 14.10s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 248/573 [58:16<1:16:21, 14.10s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1568])
Epoch 3:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 249/573 [58:30<1:16:08, 14.10s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 249/573 [58:30<1:16:08, 14.10s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12850 examples [8:11:17,  2.33s/ examples]embeddings shape: torch.Size([32, 768, 1563])
Epoch 3:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 250/573 [58:57<1:16:09, 14.15s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 250/573 [58:57<1:16:09, 14.15s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1577])
Epoch 3:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 251/573 [59:20<1:16:07, 14.18s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 251/573 [59:20<1:16:07, 14.18s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 10485 examples [8:10:53,  2.84s/ examples]embeddings shape: torch.Size([32, 768, 1620])
Epoch 3:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 252/573 [59:35<1:15:55, 14.19s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 252/573 [59:35<1:15:55, 14.19s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1630])
Epoch 3:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 253/573 [59:44<1:15:34, 14.17s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 253/573 [59:44<1:15:34, 14.17s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1623])
Epoch 3:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 254/573 [59:51<1:15:10, 14.14s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 254/573 [59:51<1:15:10, 14.14s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1610])
Epoch 3:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 255/573 [59:58<1:14:46, 14.11s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 255/573 [59:58<1:14:46, 14.11s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1600])
Epoch 3:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 256/573 [1:00:05<1:14:24, 14.08s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 256/573 [1:00:05<1:14:24, 14.08s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1645])
Epoch 3:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 257/573 [1:00:11<1:14:00, 14.05s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 257/573 [1:00:11<1:14:00, 14.05s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1323])
Epoch 3:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 258/573 [1:00:33<1:13:55, 14.08s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 258/573 [1:00:33<1:13:55, 14.08s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1617])
Epoch 3:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 259/573 [1:01:08<1:14:08, 14.17s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 259/573 [1:01:08<1:14:08, 14.17s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12934 examples [8:13:53,  2.31s/ examples]Generating train split: 12910 examples [8:13:45,  2.37s/ examples]embeddings shape: torch.Size([32, 768, 1597])
Epoch 3:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 260/573 [1:01:37<1:14:10, 14.22s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 260/573 [1:01:37<1:14:10, 14.22s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 10534 examples [8:13:16,  2.87s/ examples]embeddings shape: torch.Size([32, 768, 1603])
Epoch 3:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 261/573 [1:01:58<1:14:05, 14.25s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 261/573 [1:01:58<1:14:05, 14.25s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1582])
Epoch 3:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 262/573 [1:02:05<1:13:41, 14.22s/it, loss=0.333, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.826, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 262/573 [1:02:05<1:13:41, 14.22s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1588])
Epoch 3:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 263/573 [1:02:12<1:13:19, 14.19s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 263/573 [1:02:12<1:13:19, 14.19s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1588])
Epoch 3:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 264/573 [1:02:19<1:12:56, 14.16s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 264/573 [1:02:19<1:12:56, 14.16s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1553])
Epoch 3:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 265/573 [1:02:26<1:12:34, 14.14s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 265/573 [1:02:26<1:12:34, 14.14s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1579])
Epoch 3:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 266/573 [1:02:34<1:12:13, 14.11s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 266/573 [1:02:34<1:12:13, 14.11s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1599])
Epoch 3:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 267/573 [1:02:40<1:11:49, 14.08s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 267/573 [1:02:40<1:11:49, 14.08s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1547])
Epoch 3:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 268/573 [1:03:06<1:11:48, 14.13s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 268/573 [1:03:06<1:11:48, 14.13s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12970 examples [8:15:37,  2.22s/ examples]embeddings shape: torch.Size([32, 768, 1603])
Epoch 3:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 269/573 [1:03:38<1:11:55, 14.20s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 269/573 [1:03:38<1:11:55, 14.20s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 10583 examples [8:15:07,  2.69s/ examples]embeddings shape: torch.Size([32, 768, 1698])
Epoch 3:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 270/573 [1:04:00<1:11:50, 14.22s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 270/573 [1:04:00<1:11:50, 14.22s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1263])
Epoch 3:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 271/573 [1:04:31<1:11:54, 14.28s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 271/573 [1:04:31<1:11:54, 14.28s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] Generating train split: 13000 examples [8:17:08,  2.51s/ examples]embeddings shape: torch.Size([32, 768, 1531])
Epoch 3:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 272/573 [1:04:39<1:11:33, 14.26s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 272/573 [1:04:39<1:11:33, 14.26s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1550])
Epoch 3:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 273/573 [1:04:47<1:11:11, 14.24s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 273/573 [1:04:47<1:11:11, 14.24s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1609])
Epoch 3:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 274/573 [1:04:56<1:10:51, 14.22s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 274/573 [1:04:56<1:10:51, 14.22s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1624])
Epoch 3:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 275/573 [1:05:04<1:10:30, 14.20s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 275/573 [1:05:04<1:10:30, 14.20s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1645])
Epoch 3:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 276/573 [1:05:12<1:10:10, 14.18s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 276/573 [1:05:12<1:10:10, 14.18s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1590])
Epoch 3:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 277/573 [1:05:47<1:10:18, 14.25s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 277/573 [1:05:47<1:10:18, 14.25s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13030 examples [8:18:13,  2.33s/ examples]Generating train split: 10632 examples [8:17:37,  2.80s/ examples]embeddings shape: torch.Size([32, 768, 1571])
Epoch 3:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 278/573 [1:06:17<1:10:20, 14.31s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 278/573 [1:06:17<1:10:20, 14.31s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1590])
Epoch 3:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 279/573 [1:06:25<1:09:59, 14.28s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 279/573 [1:06:25<1:09:59, 14.28s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1156])
Epoch 3:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 280/573 [1:06:31<1:09:36, 14.26s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 280/573 [1:06:31<1:09:36, 14.26s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1592])
Epoch 3:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 281/573 [1:07:06<1:09:43, 14.33s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 281/573 [1:07:06<1:09:43, 14.33s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13066 examples [8:19:46,  2.47s/ examples]embeddings shape: torch.Size([32, 768, 1344])
Epoch 3:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 282/573 [1:07:14<1:09:23, 14.31s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 282/573 [1:07:14<1:09:23, 14.31s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1683])
Epoch 3:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 283/573 [1:07:23<1:09:03, 14.29s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 283/573 [1:07:23<1:09:03, 14.29s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1581])
Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 284/573 [1:07:31<1:08:43, 14.27s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 284/573 [1:07:31<1:08:43, 14.27s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1611])
Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 285/573 [1:07:39<1:08:22, 14.24s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 285/573 [1:07:39<1:08:22, 14.24s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1587])
Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 286/573 [1:08:05<1:08:20, 14.29s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 286/573 [1:08:05<1:08:20, 14.29s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13090 examples [8:20:43,  2.38s/ examples]embeddings shape: torch.Size([32, 768, 1588])
Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 287/573 [1:08:40<1:08:25, 14.36s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 287/573 [1:08:40<1:08:25, 14.36s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 10681 examples [8:20:05,  2.86s/ examples]embeddings shape: torch.Size([32, 768, 1576])
Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 288/573 [1:08:50<1:08:07, 14.34s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 288/573 [1:08:50<1:08:07, 14.34s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1571])
Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 289/573 [1:08:57<1:07:46, 14.32s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 289/573 [1:08:57<1:07:46, 14.32s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1592])
Epoch 3:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 290/573 [1:09:07<1:07:26, 14.30s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 290/573 [1:09:07<1:07:26, 14.30s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.816, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1581])
Epoch 3:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 291/573 [1:09:17<1:07:09, 14.29s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.816, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 291/573 [1:09:17<1:07:09, 14.29s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1527])
Epoch 3:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 292/573 [1:09:52<1:07:14, 14.36s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 292/573 [1:09:52<1:07:14, 14.36s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13132 examples [8:22:31,  2.48s/ examples]embeddings shape: torch.Size([32, 768, 1645])
Epoch 3:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 293/573 [1:10:05<1:06:58, 14.35s/it, loss=0.331, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 293/573 [1:10:05<1:06:58, 14.35s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1591])
Epoch 3:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 294/573 [1:10:12<1:06:37, 14.33s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 294/573 [1:10:12<1:06:37, 14.33s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1313])
Epoch 3:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 295/573 [1:10:38<1:06:34, 14.37s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 295/573 [1:10:38<1:06:34, 14.37s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13150 examples [8:23:14,  2.42s/ examples]Generating train split: 10730 examples [8:22:35,  2.92s/ examples]embeddings shape: torch.Size([32, 768, 1590])
Epoch 3:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 296/573 [1:11:13<1:06:39, 14.44s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 296/573 [1:11:13<1:06:39, 14.44s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1565])
Epoch 3:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 297/573 [1:11:20<1:06:18, 14.41s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 297/573 [1:11:20<1:06:18, 14.41s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13198 examples [8:24:15,  2.21s/ examples]embeddings shape: torch.Size([32, 768, 1621])
Epoch 3:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 298/573 [1:11:40<1:06:08, 14.43s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 298/573 [1:11:40<1:06:08, 14.43s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1595])
Epoch 3:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 299/573 [1:11:48<1:05:48, 14.41s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 299/573 [1:11:48<1:05:48, 14.41s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1586])
Epoch 3:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 300/573 [1:11:55<1:05:27, 14.39s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 300/573 [1:11:55<1:05:27, 14.39s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1577])
Epoch 3:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 301/573 [1:12:05<1:05:08, 14.37s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 301/573 [1:12:05<1:05:08, 14.37s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1627])
Epoch 3:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 302/573 [1:12:23<1:04:57, 14.38s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 302/573 [1:12:23<1:04:57, 14.38s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.785, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13264 examples [8:25:01,  1.75s/ examples]embeddings shape: torch.Size([32, 768, 1631])
Epoch 3:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 303/573 [1:12:32<1:04:38, 14.36s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.785, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 303/573 [1:12:32<1:04:38, 14.36s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1655])
Epoch 3:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 304/573 [1:13:03<1:04:38, 14.42s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 304/573 [1:13:03<1:04:38, 14.42s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13210 examples [8:25:39,  2.42s/ examples]embeddings shape: torch.Size([32, 768, 1535])
Epoch 3:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 305/573 [1:13:27<1:04:32, 14.45s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 305/573 [1:13:27<1:04:32, 14.45s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.906, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 10779 examples [8:24:52,  2.89s/ examples]embeddings shape: torch.Size([32, 768, 1664])
Epoch 3:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 306/573 [1:13:39<1:04:15, 14.44s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.906, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 306/573 [1:13:39<1:04:15, 14.44s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1634])
Epoch 3:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 307/573 [1:13:47<1:03:56, 14.42s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 307/573 [1:13:47<1:03:56, 14.42s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1600])
Epoch 3:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 308/573 [1:13:54<1:03:35, 14.40s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 308/573 [1:13:54<1:03:35, 14.40s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1584])
Epoch 3:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 309/573 [1:14:01<1:03:15, 14.38s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 309/573 [1:14:01<1:03:15, 14.38s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1583])
Epoch 3:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 310/573 [1:14:10<1:02:55, 14.36s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 310/573 [1:14:10<1:02:55, 14.36s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1605])
Epoch 3:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 311/573 [1:14:18<1:02:35, 14.34s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 311/573 [1:14:18<1:02:35, 14.34s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1644])
Epoch 3:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 312/573 [1:14:26<1:02:16, 14.32s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.892, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 312/573 [1:14:26<1:02:16, 14.32s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1654])
Epoch 3:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 313/573 [1:14:53<1:02:12, 14.35s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 313/573 [1:14:53<1:02:12, 14.35s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1594])
Epoch 3:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 314/573 [1:15:27<1:02:14, 14.42s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 314/573 [1:15:27<1:02:14, 14.42s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13270 examples [8:27:59,  2.39s/ examples]Generating train split: 10828 examples [8:27:12,  2.88s/ examples]embeddings shape: torch.Size([32, 768, 1551])
Epoch 3:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 315/573 [1:16:00<1:02:15, 14.48s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 315/573 [1:16:00<1:02:15, 14.48s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13330 examples [8:28:37,  2.21s/ examples]embeddings shape: torch.Size([32, 768, 1590])
Epoch 3:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 316/573 [1:16:08<1:01:55, 14.46s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 316/573 [1:16:08<1:01:55, 14.46s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1608])
Epoch 3:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 317/573 [1:16:15<1:01:35, 14.43s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 317/573 [1:16:15<1:01:35, 14.43s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1551])
Epoch 3:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 318/573 [1:16:22<1:01:14, 14.41s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 318/573 [1:16:22<1:01:14, 14.41s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1595])
Epoch 3:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 319/573 [1:16:31<1:00:56, 14.39s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 319/573 [1:16:31<1:00:56, 14.39s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1544])
Epoch 3:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 320/573 [1:16:38<1:00:35, 14.37s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.819, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 320/573 [1:16:38<1:00:35, 14.37s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1659])
Epoch 3:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 321/573 [1:16:47<1:00:17, 14.35s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 321/573 [1:16:47<1:00:17, 14.35s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1392])
Epoch 3:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 322/573 [1:16:56<59:58, 14.34s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]  Epoch 3:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 322/573 [1:16:56<59:58, 14.34s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1462])
Epoch 3:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 323/573 [1:17:28<59:58, 14.39s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 323/573 [1:17:28<59:58, 14.39s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13330 examples [8:29:55,  2.26s/ examples]embeddings shape: torch.Size([32, 768, 1582])
Epoch 3:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 324/573 [1:17:56<59:53, 14.43s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 324/573 [1:17:56<59:53, 14.43s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 10877 examples [8:29:29,  2.85s/ examples]Generating train split: 13396 examples [8:30:55,  2.17s/ examples]embeddings shape: torch.Size([32, 768, 1601])
Epoch 3:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 325/573 [1:18:21<59:47, 14.47s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 325/573 [1:18:21<59:47, 14.47s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1569])
Epoch 3:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 326/573 [1:18:30<59:28, 14.45s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 326/573 [1:18:30<59:28, 14.45s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1474])
Epoch 3:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 327/573 [1:18:37<59:08, 14.43s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 327/573 [1:18:37<59:08, 14.43s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] Generating train split: 13462 examples [8:31:26,  1.66s/ examples]Generating train split: 13528 examples [8:31:29,  1.18s/ examples]embeddings shape: torch.Size([32, 768, 1603])
Epoch 3:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 328/573 [1:18:52<58:55, 14.43s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 328/573 [1:18:52<58:55, 14.43s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13594 examples [8:31:32,  1.19 examples/s]Generating train split: 13660 examples [8:31:35,  1.66 examples/s]Generating train split: 13726 examples [8:31:38,  2.30 examples/s]embeddings shape: torch.Size([32, 768, 1631])
Epoch 3:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 329/573 [1:19:02<58:37, 14.42s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 329/573 [1:19:02<58:37, 14.42s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13792 examples [8:31:40,  3.22 examples/s]Generating train split: 13858 examples [8:31:40,  4.52 examples/s]Generating train split: 13924 examples [8:31:41,  6.33 examples/s]Generating train split: 13990 examples [8:31:42,  8.75 examples/s]Generating train split: 14056 examples [8:31:43, 12.00 examples/s]Generating train split: 14122 examples [8:31:43, 16.21 examples/s]Generating train split: 14188 examples [8:31:44, 21.32 examples/s]Generating train split: 14254 examples [8:31:45, 27.48 examples/s]Generating train split: 14320 examples [8:31:46, 34.61 examples/s]Generating train split: 14386 examples [8:31:47, 41.15 examples/s]Generating train split: 14452 examples [8:31:48, 48.06 examples/s]embeddings shape: torch.Size([32, 768, 1635])
Epoch 3:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 330/573 [1:19:11<58:19, 14.40s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 330/573 [1:19:11<58:19, 14.40s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 14518 examples [8:31:48, 54.12 examples/s]Generating train split: 14584 examples [8:31:49, 60.41 examples/s]Generating train split: 14650 examples [8:31:54, 30.37 examples/s]Generating train split: 14650 examples [8:31:54,  2.10s/ examples]embeddings shape: torch.Size([32, 768, 1591])
Epoch 3:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 331/573 [1:19:21<58:01, 14.39s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 331/573 [1:19:21<58:01, 14.39s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13390 examples [8:32:14,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1405])
Epoch 3:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 332/573 [1:19:53<57:59, 14.44s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 332/573 [1:19:53<57:59, 14.44s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 10926 examples [8:31:47,  2.84s/ examples]embeddings shape: torch.Size([32, 768, 1593])
Epoch 3:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 333/573 [1:20:25<57:57, 14.49s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 333/573 [1:20:25<57:57, 14.49s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]
Generating validation split: 0 examples [00:00, ? examples/s]embeddings shape: torch.Size([32, 768, 1619])
Epoch 3:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 334/573 [1:20:34<57:39, 14.47s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 334/573 [1:20:34<57:39, 14.47s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1613])
Epoch 3:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 335/573 [1:20:44<57:21, 14.46s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 335/573 [1:20:44<57:21, 14.46s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1603])
Epoch 3:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 336/573 [1:20:52<57:02, 14.44s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 336/573 [1:20:52<57:02, 14.44s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1608])
Epoch 3:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 337/573 [1:21:00<56:43, 14.42s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 337/573 [1:21:00<56:43, 14.42s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1632])
Epoch 3:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 338/573 [1:21:08<56:25, 14.40s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 338/573 [1:21:08<56:25, 14.40s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1671])
Epoch 3:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 339/573 [1:21:15<56:05, 14.38s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 339/573 [1:21:15<56:05, 14.38s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13450 examples [8:34:08,  2.16s/ examples]embeddings shape: torch.Size([32, 768, 1527])
Epoch 3:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 340/573 [1:21:45<56:01, 14.43s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 340/573 [1:21:45<56:01, 14.43s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1598])
Epoch 3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 341/573 [1:22:05<55:51, 14.45s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 341/573 [1:22:05<55:51, 14.45s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.500, train binary_accuracy_step=0.917, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 10975 examples [8:33:41,  2.69s/ examples]embeddings shape: torch.Size([32, 768, 1518])
Epoch 3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 342/573 [1:22:25<55:40, 14.46s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.500, train binary_accuracy_step=0.917, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 342/573 [1:22:25<55:40, 14.46s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1588])
Epoch 3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 343/573 [1:22:33<55:21, 14.44s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 343/573 [1:22:33<55:21, 14.44s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1658])
Epoch 3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 344/573 [1:22:59<55:15, 14.48s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 344/573 [1:22:59<55:15, 14.48s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] Generating validation split: 67 examples [02:44,  2.45s/ examples]embeddings shape: torch.Size([32, 768, 1256])
Epoch 3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 345/573 [1:23:16<55:02, 14.48s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 345/573 [1:23:16<55:02, 14.48s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1433])
Epoch 3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 346/573 [1:23:24<54:43, 14.46s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 346/573 [1:23:24<54:43, 14.46s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1007])
Epoch 3:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 347/573 [1:23:29<54:22, 14.44s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 347/573 [1:23:29<54:22, 14.44s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1614])
Epoch 3:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 348/573 [1:23:37<54:04, 14.42s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 348/573 [1:23:37<54:04, 14.42s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13510 examples [8:36:30,  2.23s/ examples]embeddings shape: torch.Size([32, 768, 1559])
Epoch 3:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 349/573 [1:24:14<54:04, 14.48s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 349/573 [1:24:14<54:04, 14.48s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11024 examples [8:36:01,  2.74s/ examples]embeddings shape: torch.Size([32, 768, 1383])
Epoch 3:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 350/573 [1:24:40<53:56, 14.51s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 350/573 [1:24:40<53:56, 14.51s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.917, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1644])
Epoch 3:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 351/573 [1:24:48<53:38, 14.50s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.917, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 351/573 [1:24:48<53:38, 14.50s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1587])
Epoch 3:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 352/573 [1:24:58<53:21, 14.48s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 352/573 [1:24:58<53:21, 14.48s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1607])
Epoch 3:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 353/573 [1:25:07<53:03, 14.47s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 353/573 [1:25:07<53:03, 14.47s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1561])
Epoch 3:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 354/573 [1:25:25<52:50, 14.48s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 354/573 [1:25:25<52:50, 14.48s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 134 examples [05:21,  2.39s/ examples]embeddings shape: torch.Size([32, 768, 1605])
Epoch 3:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 355/573 [1:25:50<52:43, 14.51s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 355/573 [1:25:50<52:43, 14.51s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1640])
Epoch 3:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 356/573 [1:25:57<52:23, 14.49s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 356/573 [1:25:57<52:23, 14.49s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1619])
Epoch 3:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 357/573 [1:26:21<52:14, 14.51s/it, loss=0.312, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 357/573 [1:26:21<52:14, 14.51s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13570 examples [8:38:51,  2.26s/ examples]embeddings shape: torch.Size([32, 768, 1585])
Epoch 3:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 358/573 [1:26:40<52:03, 14.53s/it, loss=0.311, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.903, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 358/573 [1:26:40<52:03, 14.53s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11073 examples [8:38:22,  2.78s/ examples]embeddings shape: torch.Size([32, 768, 1613])
Epoch 3:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 359/573 [1:27:04<51:54, 14.55s/it, loss=0.309, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 359/573 [1:27:04<51:54, 14.55s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.816, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1588])
Epoch 3:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 360/573 [1:27:12<51:36, 14.54s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.816, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 360/573 [1:27:12<51:36, 14.54s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1536])
Epoch 3:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 361/573 [1:27:19<51:17, 14.51s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 361/573 [1:27:19<51:17, 14.51s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1548])
Epoch 3:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 362/573 [1:27:26<50:57, 14.49s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 362/573 [1:27:26<50:57, 14.49s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.906, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1569])
Epoch 3:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 363/573 [1:27:34<50:39, 14.47s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.906, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 363/573 [1:27:34<50:39, 14.47s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1279])
Epoch 3:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 364/573 [1:27:53<50:28, 14.49s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.0938, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 364/573 [1:27:53<50:28, 14.49s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] Generating validation split: 201 examples [07:38,  2.23s/ examples]embeddings shape: torch.Size([32, 768, 1589])
Epoch 3:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 365/573 [1:28:12<50:15, 14.50s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 365/573 [1:28:12<50:15, 14.50s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1596])
Epoch 3:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 366/573 [1:28:37<50:07, 14.53s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 366/573 [1:28:37<50:07, 14.53s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13630 examples [8:41:02,  2.24s/ examples]embeddings shape: torch.Size([32, 768, 1657])
Epoch 3:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 367/573 [1:29:06<50:01, 14.57s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 367/573 [1:29:06<50:01, 14.57s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11122 examples [8:40:32,  2.74s/ examples]embeddings shape: torch.Size([32, 768, 1653])
Epoch 3:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 368/573 [1:29:17<49:44, 14.56s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 368/573 [1:29:17<49:44, 14.56s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1570])
Epoch 3:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 369/573 [1:29:25<49:26, 14.54s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 369/573 [1:29:25<49:26, 14.54s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1623])
Epoch 3:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 370/573 [1:29:32<49:07, 14.52s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 370/573 [1:29:32<49:07, 14.52s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1590])
Epoch 3:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 371/573 [1:29:38<48:48, 14.50s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 371/573 [1:29:38<48:48, 14.50s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1685])
Epoch 3:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 372/573 [1:29:48<48:31, 14.49s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 372/573 [1:29:48<48:31, 14.49s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 268 examples [09:45,  2.10s/ examples]embeddings shape: torch.Size([32, 768, 1694])
Epoch 3:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 373/573 [1:30:15<48:23, 14.52s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 373/573 [1:30:15<48:23, 14.52s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1602])
Epoch 3:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 374/573 [1:30:28<48:08, 14.51s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 374/573 [1:30:28<48:08, 14.51s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13690 examples [8:43:11,  2.21s/ examples]embeddings shape: torch.Size([32, 768, 1631])
Epoch 3:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 375/573 [1:31:00<48:02, 14.56s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 375/573 [1:31:00<48:02, 14.56s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11171 examples [8:42:42,  2.72s/ examples]embeddings shape: torch.Size([32, 768, 1400])
Epoch 3:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 376/573 [1:31:23<47:53, 14.58s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 376/573 [1:31:23<47:53, 14.58s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1568])
Epoch 3:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 377/573 [1:31:30<47:34, 14.56s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 377/573 [1:31:30<47:34, 14.56s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1594])
Epoch 3:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 378/573 [1:31:36<47:15, 14.54s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 378/573 [1:31:36<47:15, 14.54s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1573])
Epoch 3:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 379/573 [1:31:44<46:57, 14.52s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 379/573 [1:31:44<46:57, 14.52s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1671])
Epoch 3:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 380/573 [1:31:53<46:40, 14.51s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 380/573 [1:31:53<46:40, 14.51s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1573])
Epoch 3:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 381/573 [1:32:21<46:32, 14.55s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 381/573 [1:32:21<46:32, 14.55s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 335 examples [11:56,  2.04s/ examples]embeddings shape: torch.Size([32, 768, 1570])
Epoch 3:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 382/573 [1:32:32<46:16, 14.54s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 382/573 [1:32:32<46:16, 14.54s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13750 examples [8:45:22,  2.20s/ examples]embeddings shape: torch.Size([32, 768, 1609])
Epoch 3:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 383/573 [1:33:01<46:08, 14.57s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 383/573 [1:33:01<46:08, 14.57s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11220 examples [8:44:54,  2.71s/ examples]embeddings shape: torch.Size([32, 768, 1674])
Epoch 3:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 384/573 [1:33:34<46:03, 14.62s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 384/573 [1:33:34<46:03, 14.62s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1569])
Epoch 3:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 385/573 [1:33:42<45:45, 14.60s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 385/573 [1:33:42<45:45, 14.60s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1568])
Epoch 3:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 386/573 [1:33:48<45:26, 14.58s/it, loss=0.314, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 386/573 [1:33:48<45:26, 14.58s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1607])
Epoch 3:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 387/573 [1:33:57<45:09, 14.57s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 387/573 [1:33:57<45:09, 14.57s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 402 examples [13:56,  1.96s/ examples]embeddings shape: torch.Size([32, 768, 1560])
Epoch 3:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 388/573 [1:34:24<45:00, 14.60s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 388/573 [1:34:24<45:00, 14.60s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1552])
Epoch 3:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 389/573 [1:34:31<44:42, 14.58s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 389/573 [1:34:31<44:42, 14.58s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1575])
Epoch 3:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 390/573 [1:34:38<44:24, 14.56s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 390/573 [1:34:38<44:24, 14.56s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1564])
Epoch 3:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 391/573 [1:35:03<44:14, 14.59s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 391/573 [1:35:03<44:14, 14.59s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.812, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13810 examples [8:47:28,  2.17s/ examples]embeddings shape: torch.Size([32, 768, 1621])
Epoch 3:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 392/573 [1:35:29<44:05, 14.62s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.812, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 392/573 [1:35:29<44:05, 14.62s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11269 examples [8:46:55,  2.64s/ examples]embeddings shape: torch.Size([32, 768, 1609])
Epoch 3:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 393/573 [1:35:40<43:49, 14.61s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 393/573 [1:35:40<43:49, 14.61s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1514])
Epoch 3:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 394/573 [1:35:47<43:31, 14.59s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 394/573 [1:35:47<43:31, 14.59s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1617])
Epoch 3:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 395/573 [1:35:56<43:14, 14.57s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 395/573 [1:35:56<43:14, 14.57s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 469 examples [15:52,  1.89s/ examples]embeddings shape: torch.Size([32, 768, 1403])
Epoch 3:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 396/573 [1:36:21<43:04, 14.60s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 396/573 [1:36:21<43:04, 14.60s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1612])
Epoch 3:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 397/573 [1:36:30<42:46, 14.59s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 397/573 [1:36:30<42:46, 14.59s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1580])
Epoch 3:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 398/573 [1:36:38<42:29, 14.57s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.823, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 398/573 [1:36:38<42:29, 14.57s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1620])
Epoch 3:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 399/573 [1:36:53<42:15, 14.57s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 399/573 [1:36:53<42:15, 14.57s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13870 examples [8:49:33,  2.14s/ examples]embeddings shape: torch.Size([32, 768, 1635])
Epoch 3:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 400/573 [1:37:19<42:05, 14.60s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 400/573 [1:37:19<42:05, 14.60s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11318 examples [8:48:57,  2.59s/ examples]embeddings shape: torch.Size([32, 768, 1660])
Epoch 3:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 401/573 [1:37:39<41:53, 14.61s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 401/573 [1:37:39<41:53, 14.61s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1560])
Epoch 3:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 402/573 [1:37:48<41:36, 14.60s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 402/573 [1:37:48<41:36, 14.60s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 536 examples [17:39,  1.80s/ examples]embeddings shape: torch.Size([32, 768, 1581])
Epoch 3:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 403/573 [1:38:11<41:25, 14.62s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 403/573 [1:38:11<41:25, 14.62s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1531])
Epoch 3:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 404/573 [1:38:18<41:07, 14.60s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 404/573 [1:38:18<41:07, 14.60s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1579])
Epoch 3:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 405/573 [1:38:26<40:50, 14.58s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 405/573 [1:38:26<40:50, 14.58s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1687])
Epoch 3:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 406/573 [1:38:34<40:32, 14.57s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 406/573 [1:38:34<40:32, 14.57s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1546])
Epoch 3:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 407/573 [1:38:48<40:18, 14.57s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 407/573 [1:38:48<40:18, 14.57s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13930 examples [8:51:33,  2.11s/ examples]embeddings shape: torch.Size([32, 768, 1626])
Epoch 3:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 408/573 [1:39:29<40:14, 14.63s/it, loss=0.329, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 408/573 [1:39:29<40:14, 14.63s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] Generating train split: 11367 examples [8:51:00,  2.57s/ examples]embeddings shape: torch.Size([32, 768, 1524])
Epoch 3:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 409/573 [1:39:54<40:03, 14.66s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 409/573 [1:39:54<40:03, 14.66s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 603 examples [19:38,  1.79s/ examples]embeddings shape: torch.Size([32, 768, 1583])
Epoch 3:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 410/573 [1:40:11<39:50, 14.66s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 410/573 [1:40:11<39:50, 14.66s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1550])
Epoch 3:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 411/573 [1:40:20<39:33, 14.65s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 411/573 [1:40:20<39:33, 14.65s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1624])
Epoch 3:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 412/573 [1:40:26<39:15, 14.63s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 412/573 [1:40:26<39:15, 14.63s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1553])
Epoch 3:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 413/573 [1:40:33<38:57, 14.61s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 413/573 [1:40:33<38:57, 14.61s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1625])
Epoch 3:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 414/573 [1:40:41<38:40, 14.59s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 414/573 [1:40:41<38:40, 14.59s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1557])
Epoch 3:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 415/573 [1:40:48<38:22, 14.57s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 415/573 [1:40:48<38:22, 14.57s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1561])
Epoch 3:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 416/573 [1:41:13<38:11, 14.60s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 416/573 [1:41:13<38:11, 14.60s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 13990 examples [8:53:46,  2.13s/ examples]Generating train split: 11416 examples [8:53:09,  2.58s/ examples]embeddings shape: torch.Size([32, 768, 1637])
Epoch 3:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 417/573 [1:41:47<38:04, 14.65s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 417/573 [1:41:47<38:04, 14.65s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1514])
Epoch 3:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 418/573 [1:41:57<37:48, 14.64s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 418/573 [1:41:57<37:48, 14.64s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1647])
Epoch 3:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 419/573 [1:42:05<37:31, 14.62s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 419/573 [1:42:05<37:31, 14.62s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1162])
Epoch 3:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 420/573 [1:42:12<37:14, 14.60s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 420/573 [1:42:12<37:14, 14.60s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1616])
Epoch 3:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 421/573 [1:42:19<36:56, 14.58s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 421/573 [1:42:19<36:56, 14.58s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1336])
Epoch 3:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 422/573 [1:42:27<36:39, 14.57s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 422/573 [1:42:27<36:39, 14.57s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1592])
Epoch 3:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 423/573 [1:42:49<36:27, 14.59s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 423/573 [1:42:49<36:27, 14.59s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1608])
Epoch 3:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 424/573 [1:43:22<36:19, 14.63s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 424/573 [1:43:22<36:19, 14.63s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 670 examples [22:59,  2.16s/ examples]embeddings shape: torch.Size([32, 768, 1588])
Epoch 3:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 425/573 [1:43:54<36:10, 14.67s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 425/573 [1:43:54<36:10, 14.67s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 14050 examples [8:56:31,  2.32s/ examples]Generating train split: 11465 examples [8:55:34,  2.70s/ examples]embeddings shape: torch.Size([32, 768, 1613])
Epoch 3:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 426/573 [1:44:17<35:59, 14.69s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 426/573 [1:44:17<35:59, 14.69s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1599])
Epoch 3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 427/573 [1:44:24<35:41, 14.67s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 427/573 [1:44:24<35:41, 14.67s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1454])
Epoch 3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 428/573 [1:44:31<35:24, 14.65s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 428/573 [1:44:31<35:24, 14.65s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1548])
Epoch 3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 429/573 [1:44:40<35:08, 14.64s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 429/573 [1:44:40<35:08, 14.64s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1590])
Epoch 3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 430/573 [1:44:49<34:51, 14.63s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 430/573 [1:44:49<34:51, 14.63s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1596])
Epoch 3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 431/573 [1:44:56<34:34, 14.61s/it, loss=0.33, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 431/573 [1:44:56<34:34, 14.61s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1575])
Epoch 3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 432/573 [1:45:05<34:18, 14.60s/it, loss=0.328, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 432/573 [1:45:05<34:18, 14.60s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1628])
Epoch 3:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 433/573 [1:45:20<34:03, 14.60s/it, loss=0.327, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 433/573 [1:45:20<34:03, 14.60s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1531])
Epoch 3:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 434/573 [1:45:58<33:56, 14.65s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 434/573 [1:45:58<33:56, 14.65s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 14110 examples [8:58:24,  2.19s/ examples]Generating train split: 11514 examples [8:57:29,  2.59s/ examples]embeddings shape: torch.Size([32, 768, 1555])
Epoch 3:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 435/573 [1:46:13<33:41, 14.65s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 435/573 [1:46:13<33:41, 14.65s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1466])
Epoch 3:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 436/573 [1:46:21<33:25, 14.64s/it, loss=0.325, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 436/573 [1:46:21<33:25, 14.64s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1606])
Epoch 3:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 437/573 [1:46:45<33:13, 14.66s/it, loss=0.326, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 437/573 [1:46:45<33:13, 14.66s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 737 examples [26:43,  2.53s/ examples]embeddings shape: torch.Size([32, 768, 1593])
Epoch 3:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 438/573 [1:47:13<33:02, 14.69s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 438/573 [1:47:13<33:02, 14.69s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1545])
Epoch 3:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 439/573 [1:47:21<32:46, 14.67s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 439/573 [1:47:21<32:46, 14.67s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1576])
Epoch 3:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 440/573 [1:47:30<32:29, 14.66s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 440/573 [1:47:30<32:29, 14.66s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1426])
Epoch 3:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 441/573 [1:47:36<32:12, 14.64s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 441/573 [1:47:36<32:12, 14.64s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1601])
Epoch 3:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 442/573 [1:47:57<31:59, 14.65s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.896, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 442/573 [1:47:57<31:59, 14.65s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 14170 examples [9:00:55,  2.29s/ examples]embeddings shape: torch.Size([32, 768, 1608])
Epoch 3:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 443/573 [1:48:34<31:51, 14.70s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 443/573 [1:48:34<31:51, 14.70s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11563 examples [9:00:00,  2.74s/ examples]embeddings shape: torch.Size([32, 768, 1574])
Epoch 3:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 444/573 [1:48:46<31:36, 14.70s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 444/573 [1:48:46<31:36, 14.70s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1578])
Epoch 3:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 445/573 [1:48:53<31:19, 14.68s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 445/573 [1:48:53<31:19, 14.68s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1501])
Epoch 3:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 446/573 [1:49:01<31:02, 14.67s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 446/573 [1:49:01<31:02, 14.67s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1585])
Epoch 3:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 447/573 [1:49:09<30:46, 14.65s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 447/573 [1:49:09<30:46, 14.65s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1578])
Epoch 3:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 448/573 [1:49:19<30:30, 14.64s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 448/573 [1:49:19<30:30, 14.64s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1552])
Epoch 3:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 449/573 [1:49:26<30:13, 14.62s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 449/573 [1:49:26<30:13, 14.62s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1586])
Epoch 3:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 450/573 [1:49:41<29:58, 14.63s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 450/573 [1:49:41<29:58, 14.63s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1687])
Epoch 3:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 451/573 [1:50:14<29:49, 14.67s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 451/573 [1:50:14<29:49, 14.67s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 14230 examples [9:02:53,  2.19s/ examples]Generating train split: 11612 examples [9:02:13,  2.74s/ examples]embeddings shape: torch.Size([32, 768, 1584])
Epoch 3:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 452/573 [1:50:54<29:41, 14.72s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 452/573 [1:50:54<29:41, 14.72s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 804 examples [30:45,  2.85s/ examples]embeddings shape: torch.Size([32, 768, 1614])
Epoch 3:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 453/573 [1:51:17<29:28, 14.74s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 453/573 [1:51:17<29:28, 14.74s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862] embeddings shape: torch.Size([32, 768, 1583])
Epoch 3:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 454/573 [1:51:25<29:12, 14.73s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 454/573 [1:51:25<29:12, 14.73s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1582])
Epoch 3:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 455/573 [1:51:33<28:55, 14.71s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 455/573 [1:51:33<28:55, 14.71s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1613])
Epoch 3:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 456/573 [1:51:42<28:39, 14.70s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 456/573 [1:51:42<28:39, 14.70s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1653])
Epoch 3:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 457/573 [1:51:51<28:23, 14.69s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 457/573 [1:51:51<28:23, 14.69s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([26, 768, 1582])
Epoch 3:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 458/573 [1:51:56<28:06, 14.66s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 458/573 [1:51:56<28:06, 14.66s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]
Validation: 0it [00:00, ?it/s][A
Validation:   0%|          | 0/115 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/115 [00:00<?, ?it/s][Aembeddings shape: torch.Size([32, 768, 1569])

Validation DataLoader 0:   1%|          | 1/115 [00:00<00:00, 155.64it/s][AEpoch 3:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 459/573 [1:52:21<27:54, 14.69s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 14290 examples [9:04:54,  2.14s/ examples]Generating train split: 11661 examples [9:04:22,  2.70s/ examples]embeddings shape: torch.Size([32, 768, 1635])

Validation DataLoader 0:   2%|‚ñè         | 2/115 [00:40<38:18, 20.34s/it] [AEpoch 3:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 460/573 [1:53:02<27:46, 14.74s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1614])

Validation DataLoader 0:   3%|‚ñé         | 3/115 [00:49<30:30, 16.35s/it][AEpoch 3:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 461/573 [1:53:10<27:29, 14.73s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1551])

Validation DataLoader 0:   3%|‚ñé         | 4/115 [00:56<26:04, 14.09s/it][AEpoch 3:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 462/573 [1:53:18<27:13, 14.71s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1601])

Validation DataLoader 0:   4%|‚ñç         | 5/115 [01:02<23:03, 12.58s/it][AEpoch 3:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 463/573 [1:53:24<26:56, 14.70s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1631])

Validation DataLoader 0:   5%|‚ñå         | 6/115 [01:09<20:57, 11.54s/it][AEpoch 3:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 464/573 [1:53:30<26:39, 14.68s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1677])

Validation DataLoader 0:   6%|‚ñå         | 7/115 [01:20<20:36, 11.45s/it][AEpoch 3:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 465/573 [1:53:41<26:24, 14.67s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1594])

Validation DataLoader 0:   7%|‚ñã         | 8/115 [01:44<23:23, 13.12s/it][AEpoch 3:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 466/573 [1:54:06<26:12, 14.69s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 871 examples [33:54,  2.85s/ examples]embeddings shape: torch.Size([32, 768, 1383])

Validation DataLoader 0:   8%|‚ñä         | 9/115 [02:11<25:48, 14.61s/it][AEpoch 3:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 467/573 [1:54:33<26:00, 14.72s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 14350 examples [9:07:15,  2.20s/ examples]embeddings shape: torch.Size([32, 768, 1634])

Validation DataLoader 0:   9%|‚ñä         | 10/115 [02:35<27:14, 15.57s/it][AEpoch 3:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 468/573 [1:54:57<25:47, 14.74s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11710 examples [9:06:46,  2.77s/ examples]embeddings shape: torch.Size([32, 768, 1573])

Validation DataLoader 0:  10%|‚ñâ         | 11/115 [03:03<28:50, 16.64s/it][AEpoch 3:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 469/573 [1:55:24<25:35, 14.77s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1607])

Validation DataLoader 0:  10%|‚ñà         | 12/115 [03:09<27:04, 15.77s/it][AEpoch 3:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 470/573 [1:55:31<25:18, 14.75s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1446])

Validation DataLoader 0:  11%|‚ñà‚ñè        | 13/115 [03:14<25:27, 14.97s/it][AEpoch 3:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 471/573 [1:55:36<25:02, 14.73s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1323])

Validation DataLoader 0:  12%|‚ñà‚ñè        | 14/115 [03:20<24:06, 14.32s/it][AEpoch 3:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 472/573 [1:55:42<24:45, 14.71s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1219])

Validation DataLoader 0:  13%|‚ñà‚ñé        | 15/115 [03:24<22:44, 13.65s/it][AEpoch 3:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 473/573 [1:55:46<24:28, 14.69s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1092])

Validation DataLoader 0:  14%|‚ñà‚ñç        | 16/115 [03:30<21:40, 13.13s/it][AEpoch 3:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 474/573 [1:55:51<24:11, 14.67s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1197])

Validation DataLoader 0:  15%|‚ñà‚ñç        | 17/115 [03:35<20:41, 12.67s/it][AEpoch 3:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 475/573 [1:55:57<23:55, 14.65s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1569])

Validation DataLoader 0:  16%|‚ñà‚ñå        | 18/115 [03:41<19:53, 12.30s/it][AEpoch 3:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 476/573 [1:56:03<23:38, 14.63s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1234])

Validation DataLoader 0:  17%|‚ñà‚ñã        | 19/115 [03:47<19:09, 11.98s/it][AEpoch 3:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 477/573 [1:56:09<23:22, 14.61s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 14410 examples [9:09:04,  2.09s/ examples]embeddings shape: torch.Size([32, 768, 1597])

Validation DataLoader 0:  17%|‚ñà‚ñã        | 20/115 [04:28<21:15, 13.43s/it][AEpoch 3:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 478/573 [1:56:50<23:13, 14.67s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11759 examples [9:08:33,  2.60s/ examples]embeddings shape: torch.Size([32, 768, 1641])

Validation DataLoader 0:  18%|‚ñà‚ñä        | 21/115 [05:05<22:47, 14.54s/it][AEpoch 3:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 479/573 [1:57:27<23:02, 14.71s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 938 examples [37:32,  2.97s/ examples]embeddings shape: torch.Size([32, 768, 1672])

Validation DataLoader 0:  19%|‚ñà‚ñâ        | 22/115 [05:42<24:07, 15.57s/it][AEpoch 3:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 480/573 [1:58:04<22:52, 14.76s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1613])

Validation DataLoader 0:  20%|‚ñà‚ñà        | 23/115 [05:55<23:43, 15.47s/it][AEpoch 3:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 481/573 [1:58:17<22:37, 14.76s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1631])

Validation DataLoader 0:  21%|‚ñà‚ñà        | 24/115 [06:06<23:11, 15.29s/it][AEpoch 3:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 482/573 [1:58:28<22:22, 14.75s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1631])

Validation DataLoader 0:  22%|‚ñà‚ñà‚ñè       | 25/115 [06:31<23:30, 15.67s/it][AEpoch 3:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 483/573 [1:58:53<22:09, 14.77s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 14470 examples [9:11:30,  2.19s/ examples]Generating train split: 11808 examples [9:11:02,  2.73s/ examples]embeddings shape: torch.Size([32, 768, 1614])

Validation DataLoader 0:  23%|‚ñà‚ñà‚ñé       | 26/115 [07:21<25:09, 16.96s/it][AEpoch 3:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 484/573 [1:59:42<22:00, 14.84s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1592])

Validation DataLoader 0:  23%|‚ñà‚ñà‚ñé       | 27/115 [07:30<24:27, 16.67s/it][AEpoch 3:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 485/573 [1:59:51<21:44, 14.83s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1681])

Validation DataLoader 0:  24%|‚ñà‚ñà‚ñç       | 28/115 [07:56<24:40, 17.02s/it][AEpoch 3:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 486/573 [2:00:18<21:32, 14.85s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 1005 examples [40:01,  2.75s/ examples]embeddings shape: torch.Size([32, 768, 1618])

Validation DataLoader 0:  25%|‚ñà‚ñà‚ñå       | 29/115 [08:13<24:23, 17.02s/it][AEpoch 3:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 487/573 [2:00:35<21:17, 14.86s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1618])

Validation DataLoader 0:  26%|‚ñà‚ñà‚ñå       | 30/115 [08:21<23:39, 16.70s/it][AEpoch 3:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 488/573 [2:00:42<21:01, 14.84s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1540])

Validation DataLoader 0:  27%|‚ñà‚ñà‚ñã       | 31/115 [08:29<23:01, 16.45s/it][AEpoch 3:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 489/573 [2:00:51<20:45, 14.83s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 14530 examples [9:13:39,  2.18s/ examples]embeddings shape: torch.Size([32, 768, 1574])

Validation DataLoader 0:  28%|‚ñà‚ñà‚ñä       | 32/115 [08:57<23:13, 16.79s/it][AEpoch 3:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 490/573 [2:01:18<20:32, 14.85s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11857 examples [9:13:10,  2.69s/ examples]embeddings shape: torch.Size([32, 768, 1577])

Validation DataLoader 0:  29%|‚ñà‚ñà‚ñä       | 33/115 [09:28<23:33, 17.24s/it][AEpoch 3:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 491/573 [2:01:50<20:20, 14.89s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1574])

Validation DataLoader 0:  30%|‚ñà‚ñà‚ñâ       | 34/115 [09:34<22:47, 16.88s/it][AEpoch 3:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 492/573 [2:01:55<20:04, 14.87s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1677])

Validation DataLoader 0:  30%|‚ñà‚ñà‚ñà       | 35/115 [09:39<22:04, 16.56s/it][AEpoch 3:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 493/573 [2:02:01<19:48, 14.85s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1615])

Validation DataLoader 0:  31%|‚ñà‚ñà‚ñà‚ñè      | 36/115 [09:46<21:25, 16.28s/it][AEpoch 3:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 494/573 [2:02:07<19:31, 14.83s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1582])

Validation DataLoader 0:  32%|‚ñà‚ñà‚ñà‚ñè      | 37/115 [10:18<21:44, 16.72s/it][AEpoch 3:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 495/573 [2:02:40<19:19, 14.87s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 1072 examples [42:16,  2.52s/ examples]embeddings shape: torch.Size([32, 768, 1602])

Validation DataLoader 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 38/115 [10:29<21:15, 16.57s/it][AEpoch 3:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 496/573 [2:02:51<19:04, 14.86s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1545])

Validation DataLoader 0:  34%|‚ñà‚ñà‚ñà‚ñç      | 39/115 [10:50<21:06, 16.67s/it][AEpoch 3:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 497/573 [2:03:11<18:50, 14.87s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 14590 examples [9:15:47,  2.16s/ examples]embeddings shape: torch.Size([32, 768, 1615])

Validation DataLoader 0:  35%|‚ñà‚ñà‚ñà‚ñç      | 40/115 [11:09<20:55, 16.74s/it][AEpoch 3:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 498/573 [2:03:31<18:36, 14.88s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11906 examples [9:15:17,  2.67s/ examples]embeddings shape: torch.Size([32, 768, 1602])

Validation DataLoader 0:  36%|‚ñà‚ñà‚ñà‚ñå      | 41/115 [11:35<20:55, 16.96s/it][AEpoch 3:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 499/573 [2:03:57<18:22, 14.90s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1656])

Validation DataLoader 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 42/115 [11:42<20:21, 16.74s/it][AEpoch 3:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 500/573 [2:04:04<18:06, 14.89s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1654])

Validation DataLoader 0:  37%|‚ñà‚ñà‚ñà‚ñã      | 43/115 [11:51<19:51, 16.54s/it][AEpoch 3:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 501/573 [2:04:13<17:51, 14.88s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1570])

Validation DataLoader 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 44/115 [11:57<19:18, 16.31s/it][AEpoch 3:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 502/573 [2:04:19<17:35, 14.86s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1627])

Validation DataLoader 0:  39%|‚ñà‚ñà‚ñà‚ñâ      | 45/115 [12:16<19:05, 16.37s/it][AEpoch 3:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 503/573 [2:04:38<17:20, 14.87s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 1139 examples [44:24,  2.34s/ examples]embeddings shape: torch.Size([32, 768, 1646])

Validation DataLoader 0:  40%|‚ñà‚ñà‚ñà‚ñà      | 46/115 [12:34<18:52, 16.41s/it][AEpoch 3:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 504/573 [2:04:56<17:06, 14.87s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1492])

Validation DataLoader 0:  41%|‚ñà‚ñà‚ñà‚ñà      | 47/115 [12:45<18:27, 16.29s/it][AEpoch 3:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 505/573 [2:05:07<16:50, 14.87s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 14650 examples [9:17:51,  2.13s/ examples]Generating train split: 14650 examples [9:17:51,  2.28s/ examples]embeddings shape: torch.Size([32, 768, 1575])

Validation DataLoader 0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 48/115 [13:10<18:23, 16.48s/it][AEpoch 3:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 506/573 [2:05:32<16:37, 14.89s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1594])

Validation DataLoader 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 49/115 [13:33<18:15, 16.59s/it][AEpoch 3:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 507/573 [2:05:54<16:23, 14.90s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 11955 examples [9:17:27,  2.66s/ examples]embeddings shape: torch.Size([32, 768, 1549])

Validation DataLoader 0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 50/115 [13:49<17:58, 16.60s/it][AEpoch 3:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 508/573 [2:06:11<16:08, 14.90s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1532])

Validation DataLoader 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51/115 [13:57<17:30, 16.42s/it][AEpoch 3:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 509/573 [2:06:19<15:52, 14.89s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]
Generating validation split: 0 examples [00:00, ? examples/s]embeddings shape: torch.Size([32, 768, 1573])

Validation DataLoader 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 52/115 [14:04<17:03, 16.25s/it][AEpoch 3:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 510/573 [2:06:26<15:37, 14.88s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1604])

Validation DataLoader 0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 53/115 [14:10<16:34, 16.04s/it][AEpoch 3:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 511/573 [2:06:32<15:21, 14.86s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1557])

Validation DataLoader 0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 54/115 [14:16<16:07, 15.86s/it][AEpoch 3:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 512/573 [2:06:38<15:05, 14.84s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1535])

Validation DataLoader 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 55/115 [14:22<15:40, 15.67s/it][AEpoch 3:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 513/573 [2:06:43<14:49, 14.82s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 802])

Validation DataLoader 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 56/115 [14:25<15:11, 15.46s/it][AEpoch 3:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 514/573 [2:06:47<14:33, 14.80s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1446])

Validation DataLoader 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 57/115 [14:31<14:46, 15.28s/it][AEpoch 3:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 515/573 [2:06:52<14:17, 14.78s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1251])

Validation DataLoader 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 58/115 [14:35<14:20, 15.10s/it][AEpoch 3:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 516/573 [2:06:57<14:01, 14.76s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1607])

Validation DataLoader 0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 59/115 [14:40<13:55, 14.92s/it][AEpoch 3:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 517/573 [2:07:02<13:45, 14.74s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1191])

Validation DataLoader 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 60/115 [15:05<13:50, 15.10s/it][AEpoch 3:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 518/573 [2:07:27<13:32, 14.76s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1343])

Validation DataLoader 0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 61/115 [15:29<13:42, 15.24s/it][AEpoch 3:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 519/573 [2:07:51<13:18, 14.78s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 985])

Validation DataLoader 0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 62/115 [15:48<13:31, 15.31s/it][AEpoch 3:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 520/573 [2:08:10<13:03, 14.79s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12004 examples [9:19:36,  2.65s/ examples]Generating validation split: 1206 examples [47:49,  2.55s/ examples]embeddings shape: torch.Size([32, 768, 1337])

Validation DataLoader 0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 63/115 [16:06<13:17, 15.34s/it][AEpoch 3:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 521/573 [2:08:27<12:49, 14.79s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 61 examples [02:18,  2.27s/ examples]embeddings shape: torch.Size([32, 768, 1107])

Validation DataLoader 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64/115 [16:20<13:01, 15.32s/it][AEpoch 3:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 522/573 [2:08:41<12:34, 14.79s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1142])

Validation DataLoader 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 65/115 [16:23<12:36, 15.13s/it][AEpoch 3:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 523/573 [2:08:45<12:18, 14.77s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1570])

Validation DataLoader 0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 66/115 [16:29<12:14, 14.99s/it][AEpoch 3:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 524/573 [2:08:51<12:02, 14.75s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1587])

Validation DataLoader 0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 67/115 [16:37<11:54, 14.89s/it][AEpoch 3:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 525/573 [2:08:59<11:47, 14.74s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1609])

Validation DataLoader 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 68/115 [16:45<11:34, 14.78s/it][AEpoch 3:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 526/573 [2:09:06<11:32, 14.73s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1595])

Validation DataLoader 0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 69/115 [16:51<11:14, 14.66s/it][AEpoch 3:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 527/573 [2:09:13<11:16, 14.71s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1593])

Validation DataLoader 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 70/115 [17:01<10:56, 14.59s/it][AEpoch 3:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 528/573 [2:09:23<11:01, 14.70s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1575])

Validation DataLoader 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 71/115 [17:16<10:42, 14.61s/it][AEpoch 3:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 529/573 [2:09:38<10:47, 14.70s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12053 examples [9:21:18,  2.48s/ examples]Generating validation split: 122 examples [04:01,  1.93s/ examples]embeddings shape: torch.Size([32, 768, 1684])

Validation DataLoader 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 72/115 [18:03<10:46, 15.04s/it][AEpoch 3:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 530/573 [2:10:24<10:34, 14.76s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 1273 examples [50:28,  2.50s/ examples]embeddings shape: torch.Size([32, 768, 1662])

Validation DataLoader 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 73/115 [18:38<10:43, 15.32s/it][AEpoch 3:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 531/573 [2:11:00<10:21, 14.80s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1617])

Validation DataLoader 0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 74/115 [18:47<10:24, 15.24s/it][AEpoch 3:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 532/573 [2:11:09<10:06, 14.79s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1639])

Validation DataLoader 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 75/115 [18:57<10:06, 15.17s/it][AEpoch 3:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 533/573 [2:11:19<09:51, 14.78s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1612])

Validation DataLoader 0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 76/115 [19:07<09:48, 15.10s/it][AEpoch 3:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 534/573 [2:11:29<09:36, 14.77s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1581])

Validation DataLoader 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 77/115 [19:14<09:29, 15.00s/it][AEpoch 3:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 535/573 [2:11:36<09:20, 14.76s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1159])

Validation DataLoader 0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 78/115 [19:23<09:11, 14.91s/it][AEpoch 3:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 536/573 [2:11:44<09:05, 14.75s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12102 examples [9:23:33,  2.56s/ examples]embeddings shape: torch.Size([32, 768, 1591])

Validation DataLoader 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 79/115 [19:50<09:02, 15.07s/it][AEpoch 3:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 537/573 [2:12:12<08:51, 14.77s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1455])

Validation DataLoader 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 80/115 [20:12<08:50, 15.15s/it][AEpoch 3:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 538/573 [2:12:34<08:37, 14.78s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 183 examples [06:17,  2.07s/ examples]embeddings shape: torch.Size([32, 768, 1566])

Validation DataLoader 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 81/115 [20:23<08:33, 15.11s/it][AEpoch 3:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 539/573 [2:12:45<08:22, 14.78s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1571])

Validation DataLoader 0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 82/115 [20:31<08:15, 15.01s/it][AEpoch 3:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 540/573 [2:12:52<08:07, 14.76s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1568])

Validation DataLoader 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 83/115 [20:56<08:04, 15.14s/it][AEpoch 3:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 541/573 [2:13:18<07:53, 14.78s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 1340 examples [52:56,  2.41s/ examples]embeddings shape: torch.Size([32, 768, 1600])

Validation DataLoader 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 84/115 [21:10<07:48, 15.12s/it][AEpoch 3:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 542/573 [2:13:31<07:38, 14.78s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1569])

Validation DataLoader 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 85/115 [21:18<07:31, 15.04s/it][AEpoch 3:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 543/573 [2:13:40<07:23, 14.77s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1626])

Validation DataLoader 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 86/115 [21:28<07:14, 14.98s/it][AEpoch 3:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 544/573 [2:13:50<07:08, 14.76s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1599])

Validation DataLoader 0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 87/115 [21:43<06:59, 14.99s/it][AEpoch 3:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 545/573 [2:14:05<06:53, 14.76s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12151 examples [9:25:47,  2.61s/ examples]embeddings shape: torch.Size([32, 768, 1553])

Validation DataLoader 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 88/115 [22:15<06:49, 15.17s/it][AEpoch 3:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 546/573 [2:14:37<06:39, 14.79s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 244 examples [08:32,  2.12s/ examples]embeddings shape: torch.Size([32, 768, 1636])

Validation DataLoader 0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 89/115 [22:36<06:36, 15.24s/it][AEpoch 3:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 547/573 [2:14:57<06:24, 14.80s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1600])

Validation DataLoader 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 90/115 [22:43<06:18, 15.15s/it][AEpoch 3:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 548/573 [2:15:04<06:09, 14.79s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1475])

Validation DataLoader 0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 91/115 [22:49<06:01, 15.05s/it][AEpoch 3:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 549/573 [2:15:11<05:54, 14.78s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1533])

Validation DataLoader 0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 92/115 [22:53<05:43, 14.93s/it][AEpoch 3:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 550/573 [2:15:15<05:39, 14.75s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1593])

Validation DataLoader 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 93/115 [23:01<05:26, 14.86s/it][AEpoch 3:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 551/573 [2:15:23<05:24, 14.74s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 48])

Validation DataLoader 0:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 94/115 [23:02<05:08, 14.71s/it][AEpoch 3:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 552/573 [2:15:24<05:09, 14.72s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1074])

Validation DataLoader 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 95/115 [23:15<04:53, 14.69s/it][AEpoch 3:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 553/573 [2:15:37<04:54, 14.72s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 1407 examples [55:25,  2.36s/ examples]embeddings shape: torch.Size([32, 768, 1660])

Validation DataLoader 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 96/115 [23:40<04:41, 14.79s/it][AEpoch 3:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 554/573 [2:16:01<04:39, 14.73s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1624])

Validation DataLoader 0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 97/115 [23:53<04:25, 14.78s/it][AEpoch 3:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 555/573 [2:16:15<04:25, 14.73s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating train split: 12200 examples [9:28:00,  2.65s/ examples]embeddings shape: torch.Size([32, 768, 1568])

Validation DataLoader 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 98/115 [24:18<04:12, 14.88s/it][AEpoch 3:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 556/573 [2:16:40<04:10, 14.75s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1581])

Validation DataLoader 0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 99/115 [24:39<03:59, 14.94s/it][AEpoch 3:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 557/573 [2:17:01<03:56, 14.76s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 305 examples [10:45,  2.15s/ examples]embeddings shape: torch.Size([32, 768, 1602])

Validation DataLoader 0:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 100/115 [24:53<03:44, 14.94s/it][AEpoch 3:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 558/573 [2:17:15<03:41, 14.76s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1598])

Validation DataLoader 0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 101/115 [24:59<03:27, 14.85s/it][AEpoch 3:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 559/573 [2:17:21<03:26, 14.74s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1603])

Validation DataLoader 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 102/115 [25:08<03:12, 14.79s/it][AEpoch 3:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 560/573 [2:17:29<03:11, 14.73s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1564])

Validation DataLoader 0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 103/115 [25:16<02:56, 14.72s/it][AEpoch 3:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 561/573 [2:17:38<02:56, 14.72s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 1615])

Validation DataLoader 0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 104/115 [25:30<02:41, 14.72s/it][AEpoch 3:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 562/573 [2:17:52<02:41, 14.72s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Generating validation split: 1474 examples [57:40,  2.25s/ examples]embeddings shape: torch.Size([32, 768, 1659])

Validation DataLoader 0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 105/115 [25:51<02:27, 14.78s/it][AEpoch 3:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 563/573 [2:18:13<02:27, 14.73s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 34])

Validation DataLoader 0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 106/115 [25:52<02:11, 14.64s/it][AEpoch 3:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 564/573 [2:18:13<02:12, 14.71s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 30])

Validation DataLoader 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 107/115 [25:52<01:56, 14.51s/it][AEpoch 3:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 565/573 [2:18:14<01:57, 14.68s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 25])

Validation DataLoader 0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 108/115 [25:52<01:40, 14.38s/it][AEpoch 3:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 566/573 [2:18:14<01:42, 14.65s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 109/115 [25:52<01:25, 14.25s/it][AEpoch 3:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 567/573 [2:18:14<01:27, 14.63s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 110/115 [25:52<01:10, 14.12s/it][AEpoch 3:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 568/573 [2:18:14<01:13, 14.60s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 111/115 [25:52<00:55, 13.99s/it][AEpoch 3:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 569/573 [2:18:14<00:58, 14.58s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 112/115 [25:53<00:41, 13.87s/it][AEpoch 3:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 570/573 [2:18:14<00:43, 14.55s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 7])

Validation DataLoader 0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 113/115 [25:53<00:27, 13.74s/it][AEpoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 571/573 [2:18:14<00:29, 14.53s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([32, 768, 6])

Validation DataLoader 0:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 114/115 [25:53<00:13, 13.62s/it][AEpoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 572/573 [2:18:14<00:14, 14.50s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]embeddings shape: torch.Size([22, 768, 22])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [25:53<00:00, 13.51s/it][AEpoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 573/573 [2:18:14<00:00, 14.48s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.174, val binary_accuracy_epoch=0.850, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 573/573 [2:18:14<00:00, 14.48s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.413, train categorical_accuracy_strict_epoch=0.265, train binary_accuracy_epoch=0.862]
                                                                          [AEpoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 573/573 [2:18:14<00:00, 14.48s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 3, global step 1832: 'val_loss' was not in top 1
Epoch duration: 8294.99 seconds
Epoch 3:   0%|          | 0/573 [00:00<?, ?it/s, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]            Epoch 4:   0%|          | 0/573 [00:00<?, ?it/s, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1618])
Epoch 4:   0%|          | 1/573 [00:13<2:05:31, 13.17s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.462, train categorical_accuracy_strict_step=0.308, train binary_accuracy_step=0.850, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   0%|          | 1/573 [00:13<2:05:31, 13.17s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating train split: 12249 examples [9:30:08,  2.63s/ examples]embeddings shape: torch.Size([32, 768, 1122])
Epoch 4:   0%|          | 2/573 [00:33<2:38:49, 16.69s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   0%|          | 2/573 [00:33<2:38:49, 16.69s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1541])
Epoch 4:   1%|          | 3/573 [00:46<2:28:13, 15.60s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   1%|          | 3/573 [00:46<2:28:13, 15.60s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating validation split: 366 examples [12:56,  2.15s/ examples]embeddings shape: torch.Size([32, 768, 1594])
Epoch 4:   1%|          | 4/573 [01:08<2:42:02, 17.09s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.312, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   1%|          | 4/573 [01:08<2:42:02, 17.09s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1589])
Epoch 4:   1%|          | 5/573 [01:15<2:22:23, 15.04s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   1%|          | 5/573 [01:15<2:22:23, 15.04s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1651])
Epoch 4:   1%|          | 6/573 [01:21<2:08:54, 13.64s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.469, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   1%|          | 6/573 [01:21<2:08:54, 13.64s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1644])
Epoch 4:   1%|          | 7/573 [01:28<1:58:37, 12.58s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   1%|          | 7/573 [01:28<1:58:37, 12.58s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861] embeddings shape: torch.Size([32, 768, 1550])
Epoch 4:   1%|‚ñè         | 8/573 [01:36<1:53:53, 12.09s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   1%|‚ñè         | 8/573 [01:36<1:53:53, 12.09s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1631])
Epoch 4:   2%|‚ñè         | 9/573 [01:51<1:56:23, 12.38s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   2%|‚ñè         | 9/573 [01:51<1:56:23, 12.38s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861] Generating validation split: 1541 examples [59:46,  2.14s/ examples]embeddings shape: torch.Size([32, 768, 1070])
Epoch 4:   2%|‚ñè         | 10/573 [02:03<1:55:38, 12.32s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   2%|‚ñè         | 10/573 [02:03<1:55:38, 12.32s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1455])
Epoch 4:   2%|‚ñè         | 11/573 [02:11<1:51:50, 11.94s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   2%|‚ñè         | 11/573 [02:11<1:51:50, 11.94s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1548])
Epoch 4:   2%|‚ñè         | 12/573 [02:25<1:53:40, 12.16s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   2%|‚ñè         | 12/573 [02:25<1:53:40, 12.16s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating train split: 12298 examples [9:32:09,  2.59s/ examples]embeddings shape: torch.Size([32, 768, 1605])
Epoch 4:   2%|‚ñè         | 13/573 [02:35<1:51:55, 11.99s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.594, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   2%|‚ñè         | 13/573 [02:35<1:51:55, 11.99s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.812, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1644])
Epoch 4:   2%|‚ñè         | 14/573 [02:51<1:54:19, 12.27s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.812, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   2%|‚ñè         | 14/573 [02:51<1:54:19, 12.27s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.910, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating validation split: 427 examples [14:57,  2.09s/ examples]embeddings shape: torch.Size([32, 768, 1578])
Epoch 4:   3%|‚ñé         | 15/573 [03:08<1:57:07, 12.59s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.910, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   3%|‚ñé         | 15/573 [03:08<1:57:07, 12.59s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861] embeddings shape: torch.Size([32, 768, 1614])
Epoch 4:   3%|‚ñé         | 16/573 [03:14<1:52:50, 12.16s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   3%|‚ñé         | 16/573 [03:14<1:52:51, 12.16s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1563])
Epoch 4:   3%|‚ñé         | 17/573 [03:21<1:49:44, 11.84s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   3%|‚ñé         | 17/573 [03:21<1:49:44, 11.84s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1575])
Epoch 4:   3%|‚ñé         | 18/573 [03:26<1:45:57, 11.45s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   3%|‚ñé         | 18/573 [03:26<1:45:57, 11.45s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1330])
Epoch 4:   3%|‚ñé         | 19/573 [03:35<1:44:55, 11.36s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   3%|‚ñé         | 19/573 [03:35<1:44:55, 11.36s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating validation split: 1608 examples [1:01:39,  2.01s/ examples]embeddings shape: torch.Size([32, 768, 1620])
Epoch 4:   3%|‚ñé         | 20/573 [03:56<1:48:46, 11.80s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   3%|‚ñé         | 20/573 [03:56<1:48:46, 11.80s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1659])
Epoch 4:   4%|‚ñé         | 21/573 [04:00<1:45:24, 11.46s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   4%|‚ñé         | 21/573 [04:00<1:45:24, 11.46s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1659])
Epoch 4:   4%|‚ñç         | 22/573 [04:05<1:42:39, 11.18s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   4%|‚ñç         | 22/573 [04:05<1:42:39, 11.18s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861] embeddings shape: torch.Size([32, 768, 1558])
Epoch 4:   4%|‚ñç         | 23/573 [04:17<1:42:45, 11.21s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   4%|‚ñç         | 23/573 [04:17<1:42:45, 11.21s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1427])
Epoch 4:   4%|‚ñç         | 24/573 [04:30<1:43:03, 11.26s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   4%|‚ñç         | 24/573 [04:30<1:43:03, 11.26s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating train split: 12347 examples [9:34:08,  2.54s/ examples]embeddings shape: torch.Size([32, 768, 1536])
Epoch 4:   4%|‚ñç         | 25/573 [04:36<1:40:59, 11.06s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   4%|‚ñç         | 25/573 [04:36<1:40:59, 11.06s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1318])
Epoch 4:   5%|‚ñç         | 26/573 [04:54<1:43:15, 11.33s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   5%|‚ñç         | 26/573 [04:54<1:43:15, 11.33s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating validation split: 488 examples [16:54,  2.04s/ examples]embeddings shape: torch.Size([32, 768, 1579])
Epoch 4:   5%|‚ñç         | 27/573 [05:06<1:43:10, 11.34s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   5%|‚ñç         | 27/573 [05:06<1:43:10, 11.34s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1566])
Epoch 4:   5%|‚ñç         | 28/573 [05:12<1:41:30, 11.17s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   5%|‚ñç         | 28/573 [05:12<1:41:30, 11.17s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1607])
Epoch 4:   5%|‚ñå         | 29/573 [05:17<1:39:16, 10.95s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   5%|‚ñå         | 29/573 [05:17<1:39:16, 10.95s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1634])
Epoch 4:   5%|‚ñå         | 30/573 [05:32<1:40:15, 11.08s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   5%|‚ñå         | 30/573 [05:32<1:40:15, 11.08s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating validation split: 1675 examples [1:03:29,  1.90s/ examples]embeddings shape: torch.Size([32, 768, 1653])
Epoch 4:   5%|‚ñå         | 31/573 [05:44<1:40:19, 11.11s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   5%|‚ñå         | 31/573 [05:44<1:40:19, 11.11s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.809, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1515])
Epoch 4:   6%|‚ñå         | 32/573 [05:49<1:38:23, 10.91s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.0625, train binary_accuracy_step=0.809, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   6%|‚ñå         | 32/573 [05:49<1:38:23, 10.91s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]  embeddings shape: torch.Size([32, 768, 1581])
Epoch 4:   6%|‚ñå         | 33/573 [05:54<1:36:37, 10.74s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   6%|‚ñå         | 33/573 [05:54<1:36:37, 10.74s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1604])
Epoch 4:   6%|‚ñå         | 34/573 [05:59<1:35:05, 10.59s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   6%|‚ñå         | 34/573 [05:59<1:35:05, 10.59s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1578])
Epoch 4:   6%|‚ñå         | 35/573 [06:19<1:37:06, 10.83s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   6%|‚ñå         | 35/573 [06:19<1:37:07, 10.83s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating train split: 12396 examples [9:36:04,  2.49s/ examples]embeddings shape: torch.Size([32, 768, 1642])
Epoch 4:   6%|‚ñã         | 36/573 [06:33<1:37:43, 10.92s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   6%|‚ñã         | 36/573 [06:33<1:37:43, 10.92s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1576])
Epoch 4:   6%|‚ñã         | 37/573 [06:51<1:39:19, 11.12s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   6%|‚ñã         | 37/573 [06:51<1:39:19, 11.12s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating validation split: 549 examples [18:53,  2.01s/ examples]embeddings shape: torch.Size([32, 768, 1611])
Epoch 4:   7%|‚ñã         | 38/573 [07:04<1:39:33, 11.17s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   7%|‚ñã         | 38/573 [07:04<1:39:33, 11.17s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1647])
Epoch 4:   7%|‚ñã         | 39/573 [07:10<1:38:11, 11.03s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.562, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   7%|‚ñã         | 39/573 [07:10<1:38:11, 11.03s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1555])
Epoch 4:   7%|‚ñã         | 40/573 [07:14<1:36:35, 10.87s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   7%|‚ñã         | 40/573 [07:14<1:36:35, 10.87s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1574])
Epoch 4:   7%|‚ñã         | 41/573 [07:26<1:36:36, 10.90s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   7%|‚ñã         | 41/573 [07:26<1:36:36, 10.90s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating validation split: 1742 examples [1:05:31,  1.88s/ examples]embeddings shape: torch.Size([32, 768, 1596])
Epoch 4:   7%|‚ñã         | 42/573 [07:45<1:38:07, 11.09s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   7%|‚ñã         | 42/573 [07:45<1:38:07, 11.09s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1589])
Epoch 4:   8%|‚ñä         | 43/573 [07:51<1:36:50, 10.96s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   8%|‚ñä         | 43/573 [07:51<1:36:50, 10.96s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1597])
Epoch 4:   8%|‚ñä         | 44/573 [07:56<1:35:28, 10.83s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   8%|‚ñä         | 44/573 [07:56<1:35:28, 10.83s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1544])
Epoch 4:   8%|‚ñä         | 45/573 [08:08<1:35:27, 10.85s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   8%|‚ñä         | 45/573 [08:08<1:35:27, 10.85s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating train split: 12445 examples [9:38:01,  2.46s/ examples]embeddings shape: torch.Size([32, 768, 1568])
Epoch 4:   8%|‚ñä         | 46/573 [08:26<1:36:38, 11.00s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   8%|‚ñä         | 46/573 [08:26<1:36:38, 11.00s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1609])
Epoch 4:   8%|‚ñä         | 47/573 [08:31<1:35:27, 10.89s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.847, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   8%|‚ñä         | 47/573 [08:31<1:35:27, 10.89s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861] embeddings shape: torch.Size([32, 768, 1535])
Epoch 4:   8%|‚ñä         | 48/573 [08:48<1:36:16, 11.00s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.889, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   8%|‚ñä         | 48/573 [08:48<1:36:16, 11.00s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating validation split: 610 examples [20:51,  1.98s/ examples]embeddings shape: torch.Size([32, 768, 1553])
Epoch 4:   9%|‚ñä         | 49/573 [09:10<1:38:09, 11.24s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   9%|‚ñä         | 49/573 [09:10<1:38:09, 11.24s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861] Generating validation split: 1809 examples [1:07:04,  1.73s/ examples]embeddings shape: torch.Size([32, 768, 1590])
Epoch 4:   9%|‚ñä         | 50/573 [09:22<1:37:59, 11.24s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   9%|‚ñä         | 50/573 [09:22<1:37:59, 11.24s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1584])
Epoch 4:   9%|‚ñâ         | 51/573 [09:26<1:36:42, 11.12s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   9%|‚ñâ         | 51/573 [09:26<1:36:42, 11.12s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1576])
Epoch 4:   9%|‚ñâ         | 52/573 [09:32<1:35:32, 11.00s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   9%|‚ñâ         | 52/573 [09:32<1:35:32, 11.00s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861] embeddings shape: torch.Size([32, 768, 1566])
Epoch 4:   9%|‚ñâ         | 53/573 [09:37<1:34:29, 10.90s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   9%|‚ñâ         | 53/573 [09:37<1:34:29, 10.90s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1650])
Epoch 4:   9%|‚ñâ         | 54/573 [09:44<1:33:40, 10.83s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:   9%|‚ñâ         | 54/573 [09:44<1:33:40, 10.83s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1636])
Epoch 4:  10%|‚ñâ         | 55/573 [09:57<1:33:47, 10.86s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  10%|‚ñâ         | 55/573 [09:57<1:33:47, 10.86s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.156, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861] Generating train split: 12494 examples [9:39:56,  2.42s/ examples]embeddings shape: torch.Size([32, 768, 1557])
Epoch 4:  10%|‚ñâ         | 56/573 [10:21<1:35:34, 11.09s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.156, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  10%|‚ñâ         | 56/573 [10:21<1:35:34, 11.09s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1630])
Epoch 4:  10%|‚ñâ         | 57/573 [10:38<1:36:23, 11.21s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  10%|‚ñâ         | 57/573 [10:38<1:36:23, 11.21s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating validation split: 1876 examples [1:08:32,  1.60s/ examples]embeddings shape: torch.Size([32, 768, 1561])
Epoch 4:  10%|‚ñà         | 58/573 [11:02<1:38:02, 11.42s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.188, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  10%|‚ñà         | 58/573 [11:02<1:38:02, 11.42s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating validation split: 670 examples [23:01,  2.04s/ examples]embeddings shape: torch.Size([32, 768, 1599])
Epoch 4:  10%|‚ñà         | 59/573 [11:16<1:38:16, 11.47s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.219, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  10%|‚ñà         | 59/573 [11:16<1:38:16, 11.47s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1566])
Epoch 4:  10%|‚ñà         | 60/573 [11:23<1:37:20, 11.39s/it, loss=0.323, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  10%|‚ñà         | 60/573 [11:23<1:37:20, 11.39s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861] embeddings shape: torch.Size([32, 768, 1545])
Epoch 4:  11%|‚ñà         | 61/573 [11:28<1:36:16, 11.28s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  11%|‚ñà         | 61/573 [11:28<1:36:16, 11.28s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.910, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1573])
Epoch 4:  11%|‚ñà         | 62/573 [11:34<1:35:20, 11.19s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.910, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  11%|‚ñà         | 62/573 [11:34<1:35:20, 11.19s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1613])
Epoch 4:  11%|‚ñà         | 63/573 [11:39<1:34:21, 11.10s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  11%|‚ñà         | 63/573 [11:39<1:34:21, 11.10s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1408])
Epoch 4:  11%|‚ñà         | 64/573 [11:44<1:33:20, 11.00s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  11%|‚ñà         | 64/573 [11:44<1:33:20, 11.00s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1558])
Epoch 4:  11%|‚ñà‚ñè        | 65/573 [11:48<1:32:18, 10.90s/it, loss=0.318, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  11%|‚ñà‚ñè        | 65/573 [11:48<1:32:18, 10.90s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861] embeddings shape: torch.Size([32, 768, 1617])
Epoch 4:  12%|‚ñà‚ñè        | 66/573 [12:13<1:33:53, 11.11s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.830, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  12%|‚ñà‚ñè        | 66/573 [12:13<1:33:53, 11.11s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating train split: 12543 examples [9:42:03,  2.47s/ examples]embeddings shape: torch.Size([32, 768, 1595])
Epoch 4:  12%|‚ñà‚ñè        | 67/573 [12:35<1:35:02, 11.27s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.837, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  12%|‚ñà‚ñè        | 67/573 [12:35<1:35:02, 11.27s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating validation split: 1943 examples [1:10:22,  1.61s/ examples]embeddings shape: torch.Size([32, 768, 1624])
Epoch 4:  12%|‚ñà‚ñè        | 68/573 [12:53<1:35:46, 11.38s/it, loss=0.324, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.156, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  12%|‚ñà‚ñè        | 68/573 [12:53<1:35:46, 11.38s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating validation split: 730 examples [24:56,  2.00s/ examples]embeddings shape: torch.Size([32, 768, 1570])
Epoch 4:  12%|‚ñà‚ñè        | 69/573 [13:05<1:35:38, 11.39s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  12%|‚ñà‚ñè        | 69/573 [13:05<1:35:38, 11.39s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861] embeddings shape: torch.Size([32, 768, 1664])
Epoch 4:  12%|‚ñà‚ñè        | 70/573 [13:13<1:34:58, 11.33s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.438, train binary_accuracy_step=0.899, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  12%|‚ñà‚ñè        | 70/573 [13:13<1:34:58, 11.33s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1341])
Epoch 4:  12%|‚ñà‚ñè        | 71/573 [13:17<1:33:59, 11.23s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.851, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  12%|‚ñà‚ñè        | 71/573 [13:17<1:33:59, 11.23s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1584])
Epoch 4:  13%|‚ñà‚ñé        | 72/573 [13:22<1:33:00, 11.14s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.885, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  13%|‚ñà‚ñé        | 72/573 [13:22<1:33:00, 11.14s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1630])
Epoch 4:  13%|‚ñà‚ñé        | 73/573 [13:27<1:32:13, 11.07s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  13%|‚ñà‚ñé        | 73/573 [13:27<1:32:13, 11.07s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1660])
Epoch 4:  13%|‚ñà‚ñé        | 74/573 [13:32<1:31:19, 10.98s/it, loss=0.319, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.858, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  13%|‚ñà‚ñé        | 74/573 [13:32<1:31:19, 10.98s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating validation split: 2010 examples [1:11:38,  1.47s/ examples]embeddings shape: torch.Size([32, 768, 1645])
Epoch 4:  13%|‚ñà‚ñé        | 75/573 [13:51<1:32:02, 11.09s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  13%|‚ñà‚ñé        | 75/573 [13:51<1:32:02, 11.09s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1562])
Epoch 4:  13%|‚ñà‚ñé        | 76/573 [13:58<1:31:20, 11.03s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  13%|‚ñà‚ñé        | 76/573 [13:58<1:31:20, 11.03s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1576])
Epoch 4:  13%|‚ñà‚ñé        | 77/573 [14:13<1:31:36, 11.08s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  13%|‚ñà‚ñé        | 77/573 [14:13<1:31:36, 11.08s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating train split: 12592 examples [9:44:02,  2.46s/ examples]embeddings shape: torch.Size([32, 768, 1594])
Epoch 4:  14%|‚ñà‚ñé        | 78/573 [14:29<1:31:55, 11.14s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  14%|‚ñà‚ñé        | 78/573 [14:29<1:31:55, 11.14s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1624])
Epoch 4:  14%|‚ñà‚ñç        | 79/573 [14:44<1:32:09, 11.19s/it, loss=0.315, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.344, train binary_accuracy_step=0.861, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  14%|‚ñà‚ñç        | 79/573 [14:44<1:32:09, 11.19s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating validation split: 790 examples [26:48,  1.96s/ examples]embeddings shape: torch.Size([32, 768, 1623])
Epoch 4:  14%|‚ñà‚ñç        | 80/573 [14:59<1:32:26, 11.25s/it, loss=0.313, v_num=hitj, train categorical_accuracy_step=0.625, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  14%|‚ñà‚ñç        | 80/573 [14:59<1:32:26, 11.25s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1566])
Epoch 4:  14%|‚ñà‚ñç        | 81/573 [15:16<1:32:45, 11.31s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.840, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  14%|‚ñà‚ñç        | 81/573 [15:16<1:32:45, 11.31s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861] Generating validation split: 2077 examples [1:13:04,  1.42s/ examples]embeddings shape: torch.Size([32, 768, 1595])
Epoch 4:  14%|‚ñà‚ñç        | 82/573 [15:22<1:32:06, 11.26s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  14%|‚ñà‚ñç        | 82/573 [15:22<1:32:06, 11.26s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1598])
Epoch 4:  14%|‚ñà‚ñç        | 83/573 [15:28<1:31:18, 11.18s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.500, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  14%|‚ñà‚ñç        | 83/573 [15:28<1:31:18, 11.18s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1593])
Epoch 4:  15%|‚ñà‚ñç        | 84/573 [15:33<1:30:34, 11.11s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.854, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  15%|‚ñà‚ñç        | 84/573 [15:33<1:30:34, 11.11s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1596])
Epoch 4:  15%|‚ñà‚ñç        | 85/573 [15:40<1:29:59, 11.06s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  15%|‚ñà‚ñç        | 85/573 [15:40<1:29:59, 11.06s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 890])
Epoch 4:  15%|‚ñà‚ñå        | 86/573 [15:46<1:29:20, 11.01s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.406, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  15%|‚ñà‚ñå        | 86/573 [15:46<1:29:20, 11.01s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1567])
Epoch 4:  15%|‚ñà‚ñå        | 87/573 [15:53<1:28:44, 10.95s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.531, train categorical_accuracy_strict_step=0.219, train binary_accuracy_step=0.844, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  15%|‚ñà‚ñå        | 87/573 [15:53<1:28:44, 10.95s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1528])
Epoch 4:  15%|‚ñà‚ñå        | 88/573 [16:11<1:29:14, 11.04s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.375, train categorical_accuracy_strict_step=0.281, train binary_accuracy_step=0.878, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  15%|‚ñà‚ñå        | 88/573 [16:11<1:29:14, 11.04s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating train split: 12641 examples [9:45:54,  2.41s/ examples]embeddings shape: torch.Size([32, 768, 1389])
Epoch 4:  16%|‚ñà‚ñå        | 89/573 [16:21<1:28:56, 11.03s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.438, train categorical_accuracy_strict_step=0.312, train binary_accuracy_step=0.882, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  16%|‚ñà‚ñå        | 89/573 [16:21<1:28:56, 11.03s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861] embeddings shape: torch.Size([32, 768, 1625])
Epoch 4:  16%|‚ñà‚ñå        | 90/573 [16:33<1:28:54, 11.04s/it, loss=0.32, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.875, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  16%|‚ñà‚ñå        | 90/573 [16:33<1:28:54, 11.04s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.910, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating validation split: 850 examples [28:42,  1.94s/ examples]embeddings shape: torch.Size([32, 768, 1566])
Epoch 4:  16%|‚ñà‚ñå        | 91/573 [16:51<1:29:18, 11.12s/it, loss=0.316, v_num=hitj, train categorical_accuracy_step=0.281, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.910, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  16%|‚ñà‚ñå        | 91/573 [16:51<1:29:18, 11.12s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1592])
Epoch 4:  16%|‚ñà‚ñå        | 92/573 [16:59<1:28:49, 11.08s/it, loss=0.317, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.250, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  16%|‚ñà‚ñå        | 92/573 [16:59<1:28:49, 11.08s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1416])
Epoch 4:  16%|‚ñà‚ñå        | 93/573 [17:11<1:28:42, 11.09s/it, loss=0.322, v_num=hitj, train categorical_accuracy_step=0.250, train categorical_accuracy_strict_step=0.125, train binary_accuracy_step=0.833, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  16%|‚ñà‚ñå        | 93/573 [17:11<1:28:42, 11.09s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]embeddings shape: torch.Size([32, 768, 1571])
Epoch 4:  16%|‚ñà‚ñã        | 94/573 [17:30<1:29:12, 11.18s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.469, train categorical_accuracy_strict_step=0.375, train binary_accuracy_step=0.868, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  16%|‚ñà‚ñã        | 94/573 [17:30<1:29:12, 11.18s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Generating validation split: 2144 examples [1:15:25,  1.62s/ examples]embeddings shape: torch.Size([32, 768, 1015])
Epoch 4:  17%|‚ñà‚ñã        | 95/573 [17:45<1:29:20, 11.21s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.344, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.865, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]Epoch 4:  17%|‚ñà‚ñã        | 95/573 [17:45<1:29:20, 11.21s/it, loss=0.321, v_num=hitj, train categorical_accuracy_step=0.406, train categorical_accuracy_strict_step=0.188, train binary_accuracy_step=0.872, val categorical_accuracy_step=0.955, val categorical_accuracy_strict_step=0.000, val binary_accuracy_step=0.864, val categorical_accuracy_epoch=0.413, val categorical_accuracy_strict_epoch=0.180, val binary_accuracy_epoch=0.851, train categorical_accuracy_epoch=0.403, train categorical_accuracy_strict_epoch=0.267, train binary_accuracy_epoch=0.861]