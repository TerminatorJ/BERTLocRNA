{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig, DataCollatorWithPadding, AutoModel\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS # BERT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the NT model\n",
    "root_dir = \"/home/sxr280/BERTLocRNA/\"\n",
    "model_path = \"zhihan1996/DNABERT-2-117M\"\n",
    "path_join = lambda *path: os.path.abspath(os.path.join(*path))\n",
    "cache_dir = path_join(root_dir, \"..\", \"saved_model\", \"DNABERT2\")\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir = cache_dir, trust_remote_code = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, \n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"../../../\")\n",
    "from utils.embedding_generator import NucleotideTransformerEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Lora:\n",
    "    def __init__(self, max_tokens = None, lora_config = None):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.lora_config = lora_config\n",
    "    def Etrunc(self, seq):\n",
    "        if len(seq) > self.max_tokens:\n",
    "            seq = seq[:self.max_tokens/2] + seq[-self.max_tokens/2:]\n",
    "\n",
    "        else:\n",
    "            seq = seq\n",
    "        return seq\n",
    "    def wrapper(self, model):\n",
    "        lora_config = LoraConfig(\n",
    "                r=self.lora_config[\"r\"], # Rank\n",
    "                lora_alpha=self.lora_config[\"lora_alpha\"],\n",
    "                target_modules=self.lora_config[\"target_modules\"],\n",
    "                # [\"query\", \"value\"],\n",
    "                lora_dropout=0.05,\n",
    "                bias=\"none\",\n",
    "                task_type=TaskType.SEQ_CLS # BERT\n",
    "            )\n",
    "        peft_model = get_peft_model(model, lora_config)\n",
    "        return peft_model\n",
    "    @staticmethod\n",
    "    def print_number_of_trainable_model_parameters(model):\n",
    "        trainable_model_params = 0\n",
    "        all_model_params = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            all_model_params += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_model_params += param.numel()\n",
    "        return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LoraNT(NucleotideTransformerEmbedder, Lora):\n",
    "     \n",
    "    def __init__(self, model_config, lora_config):\n",
    "        super(NucleotideTransformerEmbedder).__init__(task = model_config[\"task\"], \n",
    "                                                      model_path = model_config[\"model_path\"],\n",
    "                                                      hidden_dim = model_config[\"hidden_dim\"])#the model will be load automatically, and attributions are inherented \n",
    "        super(Lora).__init__(max_tokens = self.max_tokens, lora_config = lora_config)\n",
    "        #the input need to be truncated again to fixed the length limitation\n",
    "        #we should add the last layer to fit for the multi-label prediction\n",
    "        self.maxpool = nn.MaxPool1d(self.model_config.pooling_size, stride = self.model_config.pooling_size)\n",
    "        flat_dim = self.hidden_dim*self.max_tokens/self.model_config.pooling_size\n",
    "        self.last_layer = nn.Linear(flat_dim , self.model_config.nb_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        #Calculating the effeciency of the LoRA in the PLM\n",
    "        Lora.print_number_of_trainable_model_parameters(self.model)\n",
    "        #wrap the model with LoRa\n",
    "        self.model = self.wrapper(self.model)\n",
    "        \n",
    "\n",
    "    def tokenization(self, x):\n",
    "\n",
    "        x = Lora.Etrun(x)\n",
    "        tokens = self.tokenizer(\n",
    "                    x,\n",
    "                    truncation = True,\n",
    "                    padding = \"max_length\",\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "        input_ids = tokens[\"input_ids\"].int()\n",
    "        masks = tokens[\"attention_mask\"].int()\n",
    "        return input_ids, masks\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #preprocessing\n",
    "        input_ids, masks = self.tokenization(x)[0]\n",
    "        PLM_out = self.get_embed(input_ids, masks)\n",
    "        batch_size = PLM_out.size(0)\n",
    "        #flatten the embeddings\n",
    "        PLM_out = torch.view(batch_size, -1)\n",
    "        digit = self.last_layer(PLM_out)\n",
    "        pred = self.sigmoid(digit)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "hf_cache = \"/tmp/erda/BERTLocRNA/cache\"\n",
    "save_path = os.path.join(\"/\", \"tmp\", \"erda\", \"BERTLocRNA\", \"embeddings\", \"RNAlocalization_Lora\" + \"_\" + \"DNABERT2\" + \"embedding\")\n",
    "a = load_dataset(save_path)#, cache_dir = hf_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fm\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, TaskType, IA3Config\n",
    "path_join = lambda *path: os.path.abspath(os.path.join(*path))\n",
    "model_path = \"/home/sxr280/BERTLocRNA/saved_model/RNAFM\"\n",
    "model, alphabet = fm.pretrained.rna_fm_t12(path_join(model_path, \"RNA-FM_pretrained.pth\"))\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval() \n",
    "\n",
    "data = [\n",
    "    (\"RNA1\", \"GGGUGCGAUCAUACCAGCACUAAUGCCCUCCUGGGAAGUCCUCGUGUUGCACCCCU\"),\n",
    "    (\"RNA2\", \"GGGUGUCGCUCAGUUGGUAGAGUGCUUGCCUGGCAUGCAAGAAACCUUGGUUCAAUCCCCAGCACUGCA\"),\n",
    "    (\"RNA3\", \"CGAUUCNCGUUCCC--CCGCCUCCA\"),\n",
    "]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "# Extract embeddings (on CPU)\n",
    "\n",
    "results = model(batch_tokens, repr_layers=[12])\n",
    "token_embeddings = results[\"representations\"][12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "                r=8, # Rank\n",
    "                lora_alpha=16,\n",
    "                target_modules=[\"k_proj\", \"v_proj\", \"q_proj\"],\n",
    "                lora_dropout=0.05,\n",
    "                bias=\"all\"\n",
    "            )\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "results = peft_model(batch_tokens, repr_layers=[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 486,426 || all params: 99,890,186 || trainable%: 0.4869607510791901\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 71, 640])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = results[\"representations\"][12]\n",
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IA3_config = IA3Config(\n",
    "    task_type=TaskType.SEQ_CLS, target_modules=[\"k_proj\", \"v_proj\", \"f1\", \"f2\"], feedforward_modules=[\"f1\", \"f2\"]\n",
    ")\n",
    "peft_model = get_peft_model(model, IA3_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = peft_model(batch_tokens, repr_layers=[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results = peft_model(batch_tokens, repr_layers=[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = peft_model(batch_tokens, repr_layers=[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeploc_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
