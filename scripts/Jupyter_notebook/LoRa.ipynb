{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig, DataCollatorWithPadding, AutoModel\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS # BERT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m root_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/sxr280/BERTLocRNA/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m path_join \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39mpath: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;241m*\u001b[39mpath))\n\u001b[0;32m----> 4\u001b[0m local_path \u001b[38;5;241m=\u001b[39m \u001b[43mpath_join\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaved_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForMaskedLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(local_path)\n\u001b[1;32m      6\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstaDeepAI/nucleotide-transformer-2.5b-multi-species\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(*path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#loading the NT model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m root_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/sxr280/BERTLocRNA/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m path_join \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39mpath: \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;241m*\u001b[39mpath))\n\u001b[1;32m      4\u001b[0m local_path \u001b[38;5;241m=\u001b[39m path_join(root_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForMaskedLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(local_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#loading the NT model\n",
    "root_dir = \"/home/sxr280/BERTLocRNA/\"\n",
    "path_join = lambda *path: os.path.abspath(os.path.join(*path))\n",
    "local_path = path_join(root_dir, \"saved_model\", \"NT\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(local_path)\n",
    "model_path = \"InstaDeepAI/nucleotide-transformer-2.5b-multi-species\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, \n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"../../../\")\n",
    "from utils.embedding_generator import NucleotideTransformerEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Lora:\n",
    "    def __init__(self, max_tokens = None, lora_config = None):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.lora_config = lora_config\n",
    "    def Etrunc(self, seq):\n",
    "        if len(seq) > self.max_tokens:\n",
    "            seq = seq[:self.max_tokens/2] + seq[-self.max_tokens/2:]\n",
    "\n",
    "        else:\n",
    "            seq = seq\n",
    "        return seq\n",
    "    def wrapper(self, model):\n",
    "        lora_config = LoraConfig(\n",
    "                r=self.lora_config[\"r\"], # Rank\n",
    "                lora_alpha=self.lora_config[\"lora_alpha\"],\n",
    "                target_modules=self.lora_config[\"target_modules\"],\n",
    "                # [\"query\", \"value\"],\n",
    "                lora_dropout=0.05,\n",
    "                bias=\"none\",\n",
    "                task_type=TaskType.SEQ_CLS # BERT\n",
    "            )\n",
    "        peft_model = get_peft_model(model, lora_config)\n",
    "        return peft_model\n",
    "    @staticmethod\n",
    "    def print_number_of_trainable_model_parameters(model):\n",
    "        trainable_model_params = 0\n",
    "        all_model_params = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            all_model_params += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_model_params += param.numel()\n",
    "        return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LoraNT(NucleotideTransformerEmbedder, Lora):\n",
    "     \n",
    "    def __init__(self, model_config, lora_config):\n",
    "        super(NucleotideTransformerEmbedder).__init__(task = model_config[\"task\"], \n",
    "                                                      model_path = model_config[\"model_path\"],\n",
    "                                                      hidden_dim = model_config[\"hidden_dim\"])#the model will be load automatically, and attributions are inherented \n",
    "        super(Lora).__init__(max_tokens = self.max_tokens, lora_config = lora_config)\n",
    "        #the input need to be truncated again to fixed the length limitation\n",
    "        #we should add the last layer to fit for the multi-label prediction\n",
    "        self.maxpool = nn.MaxPool1d(self.model_config.pooling_size, stride = self.model_config.pooling_size)\n",
    "        flat_dim = self.hidden_dim*self.max_tokens/self.model_config.pooling_size\n",
    "        self.last_layer = nn.Linear(flat_dim , self.model_config.nb_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        #Calculating the effeciency of the LoRA in the PLM\n",
    "        Lora.print_number_of_trainable_model_parameters(self.model)\n",
    "        #wrap the model with LoRa\n",
    "        self.model = self.wrapper(self.model)\n",
    "        \n",
    "\n",
    "    def tokenization(self, x):\n",
    "\n",
    "        x = Lora.Etrun(x)\n",
    "        tokens = self.tokenizer(\n",
    "                    x,\n",
    "                    truncation = True,\n",
    "                    padding = \"max_length\",\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "        input_ids = tokens[\"input_ids\"].int()\n",
    "        masks = tokens[\"attention_mask\"].int()\n",
    "        return input_ids, masks\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #preprocessing\n",
    "        input_ids, masks = self.tokenization(x)[0]\n",
    "        PLM_out = self.get_embed(input_ids, masks)\n",
    "        batch_size = PLM_out.size(0)\n",
    "        #flatten the embeddings\n",
    "        PLM_out = torch.view(batch_size, -1)\n",
    "        digit = self.last_layer(PLM_out)\n",
    "        pred = self.sigmoid(digit)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeploc_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
