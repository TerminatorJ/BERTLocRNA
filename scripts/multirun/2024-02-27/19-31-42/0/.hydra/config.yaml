task: RNAlocalization_Lora
tokenizer: RNAFM
model: FullPLM
output_dir: ${hydra:runtime.cwd}/../output/${task}/${tokenizer}
nb_classes: ${FullPLM.model_config.nb_classes}
compartments:
- Nucleus
- Exosome
- Cytosol
- Cytoplasm
- Ribosome
- Membrane
- Endoplasmic reticulum
- Microvesicle
- Mitochondrion
loss_weight: false
sample_t: 80
RNAlocalization_Lora:
  path: TerminatorJ/localization_multiRNA
wandb:
  project: BERTLocRNA
  name: ${tokenizer}_fine-tune
FullPLM:
  model_config:
    nb_classes: 9
    drop_flat: 0.25
    normalizeatt: true
    activation: gelu
    RNA_dim: 4
    pooling_size: ${${tokenizer}.pooling_size}
    hidden_dim: ${${tokenizer}.hidden_dim}
    RNA_order:
    - UNK
    - A
    - C
    - G
    - T
    - 'N'
    - Y RNA
    - lincRNA
    - lncRNA
    - mRNA
    - miRNA
    - ncRNA
    - pseudo
    - rRNA
    - scRNA
    - scaRNA
    - snRNA
    - snoRNA
    - vRNA
nucleotidetransformer:
  _target_: BERTLocRNA.utils.token_generator.NTModuleTokenizer
  lora_config:
    r: 32
    lora_alpha: 64
    target_modules:
    - query
    - value
  model_path: InstaDeepAI/nucleotide-transformer-2.5b-multi-species
  batch_size: 1
  run_batch: 3000
  dataloader: true
  hidden_dim: 2560
  collator: true
  task: ${task}
  pooling_size: 8
DNABERT2:
  _target_: BERTLocRNA.utils.token_generator.DNABERT2ModuleTokenizer
  model_path: zhihan1996/DNABERT-2-117M
  lora_config:
    r: 32
    lora_alpha: 64
    target_modules:
    - Wqkv
  batch_size: 4
  run_batch: 3000
  dataloader: true
  hidden_dim: 768
  collator: true
  pooling_size: 6
  task: ${task}
RNAFM:
  _target_: BERTLocRNA.utils.token_generator.RNAFMModuleTokenizer
  model_path: /home/sxr280/BERTLocRNA/saved_model/RNAFM
  lora_config:
    r: 32
    lora_alpha: 64
    target_modules:
    - k_proj
    - v_proj
    - q_proj
  batch_size: 8
  run_batch: 3000
  dataloader: true
  hidden_dim: 640
  collator: true
  pooling_size: 8
  task: ${task}
parnet:
  _target_: BERTLocRNA.utils.token_generator.parnetModuleTokenizer
  model_path: /home/sxr280/BERTLocRNA/RBPLLM/Parnet
  lora_config: None
  batch_size: 8
  fine_tune_layers: 0
  run_batch: 3000
  dataloader: true
  hidden_dim: 256
  collator: true
  pooling_size: 8
  task: ${task}
Trainer:
  config:
    rna_ref:
    - lncRNA
    - mRNA
    - miRNA
    - snRNA
    - snoRNA
    accelerator: gpu
    strategy: ddp
    max_epochs: 200
    precison: 32
    num_nodes: 1
    log_every_n_steps: 1000
    patience: 10
    gradient_clip: true
    optimizer_cls: torch.optim.Adam
    lr: 0.001
    lora: true
    weight_decay: 1.0e-05
    nb_classes: ${nb_classes}
    output_dir: ${output_dir}
    RNA_order: ${${model}.model_config.RNA_order}
    compartments: ${compartments}
